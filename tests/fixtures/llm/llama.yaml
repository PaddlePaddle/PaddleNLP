pretrain:
  default:
    model_type: llama
    model_name_or_path: __internal_testing__/micro-random-llama
    input_dir: ./llm/llama/data/
    output: ./output/pretrain
    weight_decay: 0.01
    max_steps: 10
    save_steps: 10
    warmup_steps: 2
    warmup_ratio: 0.01
    per_device_train_batch_size: 4
    device: gpu
    eval_steps: 10
    do_train: true
    do_predict: true
    use_flash_attention: 0
    use_fused_rms_norm: 0 
  
  slow:
    model_type: llama
    model_name_or_path: facebook/llama-7b
    input_dir: ./llm/llama/data/
    output: ./output/pretrain
    weight_decay: 0.01
    max_steps: 5000
    save_steps: 1000
    warmup_steps: 2
    warmup_ratio: 0.01
    per_device_train_batch_size: 4
    device: gpu
    eval_steps: 10
    do_train: true
    do_predict: true
    use_flash_attention: 0
    use_fused_rms_norm: 0 
  
merge:
  default:
    model_name_or_path: ./checkpoints/llama_sft_ckpts/checkpoint-5

  slow:
    model_name_or_path: ./checkpoints/llama_sft_ckpts/checkpoint-5000
