pretrain:
  slow:
    model_type: llama
    model_name_or_path: __internal_testing__/micro-random-llama
    input_dir: ./llm/llama/data/
    output: ./llm/llama/output/pretrain
    weight_decay: 0.01
    max_steps: 2
    save_steps: 2
    eval_steps: 2
    logging_steps: 1
    warmup_steps: 2
    warmup_ratio: 0.01
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    device: gpu
    data_impl: "mmap"
    fp16: true
    fp16_opt_level: "O2" 
    do_train: true
    do_predict: true
    use_flash_attention: 1
    use_fused_rms_norm: 0 
    continue_training: 1
  
merge_tp_params:
  slow:
    model_name_or_path: llm/checkpoints/llama_sft_ckpts/checkpoint-2

merge_lora_params:
  slow:
    model_name_or_path: __internal_testing__/micro-random-llama
    lora_path: llm/checkpoints/llama_lora_ckpts/checkpoint-2

predict:
  slow:
    model_name_or_path: __internal_testing__/micro-random-llama
    batch_size: 1 
    data_file: ./tests/fixtures/llm/data/dev.json 
    dtype: "float16" 
    mode: "dynamic"