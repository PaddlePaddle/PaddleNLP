rm:
  base:
    train_dataset_path: "./tests/fixtures/llm/pref_data/train.jsonl"
    dev_dataset_path: "./tests/fixtures/llm/pref_data/dev.jsonl"
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 2
    per_device_eval_batch_size: 1
    eval_accumulation_steps: 1
    max_steps: 3
    learning_rate: 1e-06
    warmup_steps: 0
    logging_steps: 1
    evaluation_strategy: "steps"
    save_strategy: "steps"
    max_prompt_len: 1024
    max_seq_len: 2048
    fp16: true
    fp16_opt_level: "O2"
    do_train: true
    do_eval: true
    use_flash_attention: true
    disable_tqdm: true
    recompute: true
    save_total_limit: 1
    tensor_parallel_degree: 1
    pipeline_parallel_degree: 1
    ignore_save_lr_and_optim: 1
    flash_mask: 1
  default:
    llama:
      model_name_or_path: __internal_testing__/tiny-random-llama
    chatglm2:
      model_name_or_path: __internal_testing__/tiny-fused-chatglm2
    qwen:
      model_name_or_path: __internal_testing__/tiny-fused-qwen
    baichuan:
      model_name_or_path: __internal_testing__/tiny-fused-baichuan
    qwen2:
      model_name_or_path: __internal_testing__/tiny-random-qwen2

inference-predict:
  default:
    mode: dynamic 
    max_length: 20
    batch_size: 2
    decode_strategy: greedy_search
    dtype: float16

inference-to-static:
  default:
    dtype: float16
    max_length: 20

inference-infer:
  default:
    mode: static
    dtype: float16
    batch_size: 2
    decode_strategy: greedy_search
    max_length: 20