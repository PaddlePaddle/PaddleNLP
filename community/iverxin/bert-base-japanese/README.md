## bert-base-japanese
12 repeating layers, 768-hidden, 12-heads. 

This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece  subword tokenization. 
[reference](https://huggingface.co/cl-tohoku/bert-base-japanese)

