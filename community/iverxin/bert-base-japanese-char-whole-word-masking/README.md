## bert-base-japanese
12 repeating layers, 768-hidden, 12-heads. 

This version of the model processes input texts with word-level  tokenization based on the IPA dictionary, followed by character-level  tokenization. Additionally, the model is trained with the whole word masking enabled  for the masked language modeling (MLM) objective.
[reference](https://huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking) 
