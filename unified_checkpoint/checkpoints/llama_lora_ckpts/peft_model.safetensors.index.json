{
    "metadata": {
        "total_size": 2523136
    },
    "weight_map": {
        "llama.layers.0.self_attn.q_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.q_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.k_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.k_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.v_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.v_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.o_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.o_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.gate_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.gate_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.up_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.up_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.down_proj.lora_A": "peft_model-00001-of-00008.safetensors",
        "llama.layers.0.mlp.down_proj.lora_B": "peft_model-00001-of-00008.safetensors",
        "llama.layers.1.self_attn.q_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.q_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.k_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.k_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.v_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.v_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.o_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.o_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.gate_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.gate_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.up_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.up_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.down_proj.lora_A": "peft_model-00002-of-00008.safetensors",
        "llama.layers.1.mlp.down_proj.lora_B": "peft_model-00002-of-00008.safetensors",
        "llama.layers.2.self_attn.q_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.q_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.k_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.k_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.v_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.v_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.o_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.o_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.gate_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.gate_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.up_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.up_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.down_proj.lora_A": "peft_model-00003-of-00008.safetensors",
        "llama.layers.2.mlp.down_proj.lora_B": "peft_model-00003-of-00008.safetensors",
        "llama.layers.3.self_attn.q_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.q_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.k_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.k_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.v_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.v_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.o_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.o_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.gate_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.gate_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.up_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.up_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.down_proj.lora_A": "peft_model-00004-of-00008.safetensors",
        "llama.layers.3.mlp.down_proj.lora_B": "peft_model-00004-of-00008.safetensors",
        "llama.layers.4.self_attn.q_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.q_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.k_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.k_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.v_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.v_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.o_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.o_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.gate_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.gate_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.up_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.up_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.down_proj.lora_A": "peft_model-00005-of-00008.safetensors",
        "llama.layers.4.mlp.down_proj.lora_B": "peft_model-00005-of-00008.safetensors",
        "llama.layers.5.self_attn.q_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.q_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.k_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.k_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.v_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.v_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.o_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.o_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.gate_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.gate_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.up_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.up_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.down_proj.lora_A": "peft_model-00006-of-00008.safetensors",
        "llama.layers.5.mlp.down_proj.lora_B": "peft_model-00006-of-00008.safetensors",
        "llama.layers.6.self_attn.q_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.q_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.k_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.k_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.v_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.v_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.o_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.o_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.gate_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.gate_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.up_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.up_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.down_proj.lora_A": "peft_model-00007-of-00008.safetensors",
        "llama.layers.6.mlp.down_proj.lora_B": "peft_model-00007-of-00008.safetensors",
        "llama.layers.7.self_attn.q_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.q_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.k_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.k_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.v_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.v_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.o_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.o_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.gate_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.gate_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.up_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.up_proj.lora_B": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.down_proj.lora_A": "peft_model-00008-of-00008.safetensors",
        "llama.layers.7.mlp.down_proj.lora_B": "peft_model-00008-of-00008.safetensors"
    },
    "type": "lora"
}