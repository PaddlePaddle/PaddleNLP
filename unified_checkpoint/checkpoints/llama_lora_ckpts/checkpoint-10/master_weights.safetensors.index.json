{
    "metadata": {
        "total_size": 5046272
    },
    "weight_map": {
        "llama.layers.0.self_attn.q_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.q_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.k_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.k_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.v_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.v_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.o_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.self_attn.o_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.gate_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.gate_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.up_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.up_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.down_proj.lora_A": "master_weights-00001-of-00008.safetensors",
        "llama.layers.0.mlp.down_proj.lora_B": "master_weights-00001-of-00008.safetensors",
        "llama.layers.1.self_attn.q_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.q_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.k_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.k_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.v_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.v_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.o_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.self_attn.o_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.gate_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.gate_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.up_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.up_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.down_proj.lora_A": "master_weights-00002-of-00008.safetensors",
        "llama.layers.1.mlp.down_proj.lora_B": "master_weights-00002-of-00008.safetensors",
        "llama.layers.2.self_attn.q_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.q_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.k_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.k_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.v_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.v_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.o_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.self_attn.o_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.gate_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.gate_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.up_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.up_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.down_proj.lora_A": "master_weights-00003-of-00008.safetensors",
        "llama.layers.2.mlp.down_proj.lora_B": "master_weights-00003-of-00008.safetensors",
        "llama.layers.3.self_attn.q_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.q_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.k_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.k_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.v_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.v_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.o_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.self_attn.o_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.gate_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.gate_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.up_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.up_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.down_proj.lora_A": "master_weights-00004-of-00008.safetensors",
        "llama.layers.3.mlp.down_proj.lora_B": "master_weights-00004-of-00008.safetensors",
        "llama.layers.4.self_attn.q_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.q_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.k_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.k_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.v_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.v_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.o_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.self_attn.o_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.gate_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.gate_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.up_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.up_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.down_proj.lora_A": "master_weights-00005-of-00008.safetensors",
        "llama.layers.4.mlp.down_proj.lora_B": "master_weights-00005-of-00008.safetensors",
        "llama.layers.5.self_attn.q_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.q_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.k_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.k_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.v_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.v_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.o_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.self_attn.o_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.gate_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.gate_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.up_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.up_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.down_proj.lora_A": "master_weights-00006-of-00008.safetensors",
        "llama.layers.5.mlp.down_proj.lora_B": "master_weights-00006-of-00008.safetensors",
        "llama.layers.6.self_attn.q_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.q_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.k_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.k_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.v_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.v_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.o_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.self_attn.o_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.gate_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.gate_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.up_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.up_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.down_proj.lora_A": "master_weights-00007-of-00008.safetensors",
        "llama.layers.6.mlp.down_proj.lora_B": "master_weights-00007-of-00008.safetensors",
        "llama.layers.7.self_attn.q_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.q_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.k_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.k_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.v_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.v_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.o_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.self_attn.o_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.gate_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.gate_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.up_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.up_proj.lora_B": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.down_proj.lora_A": "master_weights-00008-of-00008.safetensors",
        "llama.layers.7.mlp.down_proj.lora_B": "master_weights-00008-of-00008.safetensors"
    }
}