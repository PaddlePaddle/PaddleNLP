<div align="center">
  <img src="https://user-images.githubusercontent.com/11793384/215372703-4385f66a-abe4-44c7-9626-96b7b65270c8.png" width="40%" height="40%" />
</div>

<p align="center">
    <a href="https://pypi.org/project/paddlenlp/"><img src="https://img.shields.io/pypi/pyversions/paddlenlp"></a>
    <a href=""><img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-yellow.svg"></a>
    <a href="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/LICENSE"><img src="https://img.shields.io/github/license/paddlepaddle/paddlenlp"></a>
</p>

<h4 align="center">
  <a href=#ç‰¹æ€§> ç‰¹æ€§ </a> |
  <a href=#å®‰è£…> å®‰è£… </a> |
  <a href=#å¿«é€Ÿå¼€å§‹> å¿«é€Ÿå¼€å§‹ </a> |
  <a href=#æ¨¡å‹éƒ¨ç½²> æ¨¡å‹éƒ¨ç½²</a>
</h4>

# PPDiffusers: Diffusers toolbox implemented based on PaddlePaddle

**PPDiffusers**æ˜¯ä¸€æ¬¾æ”¯æŒå¤šç§æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å›¾åƒè·¨æ¨¡æ€ã€å›¾åƒã€è¯­éŸ³ï¼‰æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰è®­ç»ƒå’Œæ¨ç†çš„å›½äº§åŒ–å·¥å…·ç®±ï¼Œä¾æ‰˜äº[**PaddlePaddle**](https://www.paddlepaddle.org.cn/)æ¡†æ¶å’Œ[**PaddleNLP**](https://github.com/PaddlePaddle/PaddleNLP)è‡ªç„¶è¯­è¨€å¤„ç†å¼€å‘åº“ã€‚

## News ğŸ“¢
* ğŸ”¥ **2023.07.25 PPDiffusers ç°å·²è¿è‡³ [PaddleMIX](https://github.com/PaddlePaddle/PaddleMIX)ï¼Œåç»­æ›´æ–°è¿­ä»£å°†äº PaddleMIX è¿›è¡Œã€‚PaddleMIX æ˜¯åŸºäºé£æ¡¨çš„è·¨æ¨¡æ€å¤§æ¨¡å‹å¼€å‘å¥—ä»¶ï¼Œæˆ‘ä»¬å°†ä¸€å¦‚æ—¢å¾€çš„æä¾›ä¼˜è´¨å†…å®¹ï¼Œå›´ç»•è·¨æ¨¡æ€å‰æ²¿æŠ€æœ¯æ›´å……åˆ†çš„å‘æŒ¥é£æ¡¨å¤§æ¨¡å‹æŠ€æœ¯å…ˆè¿›æ€§ï¼Œæ‰“é€ é«˜æ•ˆå¼€å‘ä½¿ç”¨è·¨æ¨¡æ€å¤§æ¨¡å‹åŠåº”ç”¨çš„åŸºç¡€è®¾æ–½ã€‚æ„Ÿè°¢å¤§å®¶å…³æ³¨ï¼Œæ¬¢è¿å¤§å®¶ä½¿ç”¨ PaddleMIXã€‚**

* ğŸ”¥ **2023.06.20 å‘å¸ƒ 0.16.1 ç‰ˆæœ¬ï¼Œæ–°å¢[T2I-Adapter](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/t2i-adapter)ï¼Œæ”¯æŒè®­ç»ƒä¸æ¨ç†ï¼›ControlNetå‡çº§ï¼Œæ”¯æŒ[reference onlyæ¨ç†](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/community#controlnet-reference-only)ï¼›æ–°å¢[WebUIStableDiffusionPipeline](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/community#automatic1111-webui-stable-diffusion)ï¼Œ
æ”¯æŒé€šè¿‡promptçš„æ–¹å¼åŠ¨æ€åŠ è½½loraã€textual_inversionæƒé‡ï¼›
æ–°å¢[StableDiffusionHiresFixPipeline](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/community#stable-diffusion-with-high-resolution-fixing)ï¼Œæ”¯æŒé«˜åˆ†è¾¨ç‡ä¿®å¤ï¼›
æ–°å¢å…³é”®ç‚¹æ§åˆ¶ç”Ÿæˆä»»åŠ¡è¯„ä»·æŒ‡æ ‡[COCOeval](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/scripts/cocoeval_keypoints_score)ï¼›
æ–°å¢å¤šç§æ¨¡æ€æ‰©æ•£æ¨¡å‹Pipelineï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆï¼ˆ[Text-to-Video-Synth](#æ–‡æœ¬è§†é¢‘å¤šæ¨¡)ã€[Text-to-Video-Zero](#æ–‡æœ¬è§†é¢‘å¤šæ¨¡)ï¼‰ã€éŸ³é¢‘ç”Ÿæˆï¼ˆ[AudioLDM](#æ–‡æœ¬éŸ³é¢‘å¤šæ¨¡)ã€[Spectrogram Diffusion](#éŸ³é¢‘)ï¼‰ï¼›æ–°å¢æ–‡å›¾ç”Ÿæˆæ¨¡å‹[IF](#æ–‡æœ¬å›¾åƒå¤šæ¨¡)ã€‚**

* ğŸ”¥ **2023.03.29 å‘å¸ƒ 0.14.0 ç‰ˆæœ¬ï¼Œæ–°å¢[LoRA](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/dreambooth)ã€[ControlNet](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/controlnet)ï¼Œæ”¯æŒè®­ç»ƒä¸æ¨ç†ï¼›
æ¨¡å‹åŠ è½½å‡çº§ï¼Œ[å¯ç›´æ¥åŠ è½½HF Diffusersçš„æƒé‡](#åŠ è½½HF-Diffusersæƒé‡)ï¼ˆsafetensorså’Œptï¼‰æˆ– [SDç­‰åŸåº“çš„Lightningæƒé‡è¿›è¡Œæ¨ç†](#åŠ è½½åŸåº“çš„Lightningæƒé‡)ï¼Œ[æ”¯æŒåŠ è½½Civitaiç¤¾åŒºçš„LoRAæƒé‡](#åŠ è½½Civitaiç¤¾åŒºçš„LoRAæƒé‡)ï¼›
[æ”¯æŒxformers](#XFormersåŠ é€Ÿ) è®­ç»ƒä¸æ¨ç†ï¼›
æ–°å¢ç”¨äºè¶…é«˜åˆ†è¾¨ç‡ç”Ÿæˆçš„VAE tilingï¼›
æ–°å¢Instruct Pix2Pixã€Semantic guidanceã€Depth2imageç­‰æ¨¡å‹ã€‚**



## ç‰¹æ€§
#### ğŸ“¦ SOTAæ‰©æ•£æ¨¡å‹Pipelinesé›†åˆ
æˆ‘ä»¬æä¾›**SOTAï¼ˆState-of-the-Artï¼‰** çš„æ‰©æ•£æ¨¡å‹Pipelinesé›†åˆã€‚
ç›®å‰**PPDiffusers**å·²ç»é›†æˆäº†**50+Pipelines**ï¼Œæ”¯æŒæ–‡å›¾ç”Ÿæˆï¼ˆText-to-Image Generationï¼‰ã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼ˆText-Guided Image Inpaintingï¼‰ã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒå˜æ¢ï¼ˆImage-to-Image Text-Guided Generationï¼‰ã€æ–‡æœ¬æ¡ä»¶è§†é¢‘ç”Ÿæˆï¼ˆText-to-Video Generationï¼‰ã€è¶…åˆ†ï¼ˆSuper Superresolutionï¼‰åœ¨å†…çš„**10ä½™é¡¹**ä»»åŠ¡ï¼Œè¦†ç›–**æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘**ç­‰å¤šç§æ¨¡æ€ã€‚
å¦‚æœæƒ³è¦äº†è§£å½“å‰æ”¯æŒçš„æ‰€æœ‰**Pipelines**ä»¥åŠå¯¹åº”çš„æ¥æºä¿¡æ¯ï¼Œå¯ä»¥é˜…è¯»[ğŸ”¥ PPDiffusers Pipelines](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/pipelines/README.md)æ–‡æ¡£ã€‚


#### ğŸ”Š æä¾›ä¸°å¯Œçš„Noise Scheduler
æˆ‘ä»¬æä¾›äº†ä¸°å¯Œçš„**å™ªå£°è°ƒåº¦å™¨ï¼ˆNoise Schedulerï¼‰**ï¼Œå¯ä»¥å¯¹**é€Ÿåº¦**ä¸**è´¨é‡**è¿›è¡Œæƒè¡¡ï¼Œç”¨æˆ·å¯åœ¨æ¨ç†æ—¶æ ¹æ®éœ€æ±‚å¿«é€Ÿåˆ‡æ¢ä½¿ç”¨ã€‚
å½“å‰**PPDiffusers**å·²ç»é›†æˆäº†**14+Scheduler**ï¼Œä¸ä»…æ”¯æŒ [DDPM](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/schedulers/scheduling_ddpm.py)ã€[DDIM](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/schedulers/scheduling_ddim.py) å’Œ [PNDM](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/schedulers/scheduling_pndm.py)ï¼Œè¿˜æ”¯æŒæœ€æ–°çš„ [ğŸ”¥ DPMSolver](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/schedulers/scheduling_dpmsolver_multistep.py)ï¼

#### ğŸ›ï¸ æä¾›å¤šç§æ‰©æ•£æ¨¡å‹ç»„ä»¶
æˆ‘ä»¬æä¾›äº†**å¤šç§æ‰©æ•£æ¨¡å‹**ç»„ä»¶ï¼Œå¦‚[UNet1DModel](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/unet_1d.py)ã€[UNet2DModel](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/unet_2d.py)ã€[UNet2DConditionModel](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/unet_2d_condition.py)ã€[UNet3DConditionModel](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/unet_3d_condition.py)ã€[VQModel](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/vae.py)ã€[AutoencoderKL](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/models/vae.py)ç­‰ã€‚


#### ğŸ“– æä¾›ä¸°å¯Œçš„è®­ç»ƒå’Œæ¨ç†æ•™ç¨‹
æˆ‘ä»¬æä¾›äº†ä¸°å¯Œçš„è®­ç»ƒæ•™ç¨‹ï¼Œä¸ä»…æ”¯æŒæ‰©æ•£æ¨¡å‹çš„äºŒæ¬¡å¼€å‘å¾®è°ƒï¼Œå¦‚åŸºäº[Textual Inversion](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/textual_inversion)å’Œ[DreamBooth](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/dreambooth)ä½¿ç”¨3-5å¼ å›¾å®šåˆ¶åŒ–è®­ç»ƒç”Ÿæˆå›¾åƒçš„é£æ ¼æˆ–ç‰©ä½“ï¼Œè¿˜æ”¯æŒ[ğŸ”¥ Latent Diffusion Model](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/text_to_image_laion400m)ã€[ğŸ”¥ ControlNet](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/controlnet)ã€[ğŸ”¥ T2I-Adapter](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/t2i-adapter)  ç­‰æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒï¼
æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸°å¯Œçš„[ğŸ”¥ Pipelinesæ¨ç†æ ·ä¾‹](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/inference)ã€‚

#### ğŸš€ æ”¯æŒFastDeployé«˜æ€§èƒ½éƒ¨ç½²
æˆ‘ä»¬æä¾›åŸºäº[FastDeploy](https://github.com/PaddlePaddle/FastDeploy)çš„[ğŸ”¥ é«˜æ€§èƒ½Stable Diffusion Pipeline](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/ppdiffusers/pipelines/stable_diffusion/pipeline_fastdeploy_stable_diffusion.py)ï¼Œæ›´å¤šæœ‰å…³FastDeployè¿›è¡Œå¤šæ¨ç†å¼•æ“åç«¯é«˜æ€§èƒ½éƒ¨ç½²çš„ä¿¡æ¯è¯·å‚è€ƒ[ğŸ”¥ é«˜æ€§èƒ½FastDeployæ¨ç†æ•™ç¨‹](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/deploy)ã€‚
```python
from ppdiffusers import StableDiffusionPipeline, FastDeployStableDiffusionPipeline

orig_pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
fd_pipe = FastDeployStableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5@fastdeploy")
```

## ä»»åŠ¡å±•ç¤º
### æ–‡æœ¬å›¾åƒå¤šæ¨¡

<details open>
<summary>&emsp;æ–‡å›¾ç”Ÿæˆï¼ˆText-to-Image Generationï¼‰</summary>

#### text_to_image_generation-stable_diffusion

```python
from ppdiffusers import StableDiffusionPipeline

# åŠ è½½æ¨¡å‹å’Œscheduler
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

# æ‰§è¡Œpipelineè¿›è¡Œæ¨ç†
prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]

# ä¿å­˜å›¾ç‰‡
image.save("astronaut_rides_horse_sd.png")
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209322401-6ecfeaaa-6878-4302-b592-07a31de4e590.png">
</div>

#### text_to_image_generation-deepfloyd_if

```python
import paddle

from ppdiffusers import DiffusionPipeline, IFPipeline, IFSuperResolutionPipeline
from ppdiffusers.utils import pd_to_pil

# Stage 1: generate images
pipe = IFPipeline.from_pretrained("DeepFloyd/IF-I-XL-v1.0", variant="fp16", paddle_dtype=paddle.float16)
pipe.enable_xformers_memory_efficient_attention()
prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says "very deep learning"'
prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
image = pipe(
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    output_type="pd",
).images

# save intermediate image
pil_image = pd_to_pil(image)
pil_image[0].save("text_to_image_generation-deepfloyd_if-result-if_stage_I.png")
# save gpu memory
pipe.to(paddle_device="cpu")

# Stage 2: super resolution stage1
super_res_1_pipe = IFSuperResolutionPipeline.from_pretrained(
    "DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", paddle_dtype=paddle.float16
)
super_res_1_pipe.enable_xformers_memory_efficient_attention()

image = super_res_1_pipe(
    image=image,
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    output_type="pd",
).images
# save intermediate image
pil_image = pd_to_pil(image)
pil_image[0].save("text_to_image_generation-deepfloyd_if-result-if_stage_II.png")
# save gpu memory
super_res_1_pipe.to(paddle_device="cpu")
```
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/246785766-700dfad9-159d-4bfb-bfc7-c18df938a052.png">
</div>
<div align="center">
<center>if_stage_I</center>
</div>
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/246785773-3359ca5f-dadf-4cc8-b318-ff1f9d4a2d35.png">
</div>
<div align="center">
<center>if_stage_II</center>
<!-- <img alt="image" src="https://user-images.githubusercontent.com/20476674/246785774-8870829a-354b-4a87-9d67-93af315f51e6.png">
<center>if_stage_III</center> -->
</div>
</details>


<details><summary>&emsp;æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ”¾å¤§ï¼ˆText-Guided Image Upscalingï¼‰</summary>

#### text_guided_image_upscaling-stable_diffusion_2

```python
from ppdiffusers import StableDiffusionUpscalePipeline
from ppdiffusers.utils import load_image

pipe = StableDiffusionUpscalePipeline.from_pretrained("stabilityai/stable-diffusion-x4-upscaler")

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/low_res_cat.png"
low_res_img = load_image(url).resize((128, 128))

prompt = "a white cat"
upscaled_image = pipe(prompt=prompt, image=low_res_img).images[0]
upscaled_image.save("upsampled_cat_sd2.png")
```
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/209324085-0d058b70-89b0-43c2-affe-534eedf116cf.png">
<center>åŸå›¾åƒ</center>
<img alt="image" src="https://user-images.githubusercontent.com/20476674/209323862-ce2d8658-a52b-4f35-90cb-aa7d310022e7.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>

<details><summary>&emsp;æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼ˆText-Guided Image Inpaintingï¼‰</summary>

#### text_guided_image_inpainting-stable_diffusion_2

```python
import paddle

from ppdiffusers import PaintByExamplePipeline
from ppdiffusers.utils import load_image

img_url = "https://paddlenlp.bj.bcebos.com/models/community/Fantasy-Studio/data/image_example_1.png"
mask_url = "https://paddlenlp.bj.bcebos.com/models/community/Fantasy-Studio/data/mask_example_1.png"
example_url = "https://paddlenlp.bj.bcebos.com/models/community/Fantasy-Studio/data/reference_example_1.jpeg"

init_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))
example_image = load_image(example_url).resize((512, 512))

pipe = PaintByExamplePipeline.from_pretrained("Fantasy-Studio/Paint-by-Example")

# ä½¿ç”¨fp16åŠ å¿«ç”Ÿæˆé€Ÿåº¦
with paddle.amp.auto_cast(True):
    image = pipe(image=init_image, mask_image=mask_image, example_image=example_image).images[0]
image.save("image_guided_image_inpainting-paint_by_example-result.png")
```
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/247118364-5d91f433-f9ac-4514-b5f0-cb4599905847.png" width=300>
<center>åŸå›¾åƒ</center>
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/247118361-0f78d6db-6896-4f8d-b1bd-8350192f7a4e.png" width=300>
<center>æ©ç å›¾åƒ</center>
<div align="center">
<img alt="image" src="https://user-images.githubusercontent.com/20476674/247118368-305a048d-ddc3-4a5f-8915-58591ef680f0.jpeg" width=300>
<center>å‚è€ƒå›¾åƒ</center>
<img alt="image" src="https://user-images.githubusercontent.com/20476674/247117963-e5b9b754-39a3-480b-a557-46a2f9310e79.png" width=300>
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>


<details><summary>&emsp;æ–‡æœ¬å¼•å¯¼çš„å›¾åƒå˜æ¢ï¼ˆImage-to-Image Text-Guided Generationï¼‰</summary>

#### image_to_image_text_guided_generation-stable_diffusion
```python
import paddle

from ppdiffusers import StableDiffusionImg2ImgPipeline
from ppdiffusers.utils import load_image

# åŠ è½½pipeline
pipe = StableDiffusionImg2ImgPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

# ä¸‹è½½åˆå§‹å›¾ç‰‡
url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/sketch-mountains-input.png"

init_image = load_image(url).resize((768, 512))

prompt = "A fantasy landscape, trending on artstation"
# ä½¿ç”¨fp16åŠ å¿«ç”Ÿæˆé€Ÿåº¦
with paddle.amp.auto_cast(True):
    image = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images[0]

image.save("fantasy_landscape.png")
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209327142-d8e1d0c7-3bf8-4a08-a0e8-b11451fc84d8.png">
<center>åŸå›¾åƒ</center>
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209325799-d9ff279b-0d57-435f-bda7-763e3323be23.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>
</details>

<details><summary>&emsp;æ–‡æœ¬å›¾åƒåŒå¼•å¯¼å›¾åƒç”Ÿæˆï¼ˆDual Text and Image Guided Generationï¼‰</summary>

#### dual_text_and_image_guided_generation-versatile_diffusion
```python
from ppdiffusers import VersatileDiffusionDualGuidedPipeline
from ppdiffusers.utils import load_image

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/benz.jpg"
image = load_image(url)
text = "a red car in the sun"

pipe = VersatileDiffusionDualGuidedPipeline.from_pretrained("shi-labs/versatile-diffusion")
pipe.remove_unused_weights()

text_to_image_strength = 0.75
image = pipe(prompt=text, image=image, text_to_image_strength=text_to_image_strength).images[0]
image.save("versatile-diffusion-red_car.png")
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209325965-2475e9c4-a524-4970-8498-dfe10ff9cf24.jpg" >
<center>åŸå›¾åƒ</center>
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209325293-049098d0-d591-4abc-b151-9291ac2636da.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>

### æ–‡æœ¬è§†é¢‘å¤šæ¨¡

<details open>
<summary>&emsp;æ–‡æœ¬æ¡ä»¶çš„è§†é¢‘ç”Ÿæˆï¼ˆText-to-Video Generationï¼‰</summary>

#### text_to_video_generation-synth

```python
import imageio

from ppdiffusers import DPMSolverMultistepScheduler, TextToVideoSDPipeline

pipe = TextToVideoSDPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b")
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

prompt = "An astronaut riding a horse."
video_frames = pipe(prompt, num_inference_steps=25).frames
imageio.mimsave("text_to_video_generation-synth-result-astronaut_riding_a_horse.mp4", video_frames, fps=8)
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/246780441-8242a955-490b-4326-8415-84264a54a938.gif">
</div>

#### text_to_video_generation-zero

```python
import imageio

# pip install imageio[ffmpeg]
import paddle

from ppdiffusers import TextToVideoZeroPipeline

model_id = "runwayml/stable-diffusion-v1-5"
pipe = TextToVideoZeroPipeline.from_pretrained(model_id, paddle_dtype=paddle.float16)

prompt = "A panda is playing guitar on times square"
result = pipe(prompt=prompt).images
result = [(r * 255).astype("uint8") for r in result]
imageio.mimsave("text_to_video_generation-zero-result-panda.mp4", result, fps=4)
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/246779321-c2b0c2b4-e383-40c7-a4d8-f417e8062b35.gif">
</div>

</details>

### æ–‡æœ¬éŸ³é¢‘å¤šæ¨¡
<details open>
<summary>&emsp;æ–‡æœ¬æ¡ä»¶çš„éŸ³é¢‘ç”Ÿæˆï¼ˆText-to-Audio Generationï¼‰</summary>

#### text_to_audio_generation-audio_ldm

```python
import paddle
import scipy

from ppdiffusers import AudioLDMPipeline

pipe = AudioLDMPipeline.from_pretrained("cvssp/audioldm", paddle_dtype=paddle.float16)

prompt = "Techno music with a strong, upbeat tempo and high melodic riffs"
audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]

output_path = "text_to_audio_generation-audio_ldm-techno.wav"
# save the audio sample as a .wav file
scipy.io.wavfile.write(output_path, rate=16000, data=audio)
```
<div align = "center">
  <thead>
  </thead>
  <tbody>
   <tr>
      <td align = "center">
      <a href="https://paddlenlp.bj.bcebos.com/models/community/westfish/develop_ppdiffusers_data/techno.wav" rel="nofollow">
            <img align="center" src="https://user-images.githubusercontent.com/20476674/209344877-edbf1c24-f08d-4e3b-88a4-a27e1fd0a858.png" width="200 style="max-width: 100%;"></a><br>
      </td>
    </tr>
  </tbody>
</div>
</details>

### å›¾åƒ

<details><summary>&emsp;æ— æ¡ä»¶å›¾åƒç”Ÿæˆï¼ˆUnconditional Image Generationï¼‰</summary>

#### unconditional_image_generation-latent_diffusion_uncond

```python
from ppdiffusers import LDMPipeline

# åŠ è½½æ¨¡å‹å’Œscheduler
pipe = LDMPipeline.from_pretrained("CompVis/ldm-celebahq-256")

# æ‰§è¡Œpipelineè¿›è¡Œæ¨ç†
image = pipe(num_inference_steps=200).images[0]

# ä¿å­˜å›¾ç‰‡
image.save("ldm_generated_image.png")
```
<div align="center">
<img width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209327936-7fe914e0-0ea0-4e21-a433-24eaed6ee94c.png">
</div>
</details>

<details><summary>&emsp;è¶…åˆ†ï¼ˆSuper Superresolutionï¼‰</summary>

#### super_resolution-latent_diffusion
```python
import paddle

from ppdiffusers import LDMSuperResolutionPipeline
from ppdiffusers.utils import load_image

# åŠ è½½pipeline
pipe = LDMSuperResolutionPipeline.from_pretrained("CompVis/ldm-super-resolution-4x-openimages")

# ä¸‹è½½åˆå§‹å›¾ç‰‡
url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations.png"

init_image = load_image(url).resize((128, 128))
init_image.save("original-image.png")

# ä½¿ç”¨fp16åŠ å¿«ç”Ÿæˆé€Ÿåº¦
with paddle.amp.auto_cast(True):
    image = pipe(init_image, num_inference_steps=100, eta=1).images[0]

image.save("super-resolution-image.png")
```
<div align="center">
<img  alt="image" src="https://user-images.githubusercontent.com/20476674/209328660-9700fdc3-72b3-43bd-9a00-23b370ba030b.png">
<center>åŸå›¾åƒ</center>
<img  alt="image" src="https://user-images.githubusercontent.com/20476674/209328479-4eaea5d8-aa4a-4f31-aa2a-b47e3c730f15.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>


<details><summary>&emsp;å›¾åƒç¼–è¾‘ï¼ˆImage Inpaintingï¼‰</summary>

#### image_inpainting-repaint
```python
from ppdiffusers import RePaintPipeline, RePaintScheduler
from ppdiffusers.utils import load_image

img_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/celeba_hq_256.png"
mask_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/mask_256.png"

# Load the original image and the mask as PIL images
original_image = load_image(img_url).resize((256, 256))
mask_image = load_image(mask_url).resize((256, 256))

scheduler = RePaintScheduler.from_pretrained("google/ddpm-ema-celebahq-256", subfolder="scheduler")
pipe = RePaintPipeline.from_pretrained("google/ddpm-ema-celebahq-256", scheduler=scheduler)

output = pipe(
    original_image=original_image,
    mask_image=mask_image,
    num_inference_steps=250,
    eta=0.0,
    jump_length=10,
    jump_n_sample=10,
)
inpainted_image = output.images[0]

inpainted_image.save("repaint-image.png")
```
<div align="center">
<img  alt="image" src="https://user-images.githubusercontent.com/20476674/209329052-b6fc2aaf-1a59-49a3-92ef-60180fdffd81.png">
<center>åŸå›¾åƒ</center>
<img  alt="image" src="https://user-images.githubusercontent.com/20476674/209329048-4fe12176-32a0-4800-98f2-49bd8d593799.png">
<center>maskå›¾åƒ</center>
<img  alt="image" src="https://user-images.githubusercontent.com/20476674/209329241-b7e4d99e-468a-4b95-8829-d77ee14bfe98.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>



<details><summary>&emsp;å›¾åƒå˜åŒ–ï¼ˆImage Variationï¼‰</summary>

#### image_variation-versatile_diffusion
```python
from ppdiffusers import VersatileDiffusionImageVariationPipeline
from ppdiffusers.utils import load_image

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/benz.jpg"
image = load_image(url)

pipe = VersatileDiffusionImageVariationPipeline.from_pretrained("shi-labs/versatile-diffusion")

image = pipe(image).images[0]
image.save("versatile-diffusion-car_variation.png")
```
<div align="center">
<img  width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209331434-51f6cdbd-b8e4-4faa-8e49-1cc852e35603.jpg">
<center>åŸå›¾åƒ</center>
<img  width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209331591-f6cc4cd8-8430-4627-8d22-bf404fb2bfdd.png">
<center>ç”Ÿæˆå›¾åƒ</center>
</div>
</details>





### éŸ³é¢‘
<details open>
<summary>&emsp;æ— æ¡ä»¶éŸ³é¢‘ç”Ÿæˆï¼ˆUnconditional Audio Generationï¼‰</summary>

#### unconditional_audio_generation-audio_diffusion

```python
from scipy.io.wavfile import write
from ppdiffusers import AudioDiffusionPipeline
import paddle

# åŠ è½½æ¨¡å‹å’Œscheduler
pipe = AudioDiffusionPipeline.from_pretrained("teticio/audio-diffusion-ddim-256")
pipe.set_progress_bar_config(disable=None)
generator = paddle.Generator().manual_seed(42)

output = pipe(generator=generator)
audio = output.audios[0]
image = output.images[0]

# ä¿å­˜éŸ³é¢‘åˆ°æœ¬åœ°
for i, audio in enumerate(audio):
    write(f"audio_diffusion_test{i}.wav", pipe.mel.sample_rate, audio.transpose())

# ä¿å­˜å›¾ç‰‡
image.save("audio_diffusion_test.png")
```
<div align = "center">
  <thead>
  </thead>
  <tbody>
   <tr>
      <td align = "center">
      <a href="https://paddlenlp.bj.bcebos.com/models/community/teticio/data/audio_diffusion_test0.wav" rel="nofollow">
            <img align="center" src="https://user-images.githubusercontent.com/20476674/209344877-edbf1c24-f08d-4e3b-88a4-a27e1fd0a858.png" width="200 style="max-width: 100%;"></a><br>
      </td>
    </tr>
  </tbody>
</div>

<div align="center">
<img  width="300" alt="image" src="https://user-images.githubusercontent.com/20476674/209342125-93e8715e-895b-4115-9e1e-e65c6c2cd95a.png">
</div>


#### unconditional_audio_generation-spectrogram_diffusion

```python
import paddle
import scipy

from ppdiffusers import MidiProcessor, SpectrogramDiffusionPipeline
from ppdiffusers.utils.download_utils import ppdiffusers_url_download

# Download MIDI from: wget https://paddlenlp.bj.bcebos.com/models/community/junnyu/develop/beethoven_hammerklavier_2.mid
mid_file_path = ppdiffusers_url_download(
    "https://paddlenlp.bj.bcebos.com/models/community/junnyu/develop/beethoven_hammerklavier_2.mid", cache_dir="."
)
pipe = SpectrogramDiffusionPipeline.from_pretrained("google/music-spectrogram-diffusion", paddle_dtype=paddle.float16)
processor = MidiProcessor()
output = pipe(processor(mid_file_path))
audio = output.audios[0]

output_path = "unconditional_audio_generation-spectrogram_diffusion-result-beethoven_hammerklavier_2.wav"
# save the audio sample as a .wav file
scipy.io.wavfile.write(output_path, rate=16000, data=audio)
```
<div align = "center">
  <thead>
  </thead>
  <tbody>
   <tr>
      <td align = "center">
      <a href="https://paddlenlp.bj.bcebos.com/models/community/westfish/develop_ppdiffusers_data/beethoven_hammerklavier_2.wav" rel="nofollow">
            <img align="center" src="https://user-images.githubusercontent.com/20476674/209344877-edbf1c24-f08d-4e3b-88a4-a27e1fd0a858.png" width="200 style="max-width: 100%;"></a><br>
      </td>
    </tr>
  </tbody>
</div>
</details>


## å®‰è£…

### ç¯å¢ƒä¾èµ–
```
pip install -r requirements.txt
```
å…³äºPaddlePaddleå®‰è£…çš„è¯¦ç»†æ•™ç¨‹è¯·æŸ¥çœ‹[Installation](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/develop/install/pip/linux-pip.html)ã€‚

### pipå®‰è£…

```shell
pip install --upgrade ppdiffusers
```

### æ‰‹åŠ¨å®‰è£…
```shell
git clone https://github.com/PaddlePaddle/PaddleNLP
# æ³¨æ„ï¼šå¦‚æœcloneä»“åº“éå¸¸æ…¢çš„è¯ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨é•œåƒç‰ˆæœ¬
# git clone https://gitee.com/paddlepaddle/PaddleNLP
cd PaddleNLP/ppdiffusers
python setup.py install
```

## å¿«é€Ÿå¼€å§‹
æˆ‘ä»¬å°†ä»¥æ‰©æ•£æ¨¡å‹çš„å…¸å‹ä»£è¡¨**Stable Diffusion**ä¸ºä¾‹ï¼Œå¸¦ä½ å¿«é€Ÿäº†è§£PPDiffusersã€‚

**Stable Diffusion**åŸºäº**æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼‰**ï¼Œä¸“é—¨ç”¨äº**æ–‡å›¾ç”Ÿæˆï¼ˆText-to-Image Generationï¼‰ä»»åŠ¡**ã€‚è¯¥æ¨¡å‹æ˜¯ç”±æ¥è‡ª [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [LAION](https://laion.ai/)ä»¥åŠ[RunwayML](https://runwayml.com/)çš„å·¥ç¨‹å¸ˆå…±åŒå¼€å‘å®Œæˆï¼Œç›®å‰å‘å¸ƒäº†v1å’Œv2ä¸¤ä¸ªç‰ˆæœ¬ã€‚v1ç‰ˆæœ¬é‡‡ç”¨äº†LAION-5Bæ•°æ®é›†å­é›†ï¼ˆåˆ†è¾¨ç‡ä¸º 512x512ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶å…·æœ‰ä»¥ä¸‹æ¶æ„è®¾ç½®ï¼šè‡ªåŠ¨ç¼–ç å™¨ä¸‹é‡‡æ ·å› å­ä¸º8ï¼ŒUNetå¤§å°ä¸º860Mï¼Œæ–‡æœ¬ç¼–ç å™¨ä¸ºCLIP ViT-L/14ã€‚v2ç‰ˆæœ¬ç›¸è¾ƒäºv1ç‰ˆæœ¬åœ¨ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œåˆ†è¾¨ç‡ç­‰è¿›è¡Œäº†æ”¹å–„ã€‚

### Stable Diffusioné‡ç‚¹æ¨¡å‹æƒé‡

<details><summary>&emsp; Stable Diffusion æ¨¡å‹æ”¯æŒçš„æƒé‡ï¼ˆè‹±æ–‡ï¼‰ </summary>

**æˆ‘ä»¬åªéœ€è¦å°†ä¸‹é¢çš„"xxxx"ï¼Œæ›¿æ¢æˆæ‰€éœ€çš„æƒé‡åï¼Œå³å¯å¿«é€Ÿä½¿ç”¨ï¼**
```python
from ppdiffusers import *

pipe_text2img = StableDiffusionPipeline.from_pretrained("xxxx")
pipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained("xxxx")
pipe_inpaint_legacy = StableDiffusionInpaintPipelineLegacy.from_pretrained("xxxx")
pipe_mega = StableDiffusionMegaPipeline.from_pretrained("xxxx")

# pipe_mega.text2img() ç­‰äº pipe_text2img()
# pipe_mega.img2img() ç­‰äº pipe_img2img()
# pipe_mega.inpaint_legacy() ç­‰äº pipe_inpaint_legacy()
```

| PPDiffusersæ”¯æŒçš„æ¨¡å‹åç§°                     | æ”¯æŒåŠ è½½çš„Pipeline                                    | å¤‡æ³¨ | huggingface.coåœ°å€ |
| :-------------------------------------------: | :--------------------------------------------------------------------: | --- | :-----------------------------------------: |
| CompVis/stable-diffusion-v1-4           | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | Stable-Diffusion-v1-4 ä½¿ç”¨ Stable-Diffusion-v1-2 çš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚éšååœ¨"laion-aesthetics v2 5+"æ•°æ®é›†ä¸Šä»¥ **512x512** åˆ†è¾¨ç‡å¾®è°ƒäº† **225k** æ­¥æ•°ï¼Œå¯¹æ–‡æœ¬ä½¿ç”¨äº† **10%** çš„dropoutï¼ˆå³ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ–‡å›¾å¯¹ä¸­çš„æ–‡æœ¬æœ‰ 10% çš„æ¦‚ç‡ä¼šå˜æˆç©ºæ–‡æœ¬ï¼‰ã€‚æ¨¡å‹ä½¿ç”¨äº†[CLIP ViT-L/14](https://huggingface.co/openai/clip-vit-large-patch14)ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ã€‚| [åœ°å€](https://huggingface.co/CompVis/stable-diffusion-v1-4) |
| CompVis/ldm-text2im-large-256               | LDMTextToImagePipeline | [LDMè®ºæ–‡](https://arxiv.org/pdf/2112.10752.pdf) LDM-KL-8-G* æƒé‡ã€‚| [åœ°å€](https://huggingface.co/CompVis/ldm-text2im-large-256) |
| CompVis/ldm-super-resolution-4x-openimages  | LDMSuperResolutionPipeline | [LDMè®ºæ–‡](https://arxiv.org/pdf/2112.10752.pdf) LDM-VQ-4 æƒé‡ï¼Œ[åŸå§‹æƒé‡é“¾æ¥](https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip)ã€‚| [åœ°å€](https://huggingface.co/CompVis/ldm-super-resolution-4x-openimages) |
| runwayml/stable-diffusion-v1-5              | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | Stable-Diffusion-v1-5 ä½¿ç”¨ Stable-Diffusion-v1-2 çš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚éšååœ¨"laion-aesthetics v2 5+"æ•°æ®é›†ä¸Šä»¥ **512x512** åˆ†è¾¨ç‡å¾®è°ƒäº† **595k** æ­¥æ•°ï¼Œå¯¹æ–‡æœ¬ä½¿ç”¨äº† **10%** çš„dropoutï¼ˆå³ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æ–‡å›¾å¯¹ä¸­çš„æ–‡æœ¬æœ‰ 10% çš„æ¦‚ç‡ä¼šå˜æˆç©ºæ–‡æœ¬ï¼‰ã€‚æ¨¡å‹åŒæ ·ä¹Ÿä½¿ç”¨äº†[CLIP ViT-L/14](https://huggingface.co/openai/clip-vit-large-patch14)ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ã€‚| [åœ°å€](https://huggingface.co/runwayml/stable-diffusion-v1-5) |
| runwayml/stable-diffusion-inpainting        | StableDiffusionInpaintPipeline | Stable-Diffusion-Inpainting ä½¿ç”¨ Stable-Diffusion-v1-2 çš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚é¦–å…ˆè¿›è¡Œäº† **595k** æ­¥çš„å¸¸è§„è®­ç»ƒï¼ˆå®é™…ä¹Ÿå°±æ˜¯ Stable-Diffusion-v1-5 çš„æƒé‡ï¼‰ï¼Œç„¶åè¿›è¡Œäº† **440k** æ­¥çš„ inpainting ä¿®å¤è®­ç»ƒã€‚å¯¹äº inpainting ä¿®å¤è®­ç»ƒï¼Œç»™ UNet é¢å¤–å¢åŠ äº† **5** è¾“å…¥é€šé“ï¼ˆå…¶ä¸­ **4** ä¸ªç”¨äºè¢« Mask é®ç›–ä½çš„å›¾ç‰‡ï¼Œ**1** ä¸ªç”¨äº Mask æœ¬èº«ï¼‰ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œä¼šéšæœºç”Ÿæˆ Maskï¼Œå¹¶æœ‰ **25%** æ¦‚ç‡ä¼šå°†åŸå§‹å›¾ç‰‡å…¨éƒ¨ Mask æ‰ã€‚| [åœ°å€](https://huggingface.co/runwayml/stable-diffusion-inpainting) |
| stabilityai/stable-diffusion-2-base         | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | è¯¥æ¨¡å‹é¦–å…ˆåœ¨ [LAION-5B 256x256 å­é›†ä¸Š](https://laion.ai/blog/laion-5b/) ï¼ˆè¿‡æ»¤æ¡ä»¶ï¼š[punsafe = 0.1 çš„ LAION-NSFW åˆ†ç±»å™¨](https://github.com/LAION-AI/CLIP-based-NSFW-Detector) å’Œ å®¡ç¾åˆ†æ•°å¤§äºç­‰äº 4.5 ï¼‰ä»å¤´å¼€å§‹è®­ç»ƒ **550k** æ­¥ï¼Œç„¶ååˆåœ¨åˆ†è¾¨ç‡ **>= 512x512** çš„åŒä¸€æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒ **850k** æ­¥ã€‚| [åœ°å€](https://huggingface.co/stabilityai/stable-diffusion-2-base) |
| stabilityai/stable-diffusion-2              | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | stable-diffusion-2 ä½¿ç”¨ stable-diffusion-2-base æƒé‡è¿›è¡Œåˆå§‹åŒ–ï¼Œé¦–å…ˆåœ¨åŒä¸€æ•°æ®é›†ä¸Šï¼ˆ**512x512** åˆ†è¾¨ç‡ï¼‰ä½¿ç”¨ [v-objective](https://arxiv.org/abs/2202.00512) è®­ç»ƒäº† **150k** æ­¥ã€‚ç„¶ååˆåœ¨ **768x768** åˆ†è¾¨ç‡ä¸Šä½¿ç”¨ [v-objective](https://arxiv.org/abs/2202.00512) ç»§ç»­è®­ç»ƒäº† **140k** æ­¥ã€‚| [åœ°å€](https://huggingface.co/stabilityai/stable-diffusion-2) |
| stabilityai/stable-diffusion-2-inpainting   | StableDiffusionInpaintPipeline |stable-diffusion-2-inpainting ä½¿ç”¨ stable-diffusion-2-base æƒé‡åˆå§‹åŒ–ï¼Œå¹¶ä¸”é¢å¤–è®­ç»ƒäº† **200k** æ­¥ã€‚è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨äº† [LAMA](https://github.com/saic-mdal/lama) ä¸­æå‡ºçš„ Mask ç”Ÿæˆç­–ç•¥ï¼Œå¹¶ä¸”ä½¿ç”¨ Mask å›¾ç‰‡çš„ Latent è¡¨ç¤ºï¼ˆç»è¿‡ VAE ç¼–ç ï¼‰ä½œä¸ºé™„åŠ æ¡ä»¶ã€‚| [åœ°å€](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) |
| stabilityai/stable-diffusion-x4-upscaler    | StableDiffusionUpscalePipeline | è¯¥æ¨¡å‹åœ¨**LAION 10M** å­é›†ä¸Šï¼ˆ>2048x2048ï¼‰è®­ç»ƒäº† 1.25M æ­¥ã€‚è¯¥æ¨¡å‹è¿˜åœ¨åˆ†è¾¨ç‡ä¸º **512x512** çš„å›¾åƒä¸Šä½¿ç”¨ [Text-guided Latent Upscaling Diffusion Model](https://arxiv.org/abs/2112.10752) è¿›è¡Œäº†è®­ç»ƒã€‚é™¤äº†**æ–‡æœ¬è¾“å…¥**ä¹‹å¤–ï¼Œå®ƒè¿˜æ¥æ”¶ **noise_level** ä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [é¢„å®šä¹‰çš„ Scheduler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler/blob/main/low_res_scheduler/scheduler_config.json) å‘ä½åˆ†è¾¨ç‡çš„è¾“å…¥å›¾ç‰‡æ·»åŠ å™ªå£°ã€‚| [åœ°å€](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler) |
| hakurei/waifu-diffusion    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | waifu-diffusion-v1-2 ä½¿ç”¨ stable-diffusion-v1-4 æƒé‡åˆå§‹åŒ–ï¼Œå¹¶ä¸”åœ¨**é«˜è´¨é‡åŠ¨æ¼«**å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåå¾—åˆ°çš„æ¨¡å‹ã€‚ç”¨äºå¾®è°ƒçš„æ•°æ®æ˜¯ **680k** æ–‡æœ¬å›¾åƒæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ˜¯é€šè¿‡ **booru ç½‘ç«™** ä¸‹è½½çš„ã€‚| [åœ°å€](https://huggingface.co/hakurei/waifu-diffusion) |
| hakurei/waifu-diffusion-v1-3    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | waifu-diffusion-v1-3 æ˜¯ waifu-diffusion-v1-2 åŸºç¡€ä¸Šè¿›ä¸€æ­¥è®­ç»ƒå¾—åˆ°çš„ã€‚ä»–ä»¬å¯¹æ•°æ®é›†è¿›è¡Œäº†é¢å¤–æ“ä½œï¼šï¼ˆ1ï¼‰åˆ é™¤ä¸‹åˆ’çº¿ï¼›ï¼ˆ2ï¼‰åˆ é™¤æ‹¬å·ï¼›ï¼ˆ3ï¼‰ç”¨é€—å·åˆ†éš”æ¯ä¸ªbooru æ ‡ç­¾ï¼›ï¼ˆ4ï¼‰éšæœºåŒ–æ ‡ç­¾é¡ºåºã€‚| [åœ°å€](https://huggingface.co/hakurei/waifu-diffusion) |
| naclbit/trinart_stable_diffusion_v2_60k    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | trinart_stable_diffusion ä½¿ç”¨ stable-diffusion-v1-4 æƒé‡åˆå§‹åŒ–ï¼Œåœ¨ 40k **é«˜åˆ†è¾¨ç‡æ¼«ç”»/åŠ¨æ¼«é£æ ¼**çš„å›¾ç‰‡æ•°æ®é›†ä¸Šå¾®è°ƒäº† 8 ä¸ª epochã€‚V2 ç‰ˆæ¨¡å‹ä½¿ç”¨ **dropouts**ã€**10k+ å›¾åƒ**å’Œ**æ–°çš„æ ‡è®°ç­–ç•¥**è®­ç»ƒäº†**æ›´é•¿æ—¶é—´**ã€‚| [åœ°å€](https://huggingface.co/naclbit/trinart_stable_diffusion_v2) |
| naclbit/trinart_stable_diffusion_v2_95k    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | **95k** æ­¥æ•°çš„ç»“æœï¼Œå…¶ä»–åŒä¸Šã€‚| [åœ°å€](https://huggingface.co/naclbit/trinart_stable_diffusion_v2) |
| naclbit/trinart_stable_diffusion_v2_115k    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | **115k** æ­¥æ•°çš„ç»“æœï¼Œå…¶ä»–åŒä¸Šã€‚| [åœ°å€](https://huggingface.co/naclbit/trinart_stable_diffusion_v2) |
| Deltaadams/Hentai-Diffusion    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | None| [åœ°å€](https://huggingface.co/Deltaadams/Hentai-Diffusion) |
| ringhyacinth/nail-set-diffuser    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | ç¾ç”²é¢†åŸŸçš„æ‰©æ•£æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä½¿ç”¨äº† [Weekend](https://weibo.com/u/5982308498)| [åœ°å€](https://huggingface.co/ringhyacinth/nail-set-diffuser) |
| Linaqruf/anything-v3.0    | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | è¯¥æ¨¡å‹å¯é€šè¿‡è¾“å…¥å‡ ä¸ªæ–‡æœ¬æç¤ºè¯å°±èƒ½ç”Ÿæˆ**é«˜è´¨é‡ã€é«˜åº¦è¯¦ç»†çš„åŠ¨æ¼«é£æ ¼å›¾ç‰‡**ï¼Œè¯¥æ¨¡å‹æ”¯æŒä½¿ç”¨ **danbooru æ ‡ç­¾æ–‡æœ¬** ç”Ÿæˆå›¾åƒã€‚| [åœ°å€](https://huggingface.co/Linaqruf/anything-v3.0) |

</details>
<details><summary>&emsp; Stable Diffusion æ¨¡å‹æ”¯æŒçš„æƒé‡ï¼ˆä¸­æ–‡å’Œå¤šè¯­è¨€ï¼‰ </summary>


| PPDiffusersæ”¯æŒçš„æ¨¡å‹åç§°                     | æ”¯æŒåŠ è½½çš„Pipeline                                    | å¤‡æ³¨ | huggingface.coåœ°å€ |
| :-------------------------------------------: | :--------------------------------------------------------------------: | --- | :-----------------------------------------: |
| BAAI/AltDiffusion                           | AltDiffusionPipelineã€AltDiffusionImg2ImgPipeline | è¯¥æ¨¡å‹ä½¿ç”¨ [AltCLIP](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md) ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œåœ¨ Stable Diffusion åŸºç¡€ä¸Šè®­ç»ƒäº†**åŒè¯­Diffusionæ¨¡å‹**ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ®æ¥è‡ª [WuDaoæ•°æ®é›†](https://data.baai.ac.cn/details/WuDaoCorporaText) å’Œ [LAION](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6plus) ã€‚| [åœ°å€](https://huggingface.co/BAAI/AltDiffusion) |
| BAAI/AltDiffusion-m9                        | AltDiffusionPipelineã€AltDiffusionImg2ImgPipeline |è¯¥æ¨¡å‹ä½¿ç”¨9ç§è¯­è¨€çš„ [AltCLIP-m9](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md) ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œå…¶ä»–åŒä¸Šã€‚| [åœ°å€](https://huggingface.co/BAAI/AltDiffusion-m9) |
| IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1 | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | ä»–ä»¬å°† [Noah-Wukong](https://wukong-dataset.github.io/wukong-dataset/) æ•°æ®é›† (100M) å’Œ [Zero](https://zero.so.com/) æ•°æ®é›† (23M) ç”¨ä½œé¢„è®­ç»ƒçš„æ•°æ®é›†ï¼Œå…ˆç”¨ [IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese](https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese) å¯¹è¿™ä¸¤ä¸ªæ•°æ®é›†çš„å›¾æ–‡å¯¹ç›¸ä¼¼æ€§è¿›è¡Œæ‰“åˆ†ï¼Œå– CLIP Score å¤§äº 0.2 çš„å›¾æ–‡å¯¹ä½œä¸ºè®­ç»ƒé›†ã€‚ ä»–ä»¬ä½¿ç”¨ [IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese](https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese) ä½œä¸ºåˆå§‹åŒ–çš„text encoderï¼Œå†»ä½ [stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) ([è®ºæ–‡](https://arxiv.org/abs/2112.10752)) æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ï¼Œåªè®­ç»ƒ text encoderï¼Œä»¥ä¾¿ä¿ç•™åŸå§‹æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸”å®ç°ä¸­æ–‡æ¦‚å¿µçš„å¯¹é½ã€‚è¯¥æ¨¡å‹ç›®å‰åœ¨0.2äº¿å›¾æ–‡å¯¹ä¸Šè®­ç»ƒäº†ä¸€ä¸ª epochã€‚ åœ¨ 32 x A100 ä¸Šè®­ç»ƒäº†å¤§çº¦100å°æ—¶ï¼Œè¯¥ç‰ˆæœ¬åªæ˜¯ä¸€ä¸ªåˆæ­¥çš„ç‰ˆæœ¬ã€‚| [åœ°å€](https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1) |
| IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1 | StableDiffusionPipelineã€StableDiffusionImg2ImgPipelineã€StableDiffusionInpaintPipelineLegacyã€StableDiffusionMegaPipelineã€StableDiffusionPipelineAllinOne | ä»–ä»¬å°† [Noah-Wukong](https://wukong-dataset.github.io/wukong-dataset/) æ•°æ®é›† (100M) å’Œ [Zero](https://zero.so.com/) æ•°æ®é›† (23M) ç”¨ä½œé¢„è®­ç»ƒçš„æ•°æ®é›†ï¼Œå…ˆç”¨ [IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese](https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese) å¯¹è¿™ä¸¤ä¸ªæ•°æ®é›†çš„å›¾æ–‡å¯¹ç›¸ä¼¼æ€§è¿›è¡Œæ‰“åˆ†ï¼Œå– CLIP Score å¤§äº 0.2 çš„å›¾æ–‡å¯¹ä½œä¸ºè®­ç»ƒé›†ã€‚ ä»–ä»¬ä½¿ç”¨ [stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) ([è®ºæ–‡](https://arxiv.org/abs/2112.10752)) æ¨¡å‹è¿›è¡Œç»§ç»­è®­ç»ƒï¼Œå…¶ä¸­è®­ç»ƒåˆ†ä¸º**ä¸¤ä¸ªstage**ã€‚**ç¬¬ä¸€ä¸ªstage** ä¸­å†»ä½æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ï¼Œåªè®­ç»ƒ text encoder ï¼Œä»¥ä¾¿ä¿ç•™åŸå§‹æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸”å®ç°ä¸­æ–‡æ¦‚å¿µçš„å¯¹é½ã€‚**ç¬¬äºŒä¸ªstage** ä¸­å°†å…¨éƒ¨æ¨¡å‹è§£å†»ï¼Œä¸€èµ·è®­ç»ƒ text encoder å’Œ diffusion model ï¼Œä»¥ä¾¿ diffusion model æ›´å¥½çš„é€‚é…ä¸­æ–‡å¼•å¯¼ã€‚ç¬¬ä¸€ä¸ª stage ä»–ä»¬è®­ç»ƒäº† 80 å°æ—¶ï¼Œç¬¬äºŒä¸ª stage è®­ç»ƒäº† 100 å°æ—¶ï¼Œä¸¤ä¸ªstageéƒ½æ˜¯ç”¨äº†8 x A100ï¼Œè¯¥ç‰ˆæœ¬æ˜¯ä¸€ä¸ªåˆæ­¥çš„ç‰ˆæœ¬ã€‚| [åœ°å€](https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-EN-v0.1) |
</details>


### åŠ è½½HF Diffusersæƒé‡
```python
from ppdiffusers import StableDiffusionPipeline
# è®¾ç½®from_hf_hubä¸ºTrueï¼Œè¡¨ç¤ºä»huggingface hubä¸‹è½½ï¼Œfrom_diffusersä¸ºTrueè¡¨ç¤ºåŠ è½½çš„æ˜¯diffusersç‰ˆPytorchæƒé‡
pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2", from_hf_hub=True, from_diffusers=True)
```

### åŠ è½½åŸåº“çš„Lightningæƒé‡
```python
from ppdiffusers import StableDiffusionPipeline
# å¯è¾“å…¥ç½‘å€ æˆ– æœ¬åœ°ckptã€safetensorsæ–‡ä»¶
pipe = StableDiffusionPipeline.from_pretrained_original_ckpt("https://paddlenlp.bj.bcebos.com/models/community/junnyu/develop/ppdiffusers/chilloutmix_NiPrunedFp32Fix.safetensors")
```

### åŠ è½½Civitaiç¤¾åŒºçš„LoRAæƒé‡
```python
from ppdiffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained("TASUKU2023/Chilloutmix")
# åŠ è½½loraæƒé‡
pipe.apply_lora("https://paddlenlp.bj.bcebos.com/models/community/junnyu/develop/ppdiffusers/Moxin_10.safetensors")
```

### XFormersåŠ é€Ÿ
ä¸ºäº†ä½¿ç”¨**XFormersåŠ é€Ÿ**ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…`develop`ç‰ˆæœ¬çš„`paddle`ï¼ŒLinuxç³»ç»Ÿçš„å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š
```sh
python -m pip install paddlepaddle-gpu==0.0.0.post117 -f https://www.paddlepaddle.org.cn/whl/linux/gpu/develop.html
```

```python
import paddle
from ppdiffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained("TASUKU2023/Chilloutmix", paddle_dtype=paddle.float16)
# å¼€å¯xformersåŠ é€Ÿ é»˜è®¤é€‰æ‹©"cutlass"åŠ é€Ÿ
pipe.enable_xformers_memory_efficient_attention()
# flash éœ€è¦ä½¿ç”¨ A100ã€A10ã€3060ã€3070ã€3080ã€3090 ç­‰ä»¥ä¸Šæ˜¾å¡ã€‚
# pipe.enable_xformers_memory_efficient_attention("flash")
```

### ToME + ControlNet
```python
# å®‰è£…developçš„ppdiffusers
# pip install "ppdiffusers>=0.16.1"
import paddle
from ppdiffusers import ControlNetModel, StableDiffusionControlNetPipeline
from ppdiffusers.utils import load_image

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", safety_checker=None, controlnet=controlnet, paddle_dtype=paddle.float16
)

# Apply ToMe with a 50% merging ratio
pipe.apply_tome(ratio=0.5) # Can also use pipe.unet in place of pipe here

# æˆ‘ä»¬å¯ä»¥å¼€å¯ xformers
# pipe.enable_xformers_memory_efficient_attention()
generator = paddle.Generator().manual_seed(0)
prompt = "bird"
image = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png"
)

image = pipe(prompt, image, generator=generator).images[0]

image.save("bird.png")
```

### æ–‡å›¾ç”Ÿæˆ ï¼ˆText-to-Image Generationï¼‰

```python
import paddle
from ppdiffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2")

# è®¾ç½®éšæœºç§å­ï¼Œæˆ‘ä»¬å¯ä»¥å¤ç°ä¸‹é¢çš„ç»“æœï¼
paddle.seed(5232132133)
prompt = "a portrait of shiba inu with a red cap growing on its head. intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
image = pipe(prompt, guidance_scale=7.5, height=768, width=768).images[0]

image.save("shiba_dog_with_a_red_cap.png")
```
<div align="center">
<img width="500" alt="image" src="https://user-images.githubusercontent.com/50394665/204796701-d7911f76-8670-47d5-8d1b-8368b046c5e4.png">
</div>

### æ–‡æœ¬å¼•å¯¼çš„å›¾åƒå˜æ¢ï¼ˆImage-to-Image Text-Guided Generationï¼‰

<details><summary>&emsp;Image-to-Image Text-Guided Generation Demo </summary>

```python
import paddle
from ppdiffusers import StableDiffusionImg2ImgPipeline
from ppdiffusers.utils import load_image

pipe = StableDiffusionImg2ImgPipeline.from_pretrained("Linaqruf/anything-v3.0", safety_checker=None)

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/image_Kurisu.png"
image = load_image(url).resize((512, 768))

# è®¾ç½®éšæœºç§å­ï¼Œæˆ‘ä»¬å¯ä»¥å¤ç°ä¸‹é¢çš„ç»“æœï¼
paddle.seed(42)
prompt = "Kurisu Makise, looking at viewer, long hair, standing, 1girl, hair ornament, hair flower, cute, jacket, white flower, white dress"
negative_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"

image = pipe(prompt=prompt, negative_prompt=negative_prompt, image=image, strength=0.75, guidance_scale=7.5).images[0]
image.save("image_Kurisu_img2img.png")
```
<div align="center">
<img width="500" alt="image" src="https://user-images.githubusercontent.com/50394665/204799529-cd89dcdb-eb1d-4247-91ac-b0f7bad777f8.png">
</div>
</details>

### æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼ˆText-Guided Image Inpaintingï¼‰

æ³¨æ„ï¼å½“å‰æœ‰ä¸¤ç§ç‰ˆæœ¬çš„å›¾åƒç¼–è¾‘ä»£ç ï¼Œä¸€ä¸ªæ˜¯Legacyç‰ˆæœ¬ï¼Œä¸€ä¸ªæ˜¯æ­£å¼ç‰ˆæœ¬ï¼Œä¸‹é¢å°†åˆ†åˆ«ä»‹ç»ä¸¤ç§ä»£ç å¦‚ä½•ä½¿ç”¨ï¼

<details><summary>&emsp;Legacyç‰ˆæœ¬ä»£ç </summary>

```python
import paddle
from ppdiffusers import StableDiffusionInpaintPipelineLegacy
from ppdiffusers.utils import load_image

# å¯é€‰æ¨¡å‹æƒé‡
# CompVis/stable-diffusion-v1-4
# runwayml/stable-diffusion-v1-5
# stabilityai/stable-diffusion-2-base ï¼ˆåŸå§‹ç­–ç•¥ 512x512ï¼‰
# stabilityai/stable-diffusion-2 ï¼ˆv-objective 768x768ï¼‰
# Linaqruf/anything-v3.0
# ......
img_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations.png"
mask_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations-mask.png"

image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

pipe = StableDiffusionInpaintPipelineLegacy.from_pretrained("stabilityai/stable-diffusion-2-base", safety_checker=None)

# è®¾ç½®éšæœºç§å­ï¼Œæˆ‘ä»¬å¯ä»¥å¤ç°ä¸‹é¢çš„ç»“æœï¼
paddle.seed(10245)
prompt = "a red cat sitting on a bench"
image = pipe(prompt=prompt, image=image, mask_image=mask_image, strength=0.75).images[0]

image.save("a_red_cat_legacy.png")
```
<div align="center">
<img width="900" alt="image" src="https://user-images.githubusercontent.com/50394665/204802186-5a6d302b-83aa-4247-a5bb-ebabfcc3abc4.png">
</div>

</details>

<details><summary>&emsp;æ­£å¼ç‰ˆæœ¬ä»£ç </summary>

Tips: ä¸‹é¢çš„ä½¿ç”¨æ–¹æ³•æ˜¯æ–°ç‰ˆæœ¬çš„ä»£ç ï¼Œä¹Ÿæ˜¯å®˜æ–¹æ¨èçš„ä»£ç ï¼Œæ³¨æ„å¿…é¡»é…åˆ **runwayml/stable-diffusion-inpainting** å’Œ **stabilityai/stable-diffusion-2-inpainting** æ‰å¯æ­£å¸¸ä½¿ç”¨ã€‚
```python
import paddle
from ppdiffusers import StableDiffusionInpaintPipeline
from ppdiffusers.utils import load_image

# å¯é€‰æ¨¡å‹æƒé‡
# runwayml/stable-diffusion-inpainting
# stabilityai/stable-diffusion-2-inpainting
img_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations.png"
mask_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations-mask.png"

image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

pipe = StableDiffusionInpaintPipeline.from_pretrained("stabilityai/stable-diffusion-2-inpainting")

# è®¾ç½®éšæœºç§å­ï¼Œæˆ‘ä»¬å¯ä»¥å¤ç°ä¸‹é¢çš„ç»“æœï¼
paddle.seed(1024)
prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]

image.save("a_yellow_cat.png")
```
<div align="center">
<img width="900" alt="image" src="https://user-images.githubusercontent.com/50394665/204801946-6cd043bc-f3db-42cf-82cd-6a6171484523.png">
</div>
</details>

### æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ”¾å¤§ & è¶…åˆ†ï¼ˆText-Guided Image Upscaling & Super-Resolutionï¼‰

<details><summary>&emsp;Text-Guided Image Upscaling Demo</summary>

```python
import paddle
from ppdiffusers import StableDiffusionUpscalePipeline
from ppdiffusers.utils import load_image

pipe = StableDiffusionUpscalePipeline.from_pretrained("stabilityai/stable-diffusion-x4-upscaler")

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/low_res_cat.png"
# æˆ‘ä»¬äººå·¥å°†åŸå§‹å›¾ç‰‡ç¼©å°æˆ 128x128 åˆ†è¾¨ç‡ï¼Œæœ€ç»ˆä¿å­˜çš„å›¾ç‰‡ä¼šæ”¾å¤§4å€ï¼
low_res_img = load_image(url).resize((128, 128))

prompt = "a white cat"
image = pipe(prompt=prompt, image=low_res_img).images[0]

image.save("upscaled_white_cat.png")
```
<div align="center">
<img width="200" alt="image" src="https://user-images.githubusercontent.com/50394665/204806180-b7f1b9cf-8a62-4577-b5c4-91adda08a13b.png">
<img width="400" alt="image" src="https://user-images.githubusercontent.com/50394665/204806202-8c110be3-5f48-4946-95ea-21ad5a9a2340.png">
</div>
</details>

<details><summary>&emsp;Super-Resolution Demo</summary>

```python
import paddle
from ppdiffusers import LDMSuperResolutionPipeline
from ppdiffusers.utils import load_image

pipe = LDMSuperResolutionPipeline.from_pretrained("CompVis/ldm-super-resolution-4x-openimages")

url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations.png"

# æˆ‘ä»¬äººå·¥å°†åŸå§‹å›¾ç‰‡ç¼©å°æˆ 128x128 åˆ†è¾¨ç‡ï¼Œæœ€ç»ˆä¿å­˜çš„å›¾ç‰‡ä¼šæ”¾å¤§4å€ï¼
low_res_img = load_image(url).resize((128, 128))

image = pipe(image=low_res_img, num_inference_steps=100).images[0]

image.save("ldm-super-resolution-image.png")
```
<div align="center">
<img width="200" alt="image" src="https://user-images.githubusercontent.com/50394665/204804426-5e28b571-aa41-4f56-ba26-68cca75fdaae.png">
<img width="400" alt="image" src="https://user-images.githubusercontent.com/50394665/204804148-fe7c293b-6cd7-4942-ae9c-446369fe8410.png">
</div>

</details>

## æ¨¡å‹æ¨ç†éƒ¨ç½²
é™¤äº†**PaddleåŠ¨æ€å›¾**è¿è¡Œä¹‹å¤–ï¼Œå¾ˆå¤šæ¨¡å‹è¿˜æ”¯æŒå°†æ¨¡å‹å¯¼å‡ºå¹¶ä½¿ç”¨æ¨ç†å¼•æ“è¿è¡Œã€‚æˆ‘ä»¬æä¾›åŸºäº[FastDeploy](https://github.com/PaddlePaddle/FastDeploy)ä¸Šçš„**StableDiffusion**æ¨¡å‹éƒ¨ç½²ç¤ºä¾‹ï¼Œæ¶µç›–æ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ã€å›¾åƒç¼–è¾‘ç­‰ä»»åŠ¡ï¼Œç”¨æˆ·å¯ä»¥æŒ‰ç…§æˆ‘ä»¬æä¾›[StableDiffusionæ¨¡å‹å¯¼å‡ºæ•™ç¨‹](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/deploy/export.md)å°†æ¨¡å‹å¯¼å‡ºï¼Œç„¶åä½¿ç”¨`FastDeployStableDiffusionMegaPipeline`è¿›è¡Œé«˜æ€§èƒ½æ¨ç†éƒ¨ç½²ï¼

<details><summary>&emsp; å·²é¢„å…ˆå¯¼å‡ºçš„FastDeployç‰ˆStable Diffusionæƒé‡ </summary>

**æ³¨æ„ï¼šå½“å‰å¯¼å‡ºçš„vae encoderå¸¦æœ‰éšæœºå› ç´ ï¼**

- CompVis/stable-diffusion-v1-4@fastdeploy
- runwayml/stable-diffusion-v1-5@fastdeploy
- runwayml/stable-diffusion-inpainting@fastdeploy
- stabilityai/stable-diffusion-2-base@fastdeploy
- stabilityai/stable-diffusion-2@fastdeploy
- stabilityai/stable-diffusion-2-inpainting@fastdeploy
- Linaqruf/anything-v3.0@fastdeploy
- hakurei/waifu-diffusion-v1-3@fastdeploy

</details>

<details><summary>&emsp; FastDeploy Demo </summary>

```python
import paddle
import fastdeploy as fd
from ppdiffusers import FastDeployStableDiffusionMegaPipeline
from ppdiffusers.utils import load_image

def create_runtime_option(device_id=0, backend="paddle", use_cuda_stream=True):
    option = fd.RuntimeOption()
    if backend == "paddle":
        option.use_paddle_backend()
    else:
        option.use_ort_backend()
    if device_id == -1:
        option.use_cpu()
    else:
        option.use_gpu(device_id)
        if use_cuda_stream:
            paddle_stream = paddle.device.cuda.current_stream(device_id).cuda_stream
            option.set_external_raw_stream(paddle_stream)
    return option

runtime_options = {
    "text_encoder": create_runtime_option(0, "paddle"),  # use gpu:0
    "vae_encoder": create_runtime_option(0, "paddle"),  # use gpu:0
    "vae_decoder": create_runtime_option(0, "paddle"),  # use gpu:0
    "unet": create_runtime_option(0, "paddle"),  # use gpu:0
}

fd_pipe = FastDeployStableDiffusionMegaPipeline.from_pretrained(
    "Linaqruf/anything-v3.0@fastdeploy", runtime_options=runtime_options
)

# text2img
prompt = "a portrait of shiba inu with a red cap growing on its head. intricate. lifelike. soft light. sony a 7 r iv 5 5 mm. cinematic post - processing "
image_text2img = fd_pipe.text2img(prompt=prompt, num_inference_steps=50).images[0]
image_text2img.save("image_text2img.png")

# img2img
url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/data/image_Kurisu.png"
image = load_image(url).resize((512, 512))
prompt = "Kurisu Makise, looking at viewer, long hair, standing, 1girl, hair ornament, hair flower, cute, jacket, white flower, white dress"
negative_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"

image_img2img = fd_pipe.img2img(
    prompt=prompt, negative_prompt=negative_prompt, image=image, strength=0.75, guidance_scale=7.5
).images[0]
image_img2img.save("image_img2img.png")

# inpaint_legacy
img_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations.png"
mask_url = "https://paddlenlp.bj.bcebos.com/models/community/CompVis/stable-diffusion-v1-4/overture-creations-mask.png"
image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))
prompt = "a red cat sitting on a bench"

image_inpaint_legacy = fd_pipe.inpaint_legacy(
    prompt=prompt, image=image, mask_image=mask_image, strength=0.75, num_inference_steps=50
).images[0]
image_inpaint_legacy.save("image_inpaint_legacy.png")
```
</details>
<div align="center">
<img width="900" alt="image" src="https://user-images.githubusercontent.com/50394665/205297240-46b80992-34af-40cd-91a6-ae76589d0e21.png">
</div>



## License
PPDiffusers éµå¾ª [Apache-2.0å¼€æºåè®®](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/ppdiffusers/LICENSE)ã€‚

Stable Diffusion éµå¾ª [The CreativeML OpenRAIL M å¼€æºåè®®](https://huggingface.co/spaces/CompVis/stable-diffusion-license)ã€‚
> The CreativeML OpenRAIL M is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which this license is based.

## Acknowledge
æˆ‘ä»¬å€Ÿé‰´äº†ğŸ¤— Hugging Faceçš„[Diffusers](https://github.com/huggingface/diffusers)å…³äºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä½¿ç”¨çš„ä¼˜ç§€è®¾è®¡ï¼Œåœ¨æ­¤å¯¹Hugging Faceä½œè€…åŠå…¶å¼€æºç¤¾åŒºè¡¨ç¤ºæ„Ÿè°¢ã€‚


## Credits
This library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:
- @huggingface' diffusers library, available [here](https://github.com/huggingface/diffusers)
- @CompVis' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)
- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)
- @ermongroup's DDIM implementation, available [here](https://github.com/ermongroup/ddim).
- @yang-song's Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)

We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.

## Citation

```bibtex
@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
```
