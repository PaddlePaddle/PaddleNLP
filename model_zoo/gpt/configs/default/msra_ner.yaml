model_name_or_path: gpt-cpm-small-cn-distill
max_seq_length: 128
per_device_eval_batch_size: 32
learning_rate: 2e-5
num_train_epochs: 3
logging_steps: 25
save_steps: 250
output_dir: ./tmp/msra_ner/
device: gpu 
do_train: true