# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging
import os
import re
import time

import arxiv
import erniebot as eb
import gradio as gr
from utils import (
    _apply_token,
    get_parse_args,
    merge_summary,
    pdf2image,
    retrieval,
    summarize_abstract,
    tackle_history,
    translate_part,
)

FORMAT = "%(asctime)s-%(levelname)s: %(message)s"
logging.basicConfig(level=logging.INFO, format=FORMAT)
logger = logging.getLogger(__name__)
# create file handler which logs even debug messages
fh = logging.FileHandler("logger.log")
fh.setLevel(logging.DEBUG)
logger.addHandler(fh)

args = get_parse_args()
PROMPT_SYSTEM = """
ä½ ç°åœ¨éœ€è¦ä¸€æ­¥æ­¥æ‰§è¡Œä¸‹é¢çš„æ“ä½œ
ä½ éœ€è¦å…ˆå®Œæˆå…³é”®å¥æŠ½å–ä»»åŠ¡ï¼Œä»èƒŒæ™¯ä¿¡æ¯ä¸­æŠ½å–ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„å…³é”®å¥ï¼Œ
å¹¶è¾“å‡ºä¿¡å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ,å…³é”®å¥éœ€è¦æŒ‰ç…§1,2,3,...åºå·ç¼–å·ï¼Œç„¶åä½ éœ€è¦åŸºäºæŠ½å–çš„å†…å®¹å®Œæˆé—®ç­”ä»»åŠ¡,å›ç­”è¾“å‡ºé—®é¢˜ã€‚
è¯·è®°ä½ä½ è¾“å‡ºçš„æ ¼å¼æ˜¯ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
jsonæœ‰ä¸¤ä¸ªkeyå€¼ï¼Œä¸€ä¸ªæ˜¯"å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ",å¯¹åº”çš„valueæ˜¯å…³é”®å¥éœ€è¦æŒ‰ç…§1,2,3,...åºå·ç¼–å·åçš„ç»“æœï¼Œç¬¬äºŒä¸ªæ˜¯"é—®ç­”ä»»åŠ¡çš„ç»“æœ",å¯¹åº”çš„valueæ˜¯é—®ç­”ä»»åŠ¡çš„ç»“æœã€‚
è¾“å‡ºæ ¼å¼å¦‚ä¸‹:
```
json{'å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ':'1.å…³é”®å¥1 \n2.å…³é”®å¥1\n...','é—®ç­”ä»»åŠ¡çš„ç»“æœ':''}
```
"""
PROMPT_RETRIVER = """
ç°åœ¨æˆ‘ç»™ä½ èƒŒæ™¯ä¿¡æ¯å’Œé—®é¢˜ï¼š
èƒŒæ™¯ä¿¡æ¯ï¼š{documents}
è¾“å…¥é—®é¢˜ï¼š{query}
æ ¹æ®èƒŒæ™¯ä¿¡æ¯ï¼Œæ¥å®Œæˆå…³é”®å¥æŠ½å–ä»»åŠ¡å’Œé—®ç­”ä»»åŠ¡ã€‚
è¯·è®°ä½ä½ éœ€è¦å…ˆæ‰§è¡Œå…³é”®å¥æŠ½å–ä»»åŠ¡ï¼Œå†æ‰§è¡Œé—®ç­”ä»»åŠ¡ã€‚ä½ çš„è¾“å‡ºæ ¼å¼éœ€è¦æ˜¯ä¸€ä¸ªjsonæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
"""
PROMPT_RETRIVER_MUL = """
æ ¹æ®èƒŒæ™¯ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œ
è¯·è¯´ â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚
èƒŒæ™¯ä¿¡æ¯ï¼š{documents}
é—®é¢˜ï¼š{query}
"""
PROMPT_PROBLEM = """
ç»™ä½ ä¸€ç¯‡è®ºæ–‡çš„æ ‡é¢˜å’Œå…³é”®è¯ï¼Œè¯·ä½ ç»™å‡ºä¸€äº›ç”¨æˆ·å¯èƒ½é’ˆå¯¹è¿™ç¯‡è®ºæ–‡è¿›è¡Œé—®ç­”çš„é—®é¢˜ï¼Œé—®é¢˜çš„æ•°é‡ä¸è¦è¶…è¿‡3ä¸ªã€‚
è®ºæ–‡çš„æ ‡é¢˜ï¼š{title}
è®ºæ–‡çš„å…³é”®è¯ï¼š{key_words}
é—®é¢˜ï¼š"""
eb.api_type = args.api_type
access_token = _apply_token(args.api_key, args.secret_key)
eb.access_token = access_token
model = "ernie-bot-3.5" if args.ernie_model is None or args.ernie_model.strip() == "" else args.ernie_model


def retrieval_papers(history=[]):
    """
    Retrieve papers
    """
    query = history.pop()[0]
    query = query.strip().replace("<br>", "\n")
    context = tackle_history(history)
    if query:
        if len(history) == 1:
            paper_id_list = []
            context.append({"role": "user", "content": query})
            prediction = retrieval(
                query=query,
                es_host=args.es_host,
                es_port=args.es_port,
                es_username=args.es_username,
                es_password=args.es_password,
                es_index=args.es_index_abstract,
                es_chunk_size=args.es_chunk_size,
                es_thread_count=args.es_thread_count,
                es_queue_size=args.es_queue_size,
                retriever_batch_size=args.retriever_batch_size,
                retriever_api_key=args.retriever_api_key,
                retriever_secret_key=args.retriever_secret_key,
                retriever_embed_title=args.retriever_embed_title,
                retriever_topk=30,
                rank_topk=3,
            )
            documents = prediction["documents"]
            all_content = ""
            papers_absatract = []
            for i in range(len(documents)):
                if documents[i].meta["id"] not in paper_id_list:
                    paper_id_list.append(documents[i].meta["id"])
                    key_words = documents[i].meta.get("key_words", "")
                    title = documents[i].meta.get("title", "")
                    abstract = documents[i].meta.get("abstracts", "")
                    abstract = summarize_abstract(
                        abstract,
                        api_key=args.api_key,
                        secret_key=args.secret_key,
                        chunk_size=500,
                        max_token=args.max_token,
                    )
                    papers_absatract.append({"content": abstract, "meta": {}})
                    paper_content = (
                        "**" + str(len(paper_id_list)) + "." + title + "**" + "\n" + key_words + "\n" + abstract
                    )
                    all_content += paper_content + "\n\n"
            history.append(["ä¸‹é¢è¯·åŸºäºè¿™å‡ ç¯‡è®ºæ–‡è¿›è¡Œé—®ç­”ï¼Œå•ç¯‡æ–‡æ¡£é—®ç­”è¯·ä½¿ç”¨å•ç¯‡é—®ç­”ç²¾è¯»ç¿»è¯‘", ",".join(paper_id_list)])
            confine_summary = merge_summary(papers_absatract, api_key=args.api_key, secret_key=args.secret_key)
            confine_summary = "**ä¸‹é¢æ˜¯å¯¹ä¸Šé¢å‡ ç¯‡æ–‡æ¡£è¿›è¡Œçš„æ€»ç»“**" + "\n" + confine_summary
            confine_summary = confine_summary.replace("\n\n", "\n")
            history.append([query, all_content + confine_summary])
        else:
            # history = [[user_msg(None),system_msg],[user_hint(None),paper_id]]
            paper_id_list = history[1][1].split(",")
            content = ""
            for id in paper_id_list:
                prediction = retrieval(
                    query=query,
                    file_id=id,
                    es_host=args.es_host,
                    es_port=args.es_port,
                    es_username=args.es_username,
                    es_password=args.es_password,
                    es_index=args.es_index_full_text,
                    es_chunk_size=args.es_chunk_size,
                    es_thread_count=args.es_thread_count,
                    es_queue_size=args.es_queue_size,
                    retriever_batch_size=args.retriever_batch_size,
                    retriever_api_key=args.retriever_api_key,
                    retriever_secret_key=args.retriever_secret_key,
                    retriever_embed_title=args.retriever_embed_title,
                    retriever_topk=30,
                    rank_topk=2,
                )
                content += "\n".join([item.content for item in prediction["documents"]])
            content = PROMPT_RETRIVER_MUL.format(documents=content, query=query)
            content = content[: args.max_token]
            context.append({"role": "user", "content": content})
            eb.api_type = args.api_type
            access_token = _apply_token(args.api_key, args.secret_key)
            eb.access_token = access_token
            model = "ernie-bot-3.5" if args.ernie_model is None or args.ernie_model.strip() == "" else args.ernie_model
            response = eb.ChatCompletion.create(model=model, messages=context, stream=False)
            bot_response = response.result
            history.append([query, bot_response])
    return history


def retrieval_title(title):
    """
    Retrieve the paper_id  of  the title
    """
    prediction = retrieval(
        title,
        es_host=args.es_host,
        es_port=args.es_port,
        es_username=args.es_username,
        es_password=args.es_password,
        es_index=args.es_index_abstract,
        es_chunk_size=args.es_chunk_size,
        es_thread_count=args.es_thread_count,
        es_queue_size=args.es_queue_size,
        retriever_batch_size=args.retriever_batch_size,
        retriever_api_key=args.retriever_api_key,
        retriever_secret_key=args.retriever_secret_key,
        retriever_embed_title=args.retriever_embed_title,
        retriever_topk=30,
        rank_topk=1,
    )
    if prediction["documents"][0].rank_score > args.retriever_threshold:
        return prediction["documents"][0], prediction["documents"][0].meta["id"]
    return None


def infer(history=[]):
    """Model inference."""
    query = history.pop()[0]
    query = query.strip().replace("<br>", "\n")
    context = tackle_history(history)
    single_paper_id = history[1][1]
    if query:
        if single_paper_id:
            prediction = retrieval(
                query=query,
                file_id=single_paper_id,
                es_host=args.es_host,
                es_port=args.es_port,
                es_username=args.es_username,
                es_password=args.es_password,
                es_index=args.es_index_full_text,
                es_chunk_size=args.es_chunk_size,
                es_thread_count=args.es_thread_count,
                es_queue_size=args.es_queue_size,
                retriever_batch_size=args.retriever_batch_size,
                retriever_api_key=args.retriever_api_key,
                retriever_secret_key=args.retriever_secret_key,
                retriever_embed_title=args.retriever_embed_title,
                retriever_topk=30,
                rank_topk=2,
            )
            content = "\n".join([item.content for item in prediction["documents"]])
            content = PROMPT_SYSTEM + PROMPT_RETRIVER.format(documents=content, query=query)
            content = content[: args.max_token]
            context.append({"role": "user", "content": content})
            response = eb.ChatCompletion.create(model=model, messages=context, stream=False)
            bot_response = response.result
            try:
                bot_response = bot_response[bot_response.find("{") :]
                bot_response = bot_response[: bot_response.find("}") + 1]
                bot_response = json.loads(bot_response)
                if type(bot_response["å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ"]) == list:
                    bot_response["å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ"] = "\n".join(bot_response["å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ"])
                bot_response = (
                    "ä»¥ä¸‹æ˜¯æˆ‘çš„åˆ†æå†…å®¹ï¼š\n"
                    + str(bot_response["å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ"])
                    + "\n\n"
                    + "ä»¥ä¸‹æ˜¯æˆ‘çš„æ€»ç»“ï¼š"
                    + str(bot_response["é—®ç­”ä»»åŠ¡çš„ç»“æœ"])
                )
            except:
                bot_response = (
                    str(bot_response).replace("'å…³é”®å¥æŠ½å–ä»»åŠ¡çš„ç»“æœ':", "ä»¥ä¸‹æ˜¯æˆ‘çš„åˆ†æå†…å®¹").replace("'é—®ç­”ä»»åŠ¡çš„ç»“æœ':", "\nä»¥ä¸‹æ˜¯æˆ‘çš„æ€»ç»“\n")
                )
            bot_response = re.sub(r"\[|\]|{|}", "", bot_response)
            bot_response = bot_response.replace("\\n", "\n")
            history.append([query, bot_response])
        else:
            context.append({"role": "user", "content": query})
            response = eb.ChatFile.create(messages=context, stream=False)
            bot_response = response.result
            history.append([query, bot_response])
    return history


def upload_file(file_name, file_url, file_upload, history=[]):
    """
    Upload the file to bos or retrieve the json_file of the paper
    """
    if file_name:
        try:
            json_content, file_id = retrieval_title(file_name)
            content = (
                "**"
                + json_content.meta["title"]
                + "**"
                + "\n\n"
                + json_content.meta["key_words"]
                + "\n\n"
                + json_content.meta["abstracts"]
            )
            title = json_content.meta["title"]
            key_words = json_content.meta["key_words"]
            response = eb.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": PROMPT_PROBLEM.format(title=title, key_words=key_words)}],
                stream=False,
            )
            response = response.result
            history.append([None, file_id])
            history.append(["ä½ å¯ä»¥å‚è€ƒä»¥ä¸‹é—®é¢˜ï¼Œå¯¹è®ºæ–‡è¿›è¡Œæé—®", response])
        except:
            content = "è¿™ç¯‡è®ºæ–‡ç›®å‰å°šæœªåŠ å…¥åˆ°è®ºæ–‡åº“ä¸­,è¯·ä½ è‡ªè¡Œä¸Šä¼ è®ºæ–‡çš„pdfæˆ–è€…urlé“¾æ¥."
            file_id = None
            history.append([None, file_id])
        return (
            gr.Gallery.update(visible=False),
            gr.File.update(visible=False),
            history,
            gr.Chatbot.update(
                [[None, content]],
                visible=True,
                scale=30,
                height=600,
            ),
        )
    elif file_url:
        # temp image save dir
        root_path = "./images"
        os.makedirs(root_path, exist_ok=True)
        paper = next(arxiv.Search(id_list=[file_url.split("/")[-1]]).results())
        real_filename = "{}.pdf".format(file_url.split("/")[-1])
        logger.info(real_filename)
        paper.download_pdf(dirpath=root_path, filename=real_filename)
        file_name = os.path.join(root_path, real_filename)
        tim = time.time()
        image_path = os.path.join(root_path, str(tim))
        os.makedirs(image_path, exist_ok=True)
        imgs = pdf2image(pdfPath=file_name, imgPath=image_path, number_process_page=args.number_process_page)
    elif file_upload:
        file_name = file_upload.name
        real_filename = os.path.split(file_name)[-1]
        root_path = os.path.dirname(file_name)
        tim = time.time()
        image_path = os.path.join(root_path, str(tim))
        os.makedirs(image_path, exist_ok=True)
        imgs = pdf2image(pdfPath=file_name, imgPath=image_path, number_process_page=args.number_process_page)
    # ä¸Šä¼ åˆ°bosååˆ°æ–‡ä»¶æ˜¯å¦éœ€è¦åˆ é™¤
    filename_in_bos = real_filename
    url = eb.utils.upload_file_to_bos(
        file_name, filename_in_bos, access_key_id=args.bos_ak, secret_access_key=args.bos_sk
    )
    history.append([None, None])
    content = "<file>{}</file><url>{}</url>".format(real_filename, url)
    content = content.strip().replace("<br>", "\n")
    context = tackle_history(history)
    context.append({"role": "user", "content": content})
    response = eb.ChatFile.create(messages=context, stream=False)
    bot_response = response.result
    history.append([content, bot_response])
    return (
        gr.Gallery.update(imgs, visible=True),
        gr.File.update(file_name, label="åŸæ–‡ä¸‹è½½é“¾æ¥", visible=True),
        history,
        gr.Chatbot.update(visible=False),
    )


def add_messaget_chatbot(messages, history):
    history.append([messages, None])
    return None, history


def translation_txt(history=[], lang=""):
    if not lang:
        lang = "ä¸­æ–‡"
    message = history.pop()[0]
    if message:
        translation_content = translate_part(
            text=message,
            api_key=args.api_key,
            secret_key=args.secret_key,
            task="ç¿»è¯‘",
            max_length=args.translation_max_token,
            lang=lang,
            chunk_size=args.translation_chunk_size,
            cycle_num=args.translation_cycle_num,
        )
        history.append([message, translation_content])
    return history


with gr.Blocks(title="ç»´æ™®å°åŠ©æ‰‹", theme=gr.themes.Base()) as demo:
    gr.HTML("""<h1 align="center">ChatPaperç»´æ™®å°åŠ©æ‰‹</h1>""")
    with gr.Row(variant="panel"):
        with gr.Column(scale=1):
            cheetah = os.path.join(os.path.dirname(__file__), "weipu.jpg")
            gr.Image(cheetah, elem_id="banner-image", show_label=False, show_download_button=False)
        with gr.Column(scale=9):
            gr.HTML(
                """
                <p>ã€æ–‡ç« æ£€ç´¢æ‘˜è¦ã€‘
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.é€‚åˆä»å¤§é‡æ–‡ç« ä¸­ï¼Œç²—åŠ›åº¦è·å–éœ€è¦çš„ä¿¡æ¯ã€‚è¿”å›åŒ…å« : æ–‡ç« é¢˜ç›®+ä½œè€…+å…³é”®è¯+æœ¯è¯­+æ‘˜è¦.</p>
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.é€‚åˆåŸºäºæŸä¸ªæŠ€æœ¯é¢†åŸŸï¼Œç”Ÿæˆå¯¹åº”çš„æŠ€æœ¯ç»¼è¿°.</p>
                <p>ã€å•ç¯‡ç²¾è¯»ç¿»è¯‘ã€‘ï¼š
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.é€‚åˆé’ˆå¯¹å…·ä½“ä¸€ç¯‡æ–‡ç« ï¼Œè¯¦ç»†äº†è§£ç»†èŠ‚å†…å®¹.</p>
                <p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.é€‚åˆé’ˆå¯¹å…·ä½“ä¸€ç¯‡æ–‡ç« ï¼Œè¯¦ç»†ç¿»è¯‘æ‘˜è¦æœ¯è¯­.</p>
            """
            )
    with gr.Tab("æ–‡ç« æ£€ç´¢æ‘˜è¦"):
        retrieval_chatbot = gr.Chatbot(
            height=600, value=[[None, "ä½ å¥½, æˆ‘æ˜¯ç»´æ™®ChatPaperå°åŠ©æ‰‹, æˆ‘è¿™é‡Œæ”¶å½•äº†100wç¯‡è®ºæ–‡,å¯ä»¥æä¾›æ‚¨ä¸“ä¸šçš„å­¦æœ¯å’¨è¯¢.è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—?"]]
        )  # heightèŠå¤©æ¡†é«˜åº¦, value é»˜è®¤è¯­å¥
        retrieval_textbox = gr.Textbox(placeholder="æœ€è¿‘è‡ªç›‘ç£å­¦ä¹ è®ºæ–‡æœ‰å“ªäº›?")
        with gr.Row():
            retrieval_submit_btn = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
            retrieval_clear_btn = gr.Button("æ¸…é™¤", variant="primary", scale=2, min_width=0)
    retrieval_submit_btn.click(
        add_messaget_chatbot,
        inputs=[retrieval_textbox, retrieval_chatbot],
        outputs=[retrieval_textbox, retrieval_chatbot],
    ).then(retrieval_papers, retrieval_chatbot, retrieval_chatbot)
    retrieval_clear_btn.click(
        lambda _: ([[None, "ä½ å¥½, æˆ‘æ˜¯ç»´æ™®ChatPaperæ–‡ç« ç²¾è¯»ç¿»è¯‘å°åŠ©æ‰‹,å¯ä»¥æä¾›æ‚¨ä¸“ä¸šçš„å­¦æœ¯å’¨è¯¢.è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—?"]]),
        inputs=[retrieval_clear_btn],
        outputs=[retrieval_chatbot],
    )
    with gr.Tab("å•ç¯‡ç²¾è¯»"):  # å°è£…chatFileçš„èƒ½åŠ›
        with gr.Accordion("æ–‡ç« ç²¾è¯»ï¼šè¾“å…¥åŒºï¼ˆè¾“å…¥æ–¹å¼ä¸‰é€‰ä¸€ï¼Œä¸‰ç§è¾“å…¥æ–¹å¼ä¼˜å…ˆçº§ä¾æ¬¡é™ä½ï¼‰", open=True, elem_id="input-panel") as area_input_primary:
            with gr.Row():
                with gr.Group():
                    file_name = gr.Textbox(
                        label="(è¾“å…¥æ–¹å¼1) è®ºæ–‡/æœŸåˆŠæ ‡é¢˜ï¼ˆä»…æ”¯æŒå•ç¯‡æ–‡ç« ç²¾è¯»ï¼‰",
                        value="",
                        placeholder="Human-level control through deep reinforcement learning",
                        interactive=True,
                        scale=1,
                    )
                    file_url = gr.Textbox(
                        label="(è¾“å…¥æ–¹å¼2) è®ºæ–‡ axivé“¾æ¥ï¼ˆä»…æ”¯æŒå•ç¯‡æ–‡ç« ç²¾è¯»ï¼‰",
                        value="",
                        placeholder="https://arxiv.org/abs/2303.08774",
                        interactive=True,
                        scale=1,
                    )
                file_upload = gr.File(
                    label="(è¾“å…¥æ–¹å¼3) ä¸Šä¼ è®ºæ–‡/æœŸåˆŠPDF(ä»…æ”¯æŒå•ç¯‡PDFç²¾è¯»)", file_count="single", height=180, min_width=50
                )
            with gr.Row():
                clear = gr.Button(value="æ¸…ç©ºè¾“å…¥åŒº")
                submit = gr.Button(value="å…¨æ–‡ç²¾è¯»")
        with gr.Accordion("æ–‡ç« ç²¾è¯»ï¼šè¾“å‡ºåŒº", open=True, elem_id="input-panel") as area_input_primary:
            with gr.Tab("å•æ–‡è§£è¯»"):  # åŒ…å«ä¸‹è½½åŠŸèƒ½
                with gr.Row():
                    with gr.Column():
                        gr.Dropdown(choices=[""], max_choices=1, label="è®ºæ–‡åŸæ–‡-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼›æ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        ori_paper = gr.Gallery(label="è®ºæ–‡åŸæ–‡", show_label=False, elem_id="gallery").style(
                            columns=[1], rows=[1], object_fit="contain", height="700px"
                        )
                        ori_json = gr.Chatbot(label="è®ºæ–‡åŸæ–‡", visible=False)
                        ori_pdf = gr.File(label="åŸæ–‡ä¸‹è½½é“¾æ¥")
                    with gr.Accordion("   "):
                        gr.Dropdown(choices=[""], max_choices=1, label="æ–‡ç« æ‘˜è¦ç­‰æ€»ç»“-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼›æ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        chatbot = gr.Chatbot(
                            value=[[None, "ä½ å¥½, æˆ‘æ˜¯ç»´æ™®ChatPaperæ–‡ç« ç²¾è¯»ç¿»è¯‘å°åŠ©æ‰‹,å¯ä»¥æä¾›æ‚¨ä¸“ä¸šçš„å­¦æœ¯å’¨è¯¢.è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—?"]],
                            scale=30,
                            height=600,
                        )
                        message = gr.Textbox(placeholder="è¯·é—®å…·ä½“æè¿°è¿™ç¯‡æ–‡ç« çš„æ–¹æ³•?", scale=7)
                        with gr.Row():
                            submit_btn = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
                            clear_btn = gr.Button("æ¸…é™¤", variant="primary", scale=2, min_width=0)
                submit.click(
                    upload_file,
                    inputs=[file_name, file_url, file_upload, chatbot],
                    outputs=[ori_paper, ori_pdf, chatbot, ori_json],
                )
                clear.click(
                    lambda _: ("", "", None, [[None, "ä½ å¥½, æˆ‘æ˜¯ç»´æ™®ChatPaperæ–‡ç« ç²¾è¯»ç¿»è¯‘å°åŠ©æ‰‹,å¯ä»¥æä¾›æ‚¨ä¸“ä¸šçš„å­¦æœ¯å’¨è¯¢.è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—?"]]),
                    inputs=[],
                    outputs=[file_name, file_url, file_upload, chatbot],
                )
                submit_btn.click(add_messaget_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
                    infer, chatbot, chatbot
                )
                clear_btn.click(
                    lambda _: ([[None, "ä½ å¥½, æˆ‘æ˜¯ç»´æ™®ChatPaperæ–‡ç« ç²¾è¯»ç¿»è¯‘å°åŠ©æ‰‹,å¯ä»¥æä¾›æ‚¨ä¸“ä¸šçš„å­¦æœ¯å’¨è¯¢.è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—?"]]),
                    inputs=clear_btn,
                    outputs=[chatbot],
                    api_name="clear",
                    show_progress=False,
                )
    with gr.Tab("ç¿»è¯‘"):
        with gr.Column():
            chatbot_translation = gr.Chatbot(value=[[None, "ä½ å¥½, æˆ‘æ˜¯ç¿»è¯‘å°åŠ©æ‰‹"]], scale=35, height=500)
            message_translation = gr.Textbox(placeholder="è¯·è¾“å‡ºéœ€è¦ç¿»è¯‘çš„å†…å®¹", lines=5, max_lines=20)
            with gr.Row():
                lang = gr.Radio(choices=["ä¸­æ–‡", "è‹±æ–‡"], max_choices=1, scale=1, value="ä¸­æ–‡", label="è¾“å…¥è¯­è¨€")
                submit_translation = gr.Button("ğŸš€ æäº¤", variant="primary", scale=1)
                clear_translation = gr.Button("æ¸…é™¤", variant="primary", scale=1)
        submit_translation.click(
            add_messaget_chatbot,
            inputs=[message_translation, chatbot_translation],
            outputs=[message_translation, chatbot_translation],
        ).then(translation_txt, inputs=[chatbot_translation, lang], outputs=[chatbot_translation])
        clear_translation.click(
            lambda _: ([[None, "ä½ å¥½, ä½ å¥½, æˆ‘æ˜¯ç¿»è¯‘å°åŠ©æ‰‹"]]), inputs=[clear_translation], outputs=[chatbot_translation]
        )
demo.queue(concurrency_count=40, max_size=40)
demo.launch(server_name=args.serving_name, server_port=args.serving_port)
