# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import argparse
import os
import shutil
import time

import arxiv
import gradio as gr
from create_base import chat_papers
from utils import merge_summary, pdf2image, single_paper_abs_sum, translation

from pipelines.document_stores import FAISSDocumentStore
from pipelines.nodes import (
    DensePassageRetriever,
    ErnieBot,
    ErnieRanker,
    PromptTemplate,
    TruncatedConversationHistory,
)
from pipelines.pipelines import Pipeline

paper_all = []
index_name = "dureader_index"
import multiprocessing
from functools import partial
from multiprocessing import Manager, Pool

manager = Manager()
all_data_result = manager.dict()
papers_sum = manager.list()

parser = argparse.ArgumentParser()
parser.add_argument(
    "--device",
    choices=["cpu", "gpu"],
    default="gpu",
    help="Select which device to run dense_qa system, defaults to gpu.",
)
parser.add_argument("--index_name", default="dureader_index", type=str, help="The ann index name of ANN.")
parser.add_argument(
    "--max_seq_len_query", default=64, type=int, help="The maximum total length of query after tokenization."
)
parser.add_argument(
    "--max_seq_len_passage", default=256, type=int, help="The maximum total length of passage after tokenization."
)
parser.add_argument(
    "--retriever_batch_size",
    default=16,
    type=int,
    help="The batch size of retriever to extract passage embedding for building ANN index.",
)
parser.add_argument(
    "--query_embedding_model", default="moka-ai/m3e-base", type=str, help="The query_embedding_model path"
)
parser.add_argument(
    "--passage_embedding_model", default="moka-ai/m3e-base", type=str, help="The passage_embedding_model path"
)
parser.add_argument(
    "--params_path", default="checkpoints/model_40/model_state.pdparams", type=str, help="The checkpoint path"
)
parser.add_argument("--embedding_dim", default=768, type=int, help="The embedding_dim of index")
parser.add_argument("--chunk_size", default=384, type=int, help="The length of data for indexing by retriever")
parser.add_argument("--host", type=str, default="localhost", help="host ip of ANN search engine")
parser.add_argument("--embed_title", default=False, type=bool, help="The title to be  embedded into embedding")
parser.add_argument(
    "--model_type",
    choices=["ernie_search", "ernie", "bert", "neural_search"],
    default="ernie",
    help="the ernie model types",
)
parser.add_argument("--api_key", default=" ", type=str, help="The API Key.")
parser.add_argument("--secret_key", default=" ", type=str, help="The secret key.")
parser.add_argument(
    "--pooling_mode",
    default="mean_tokens",
    choices=["max_tokens", "mean_tokens", "mean_sqrt_len_tokens", "cls_token"],
    type=str,
    help="The type of sentence embedding.",
)
args = parser.parse_args()


def clear_session():
    global all_data_result
    global papers_all
    global papers_sum
    all_data_result = manager.dict()
    papers_sum = manager.list()
    papers_all = []
    return None, "https://arxiv.org/abs/2303.08774"


def chat_file(
    query,
    history=None,
    index_paper=None,
    api_key=args.api_key,
    secret_key=args.secret_key,
):
    if history is None:
        history = []
    if index_paper is None:
        index = "document"
    else:
        index = index_paper.split("/")[-1].replace(".pdf", "").replace(".", "_")
    document_store = FAISSDocumentStore.load(index_name)
    use_gpu = True if args.device == "gpu" else False
    retriever = DensePassageRetriever(
        document_store=document_store,
        query_embedding_model=args.query_embedding_model,
        passage_embedding_model=args.passage_embedding_model,
        output_emb_size=args.embedding_dim if args.model_type in ["ernie_search", "neural_search"] else None,
        max_seq_len_query=args.max_seq_len_query,
        max_seq_len_passage=args.max_seq_len_passage,
        batch_size=args.retriever_batch_size,
        use_gpu=use_gpu,
        embed_title=args.embed_title,
        pooling_mode=args.pooling_mode,
    )
    ranker = ErnieRanker(model_name_or_path="rocketqa-zh-dureader-cross-encoder", use_gpu=use_gpu)
    query_pipeline = Pipeline()
    query_pipeline.add_node(component=retriever, name="Retriever", inputs=["Query"])
    query_pipeline.add_node(component=ranker, name="Ranker", inputs=["Retriever"])
    query_pipeline.add_node(component=PromptTemplate("èƒŒæ™¯ï¼š{documents} é—®é¢˜ï¼š{query}"), name="Template", inputs=["Ranker"])
    query_pipeline.add_node(
        component=TruncatedConversationHistory(max_length=256), name="TruncateHistory", inputs=["Template"]
    )
    ernie_bot = ErnieBot(api_key=api_key, secret_key=secret_key)
    query_pipeline.add_node(component=ernie_bot, name="ErnieBot", inputs=["TruncateHistory"])
    prediction = query_pipeline.run(
        query=query, params={"Retriever": {"top_k": 30, "index": str(index)}, "Ranker": {"top_k": 3}}
    )
    history.append(["user: {}".format(query), "assistant: {}".format(prediction["result"])])
    return "", history, history


def tackle_paper(root_path, path, api_key, secret_key, lang="ç®€ä½“ä¸­æ–‡"):
    pdf_image = pdf2image(path, root_path)
    if lang == "English":
        translation_str, translation_file, sum_str, sum_file = translation(
            root_path, path, api_key=api_key, secret_key=secret_key
        )
        all_data_result[path.split("/")[-1].replace(".pdf", "").replace(".", "_")] = [
            pdf_image,
            path,
            translation_str,
            translation_file,
            sum_str,
            sum_file,
        ]
    else:
        data_split, sum_str, sum_file = single_paper_abs_sum(root_path, path, api_key=api_key, secret_key=secret_key)
        all_data_result[path.split("/")[-1].replace(".pdf", "").replace(".", "_")] = [
            pdf_image,
            path,
            "",
            None,
            sum_str,
            sum_file,
        ]
    papers_sum.append({"content": sum_str, "meta": {"name": path}})
    index = path.split("/")[-1].replace(".pdf", "").replace(".", "_")
    return index, data_split


def mul_tackle(
    p_m,
    root_path_list,
    path_list,
    api_key=" ",
    secret_key=" ",
    lang="ç®€ä½“ä¸­æ–‡",
):
    func = partial(tackle_paper, api_key=api_key, secret_key=secret_key, lang=lang)
    pool = Pool(processes=min(p_m, multiprocessing.cpu_count()))
    result = pool.starmap(func, [(root_path, path) for root_path, path in zip(root_path_list, path_list)])
    pool.close()
    pool.join()
    return result


def predict(file_upload, input1=None, lang="ç®€ä½“ä¸­æ–‡", api_key=args.api_key, secret_key=args.secret_key, p_m=3):
    if os.path.exists("faiss_document_store.db"):
        os.remove("faiss_document_store.db")
    if os.path.exists(index_name):
        shutil.rmtree(index_name)
    if lang == "English":
        if file_upload:
            path_list = [path.name for path in file_upload]
            root_path_list = [os.path.dirname(path) for path in path_list]
        else:
            paths = input1.split(";")
            path_list = []
            root_path_list = ["./" for i in range(len(path_list))]
            root_path = root_path_list[0]
            for index, path_item in enumerate(paths):
                paper = next(arxiv.Search(id_list=[path_item.split("/")[-1]]).results())
                path_name = "{}.pdf".format(path_item.split("/")[-1])
                paper.download_pdf(dirpath=root_path, filename=path_name)
                path = os.path.join(root_path, path_name)
                path_list.append(path)
    else:
        path_list = [path.name for path in file_upload]
        root_path_list = [os.path.dirname(path) for path in path_list]
        document_store = FAISSDocumentStore(
            embedding_dim=768, faiss_index_factory_str="Flat", duplicate_documents="skip"
        )
        use_gpu = True if args.device == "gpu" else False
        retriever = DensePassageRetriever(
            document_store=document_store,
            query_embedding_model=args.query_embedding_model,
            passage_embedding_model=args.passage_embedding_model,
            output_emb_size=args.embedding_dim if args.model_type in ["ernie_search", "neural_search"] else None,
            max_seq_len_query=args.max_seq_len_query,
            max_seq_len_passage=args.max_seq_len_passage,
            batch_size=args.retriever_batch_size,
            use_gpu=use_gpu,
            embed_title=args.embed_title,
            pooling_mode=args.pooling_mode,
        )
    multi_result = mul_tackle(
        p_m=p_m, root_path_list=root_path_list, path_list=path_list, api_key=api_key, secret_key=secret_key, lang=lang
    )
    for index, split_text in multi_result:
        split_text = retriever.run_indexing(split_text)[0]["documents"]
        document_store.write_documents(split_text, index=str(index))
        document_store.write_documents(split_text)
    document_store.save(index_name)
    mul_sum = merge_summary(papers_sum, api_key=api_key, secret_key=secret_key)
    file_name_sum = root_path_list[0] + "/" + "mul_papers_sum.txt"
    with open(file_name_sum, "w", encoding="utf-8") as f:
        f.write(mul_sum)
    return (
        gr.Textbox.update(value="æ‰€æœ‰pdfè§£æå®Œæˆ"),
        gr.Textbox.update(value="å¯ä»¥å¼€å§‹chatfileåŠŸèƒ½"),
        gr.Textbox.update(value="å¯ä»¥å¼€å§‹chatfileåŠŸèƒ½"),
        gr.Textbox.update(value="å¯ä»¥å¼€å§‹chatfileåŠŸèƒ½"),
        mul_sum,
        file_name_sum,
    )


def tr_result(file_name):
    global all_data_result
    file_name = file_name.split("/")[-1].replace(".pdf", "").replace(".", "_")
    while True:
        if file_name in all_data_result.keys():
            tr_result = all_data_result[file_name]
            break
        else:
            # Further optimization is needed
            time.sleep(60)
    return tr_result[:4]


def sum_result(file_name):
    global all_data_result
    file_name = file_name.split("/")[-1].replace(".pdf", "").replace(".", "_")
    while True:
        if file_name in all_data_result.keys():
            sum_result = all_data_result[file_name]
            break
        else:
            # Further optimization is needed
            time.sleep(60)
    return sum_result[:2] + sum_result[4:]


def retriever_papers(
    query,
    history=None,
    api_key=args.api_key,
    secret_key=args.secret_key,
    retriever_top=30,
    ranker_top=3,
):
    if history is not None:
        history = []
    message = chat_papers(
        query, api_key=api_key, secret_key=secret_key, retriever_top=retriever_top, ranker_top=ranker_top
    )
    history.append(["user: {}".format(query), "assistant: {}".format(message["result"])])
    return "", history, history


def Dropdown_list(papers, inputs):
    global paper_all
    if papers is not None:
        paper_list = [paper.name.split("/")[-1] for paper in papers]
    else:
        paper_list = inputs.split(";")
    paper_all += [i for i in paper_list if i not in paper_all]
    return gr.Dropdown.update(choices=paper_all, value=paper_all[0]), gr.Dropdown.update(
        choices=paper_all, value=paper_all[0]
    )


def Button_display():
    return gr.Button.update("ç»“æœå±•ç¤º", scale=1, variant="primary", size="sm", visible=True)


with gr.Blocks() as demo:
    with gr.Tab("è®ºæ–‡ç¿»è¯‘æ€»ç»“"):
        with gr.Accordion("è®ºæ–‡ç¿»è¯‘ç²¾è¯»ï¼šè¾“å…¥åŒº", open=True, elem_id="input-panel") as area_input_primary:
            with gr.Row():
                file_upload = gr.inputs.File(label="(è¾“å…¥æ–¹å¼1) è¯·ä¸Šä¼ è®ºæ–‡PDF(ä»…æ”¯æŒPDFï¼Œæ”¯æŒå¤šç¯‡æ–‡ç« ç¿»è¯‘)", file_count="multiple")
                with gr.Accordion("(è¾“å…¥æ–¹å¼2) è¾“å…¥è®ºæ–‡çš„arxivé“¾æ¥ï¼ˆæ”¯æŒå¤šç¯‡æ–‡ç« ç¿»è¯‘ï¼Œé“¾æ¥ç›´æ¥ç”¨è‹±æ–‡;éš”å¼€ï¼‰"):
                    input1 = gr.Textbox(
                        label="", value="https://arxiv.org/abs/2303.08774", placeholder="", interactive=True
                    )
                    output1 = gr.Dropdown(choices=["ç®€ä½“ä¸­æ–‡", "English"], label="è¾“å‡ºè¯­è¨€")
                    output2 = gr.Dropdown(choices=["ç®€ä½“ä¸­æ–‡", "English"], label="è¾“å…¥è®ºæ–‡ç±»åˆ«", value="ç®€ä½“ä¸­æ–‡")
        with gr.Row():
            clear = gr.Button(value="æ¸…ç©ºè¾“å…¥åŒº", scale=2)
            submit = gr.Button(value="å¼€å§‹ç¿»è¯‘ç²¾è¯»", scale=2)
            pdf_parse = gr.Textbox(value="pdfè§£æ", label="pdfè§£æ", scale=1)

        with gr.Accordion("è®ºæ–‡ç¿»è¯‘æ€»ç»“ï¼šè¾“å‡ºåŒº", open=True, elem_id="input-panel") as area_input_primary:

            with gr.Tab("å•æ–‡ç¿»è¯‘"):  # åŒ…å«ä¸‹è½½åŠŸèƒ½
                with gr.Row():
                    file_tr = gr.Dropdown(choices=[""], max_choices=1, scale=4, label="é€‰æ‹©å±•ç¤ºè®ºæ–‡")
                    tr_display = gr.Button("ç¿»è¯‘ç»“æœå±•ç¤º", scale=1, variant="primary", size="sm", visible=False)
                with gr.Row():
                    with gr.Column():
                        gr.Dropdown(choices=["è‹±æ–‡", "ä¸­æ–‡"], max_choices=1, label="è®ºæ–‡åŸæ–‡-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼›æ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        ori_paper = gr.Gallery(label="è®ºæ–‡åŸæ–‡", show_label=False, elem_id="gallery").style(
                            columns=[2], rows=[2], object_fit="contain", height="auto"
                        )
                        ori_pdf = gr.File(label="åŸæ–‡ä¸‹è½½é“¾æ¥")
                    with gr.Accordion("   "):
                        gr.Dropdown(choices=["è‹±æ–‡", "ä¸­æ–‡"], max_choices=1, label="æ•´ä½“ç¿»è¯‘-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼›æ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        trans_paper = gr.Textbox(label="ç¿»è¯‘ç»“æœ", value="", max_lines=10)
                        trans_down = gr.File(label="ç¿»è¯‘ä¸‹è½½é“¾æ¥")
                        with gr.Group():
                            start_chatfile_tr = gr.Textbox(value="æš‚æ—¶æ— æ³•é—®ç­”", label="é—®ç­”å¯åŠ¨")
                            chatbot = gr.Chatbot(label="Chatbot")
                            state = gr.State()
                            with gr.Row():
                                textbox = gr.Textbox(
                                    container=False,
                                    show_label=False,
                                    placeholder="è¿™ç¯‡è®ºæ–‡æœ€å¤§çš„è´¡çŒ®æ˜¯ä»€ä¹ˆ?",
                                    scale=10,
                                )
                                submit_button = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
                                submit_button.click(
                                    chat_file, inputs=[textbox, state, file_tr], outputs=[textbox, chatbot, state]
                                )
            with gr.Tab("å•æ–‡æ€»ç»“"):  # åŒ…å«ä¸‹è½½åŠŸèƒ½
                with gr.Row():
                    file_sum = gr.Dropdown(choices=[""], scale=4, max_choices=1, label="é€‰æ‹©è®ºæ–‡")
                    sum_display = gr.Button("ç²¾è¯»ç»“æœå±•ç¤º", scale=1, variant="primary", size="sm", visible=False)
                with gr.Row():
                    with gr.Column():
                        gr.Dropdown(choices=["è‹±æ–‡", "ä¸­æ–‡"], max_choices=1, label="è®ºæ–‡åŸæ–‡-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼›æ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        ori_paper_c = gr.Gallery(label="è®ºæ–‡åŸæ–‡", show_label=False, elem_id="gallery").style(
                            columns=[2], rows=[2], object_fit="contain", height="auto"
                        )
                        ori_pdf_c = gr.File(label="åŸæ–‡ä¸‹è½½é“¾æ¥")
                    with gr.Accordion("   "):
                        gr.Dropdown(choices=["è‹±æ–‡", "ä¸­æ–‡"], max_choices=1, label="æ•´ä½“ç¿»è¯‘-PDFæ’ä»¶-æ”¯æŒä¸‹è½½ï¼Œæ­¤å¤„ä¸ºPDFå ä½ç¬¦")
                        sum_parper = gr.Markdown(label="ç²¾è¯»å…¨æ–‡", value="")
                        sum_down = gr.File(label="å…¨æ–‡ç²¾åº¦é“¾æ¥ ")
                        with gr.Group():
                            start_chatfile_sum = gr.Textbox(value="æš‚æ—¶æ— æ³•é—®ç­”", label="é—®ç­”å¯åŠ¨")
                            chatbot = gr.Chatbot(label="Chatbot")
                            state = gr.State()
                            with gr.Row():
                                textbox = gr.Textbox(
                                    container=False,
                                    show_label=False,
                                    placeholder="è¿™ç¯‡è®ºæ–‡æœ€å¤§çš„è´¡çŒ®æ˜¯ä»€ä¹ˆ?",
                                    scale=10,
                                )
                                submit_button = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
                                submit_button.click(
                                    chat_file, inputs=[textbox, state, file_sum], outputs=[textbox, chatbot, state]
                                )
            with gr.Tab("å¤šæ–‡æ€»ç»“"):  # åŒ…å«ä¸‹è½½åŠŸèƒ½
                with gr.Accordion("   "):
                    gr.Dropdown(choices=["è‹±æ–‡", "ä¸­æ–‡"], max_choices=1, label="å®Œæ•´æ€»ç»“æ’ä»¶-æ”¯æŒä¸‹è½½ï¼Œæ­¤å¤„ä¸ºå¤šæ–‡æ€»ç»“å ä½ç¬¦ï¼Œéœ€è¦æ”¯æŒä¸Šä¸‹æ‹–åŠ¨")
                    sum_mul_papers = gr.Textbox(label="å¤šæ–‡æ¡£æ‘˜è¦", value="", max_lines=10)
                    sum_mul_papers_down = gr.File(label="å…¨æ–‡ç²¾åº¦é“¾æ¥ ")
                    with gr.Group():
                        start_chatfile_mul = gr.Textbox(value="æš‚æ—¶æ— æ³•é—®ç­”", label="é—®ç­”å¯åŠ¨")
                        chatbot = gr.Chatbot(label="Chatbot")
                        state = gr.State()
                        with gr.Row():
                            textbox = gr.Textbox(
                                container=False,
                                show_label=False,
                                placeholder="è¿™ç¯‡è®ºæ–‡æœ€å¤§çš„è´¡çŒ®æ˜¯ä»€ä¹ˆ?",
                                scale=10,
                            )
                            submit_button = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
                            submit_button.click(chat_file, inputs=[textbox, state], outputs=[textbox, chatbot, state])
            file_upload.change(Dropdown_list, inputs=[file_upload, input1], outputs=[file_tr, file_sum])
            input1.change(Dropdown_list, inputs=[file_upload, input1], outputs=[file_tr, file_sum])
            submit.click(
                predict,
                inputs=[file_upload, input1, output2],
                outputs=[
                    pdf_parse,
                    start_chatfile_tr,
                    start_chatfile_sum,
                    start_chatfile_mul,
                    sum_mul_papers,
                    sum_mul_papers_down,
                ],
            )
            clear.click(clear_session, inputs=[], outputs=[file_upload, input1])
            file_tr.change(Button_display, inputs=[], outputs=[tr_display])
            file_sum.change(Button_display, inputs=[], outputs=[sum_display])
            tr_display.click(tr_result, inputs=file_tr, outputs=[ori_paper, ori_pdf, trans_paper, trans_down])
            sum_display.click(sum_result, inputs=file_sum, outputs=[ori_paper_c, ori_pdf_c, sum_parper, sum_down])

    with gr.Tab("æŠ€æœ¯ç»¼è¿°"):
        with gr.Group():
            chatbot = gr.Chatbot(label="Chatbot")
            with gr.Row():
                textbox = gr.Textbox(
                    container=False,
                    show_label=False,
                    placeholder="æŠ€æœ¯ç»¼è¿°è¾“å…¥æ¡†å£",
                    scale=10,
                )
                submit_button = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)

    with gr.Tab("å­¦æœ¯æ£€ç´¢"):
        with gr.Group():
            chatbot = gr.Chatbot(label="Chatbot")
            state = gr.State()
            with gr.Row():
                textbox = gr.Textbox(
                    container=False,
                    show_label=False,
                    placeholder="æ£€ç´¢å†…å®¹è¾“å…¥æ¡†å£",
                    scale=10,
                )
                submit_button = gr.Button("ğŸš€ æäº¤", variant="primary", scale=2, min_width=0)
            submit_button.click(retriever_papers, inputs=[textbox, state], outputs=[textbox, chatbot, state])
demo.launch(server_name="0.0.0.0", server_port=8084)
