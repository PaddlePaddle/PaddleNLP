# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json

import erniebot
import gradio as gr
from prompt_utils import functions, get_parse_args

from pipelines.document_stores import BaiduElasticsearchDocumentStore
from pipelines.nodes import (
    BM25Retriever,
    DensePassageRetriever,
    EmbeddingRetriever,
    ErnieRanker,
)
from pipelines.pipelines import Pipeline

args = get_parse_args()
erniebot.api_type = "qianfan"
erniebot.ak = args.api_key
erniebot.sk = args.secret_key

document_store_with_docs = BaiduElasticsearchDocumentStore(
    host=args.host,
    port=args.port,
    username=args.username,
    password=args.password,
    embedding_dim=args.embedding_dim,
    similarity="dot_prod",
    vector_type="bpack_vector",
    index=args.abstract_index_name,
    index_type=args.index_type,
    ef_construction=200,
    m=32,
    number_of_shard=3,
)
if args.model_type == "ernie-embedding-v1":
    dpr_retriever = EmbeddingRetriever(
        document_store=document_store_with_docs,
        retriever_batch_size=args.retriever_batch_size,
        api_key=args.embedding_api_key,
        embed_title=args.embed_title,
        secret_key=args.embedding_secret_key,
    )
else:
    dpr_retriever = DensePassageRetriever(
        document_store=document_store_with_docs,
        query_embedding_model=args.query_embedding_model,
        passage_embedding_model=args.passage_embedding_model,
        max_seq_len_query=args.max_seq_len_query,
        max_seq_len_passage=args.max_seq_len_passage,
        batch_size=args.retriever_batch_size,
        embed_title=args.embed_title,
        precision="fp16",
    )
ranker = ErnieRanker(model_name_or_path="rocketqa-base-cross-encoder", use_gpu=True)
bm_retriever = BM25Retriever(document_store=document_store_with_docs)

pipeline = Pipeline()
pipeline.add_node(component=dpr_retriever, name="DenseRetriever", inputs=["Query"])
pipeline.add_node(component=ranker, name="Ranker", inputs=["DenseRetriever"])


single_pipe = Pipeline()
single_pipe.add_node(component=bm_retriever, name="BMRetriever", inputs=["Query"])
# å‘é‡æ£€ç´¢ä¼šå¼•å…¥å¾ˆå¤šè‹±æ–‡å™ªå£°æ•°æ®ï¼Œæš‚æ—¶æ”¾å¼ƒ
# single_pipe.add_node(component=dpr_retriever, name="DenseRetriever", inputs=["Query"])
# single_pipe.add_node(
#     component=JoinDocuments(join_mode="concatenate"), name="JoinResults", inputs=["BMRetriever", "DenseRetriever"]
# )
single_pipe.add_node(component=ranker, name="Ranker", inputs=["BMRetriever"])


def search_multi_paper(query, top_k=3):
    parameters = {
        "DenseRetriever": {
            "top_k": 10,
            "index": args.abstract_index_name,
        },
        "Ranker": {"top_k": top_k},
    }
    for i in range(3):
        try:
            prediction = pipeline.run(
                query=query,
                params=parameters,
            )
        except Exception as e:
            print(e)
            gr.Error(f"Connction error, try times {i}")
            continue

        documents = []
        for doc in prediction["documents"]:
            documents.append(
                {
                    "document": doc.content,
                    "key_words": doc.meta["key_words"],
                    "title": doc.meta["title"],
                }
            )
        if len(documents) > 0:
            break
        else:
            gr.Error(f"Connction error, try times {i}")

    return {"documents": documents}


def search_single_paper(query, title):
    filters = {
        "$and": {
            "title": {"$eq": title},
        }
    }
    parameters = {
        "BMRetriever": {"top_k": 5, "index": args.full_text_index_name, "filters": filters},
        # "DenseRetriever": {
        #     "top_k": 5,
        #     "index": args.full_text_index_name,
        #     "filters": filters,
        # },
        "Ranker": {"top_k": 3},
    }
    for i in range(3):
        try:
            prediction = single_pipe.run(
                query=query,
                params=parameters,
            )
        except Exception as e:
            print(e)
            gr.Error(f"Connction error, try times {i}")
            continue

        documents = []
        for doc in prediction["documents"]:
            documents.append(
                {
                    "document": doc.content,
                    "key_words": doc.meta["key_words"],
                    "title": doc.meta["title"],
                }
            )
        if len(documents) > 0:
            break
        else:
            gr.Error(f"Connction error, try times {i}")

    return {"documents": documents}


def get_literature_review(history, messages):
    base_prompt = """
    è¯·æ ¹æ®èŠå¤©å†å²ä¿¡æ¯æåˆ°çš„å‡ ç¯‡æ–‡ç« ï¼Œç”Ÿæˆç»¼è¿°ã€‚æŒ‰ç…§ä¸‹é¢çš„æ–¹å¼è¿›è¡Œè¾“å‡ºï¼šæŸæŸè®ºæ–‡æå‡ºä»€ä¹ˆæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•æœ‰ä»€ä¹ˆç‚¹ï¼Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜ã€‚
    """
    literature_text = base_prompt
    messages = messages[:-1]
    messages.append({"role": "user", "content": literature_text})

    resp_stream = erniebot.ChatCompletion.create(model="ernie-bot-3.5", messages=messages, stream=False)
    return {"result": resp_stream["result"]}


def history_transform(history=[]):
    messages = []
    if len(history) < 2:
        return messages

    for turn_idx in range(1, len(history)):
        messages.extend(
            [{"role": "user", "content": history[turn_idx][0]}, {"role": "assistant", "content": history[turn_idx][1]}]
        )
    return messages


def prediction(history):
    logs = []
    query = history.pop()[0]

    if query == "":
        return history, "æ³¨æ„ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©º"
    for turn_idx in range(len(history)):
        if history[turn_idx][0] is not None:
            history[turn_idx][0] = history[turn_idx][0].replace("<br>", "")
        if history[turn_idx][1] is not None:
            history[turn_idx][1] = history[turn_idx][1].replace("<br>", "")

    messages = history_transform(history)

    messages.append({"role": "user", "content": query})
    logs.append(f"Function Callçš„è¾“å…¥: {messages}")
    function_times = 0
    error_text = ""
    try:
        while True:
            # Step 1, decide whether we need function call
            resp_stream = erniebot.ChatCompletion.create(
                model="ernie-bot-3.5", messages=messages, functions=functions, stream=True
            )

            # Step 2: execute command
            stream_output = ""
            output_response = ""
            function_flag = False
            for resp in resp_stream:
                if not hasattr(resp, "function_call"):
                    if not function_flag:
                        logs.append("Function Callæœªè§¦å‘")
                        function_flag = True
                    stream_output += resp["result"]
                    yield history + [[query, stream_output]], "\n".join(logs)

                else:
                    # Function Call triggered
                    output_response = resp
                    break

            function_times += 1
            # function callæœªè§¦å‘
            if function_flag is True:
                break
            # Avoid endless function call
            elif function_times > 6:
                error_text = "å½“å‰æ–‡å¿ƒä¸€è¨€æœåŠ¡ç¹å¿™ï¼Œè¯·é‡è¯•"
                break

            # 2.1: execute function calling
            if hasattr(output_response, "function_call"):
                function_call = output_response.function_call
                logs.append(f"Function Callå·²è§¦å‘: {function_call}")
                name2function = {
                    "search_multi_paper": search_multi_paper,
                    "search_single_paper": search_single_paper,
                    "get_literature_review": get_literature_review,
                }

                # è´Ÿæ ·æœ¬, ç›®å‰ä¸åšå¤„ç†ï¼Œç›´æ¥è·³è¿‡
                if function_call["name"] not in name2function:
                    logs.append(f"Function Callçš„åç§°{function_call['name']}ä¸å­˜åœ¨")

                    response = erniebot.ChatCompletion.create(model="ernie-bot-3.5", messages=messages, stream=True)
                    stream_output = ""
                    for character in response:
                        result = character["result"]
                        stream_output += result
                        yield history + [[query, stream_output]], "\n".join(logs)
                    history.append([query, stream_output])
                    return history, "\n".join(logs)
                else:
                    func = name2function[function_call["name"]]
                    func_args = json.loads(function_call["arguments"])
                    if function_call["name"] == "get_literature_review":
                        func_args["history"] = history
                        func_args["messages"] = messages
                    res = func(**func_args)
                    # å¯¹äºå¤šç¯‡è®ºæ–‡æ£€ç´¢åŠ å…¥æ¶¦è‰²prompt
                    if function_call["name"] == "search_multi_paper":
                        res["prompt"] = "è¯·æ ¹æ®è®ºæ–‡å·¥å…·çš„ç»“æœè¿”å›æ¯ç¯‡è®ºæ–‡çš„æ ‡é¢˜ï¼ˆåŠ ç²—ï¼‰, å†…å®¹ä»¥åŠå…³é”®è¯ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€çš„æ–¹å¼è¾“å‡ºï¼Œä¸å…è®¸èƒ¡ç¼–ä¹±é€ ï¼Œä¸è¦ä½¿ç”¨jsonæˆ–è€…è¡¨æ ¼çš„å½¢å¼ã€‚"
                    elif function_call["name"] == "get_literature_review":
                        res["prompt"] = "è¯·æ ¹æ®ç”Ÿæˆçš„ç»¼è¿°ï¼Œå…ˆåœ¨å¼€å¤´åŠ ä¸Šæ€»ç»“æ€§çš„è¯è¯­ï¼Œç„¶åæŒ‰ç…§æŸæŸè®ºæ–‡æå‡ºä»€ä¹ˆæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•æœ‰ä»€ä¹ˆç‚¹ï¼Œè§£å†³äº†ä»€ä¹ˆé—®é¢˜çš„æ–¹å¼è¾“å‡ºç»¼è¿°ï¼Œä¸è¦ä½¿ç”¨jsonæˆ–è€…è¡¨æ ¼çš„å½¢å¼ã€‚"
                    logs.append(f"Function Callè°ƒç”¨ç»“æœ: {res}")
                    # Step 3: return msg to erniebot
                    messages.append({"role": "assistant", "content": None, "function_call": function_call})
                    messages.append(
                        {
                            "role": "function",
                            "name": function_call["name"],
                            "content": json.dumps(res, ensure_ascii=False),
                        }
                    )
    except Exception as e:
        logs.append(f"Function Callæ‰§è¡Œå¼‚å¸¸: {e}")
        error_text = "å½“å‰æ–‡å¿ƒä¸€è¨€æœåŠ¡ç¹å¿™ï¼Œè¯·é‡è¯•"

    if error_text != "":
        stream_output = ""
        for token in error_text:
            stream_output += token
            yield history + [[query, stream_output]], "\n".join(logs)

    history.append([query, stream_output])
    return history, "\n".join(logs)


def add_message_chatbot(messages, history):
    history.append([messages, None])
    return None, history


def launch_ui():
    with gr.Blocks(title="ç»´æ™®è®ºæ–‡åŠ©æ‰‹", theme=gr.themes.Base()) as demo:
        gr.HTML("""<h1 align="center">ç»´æ™®è®ºæ–‡åŠ©æ‰‹</h1>""")
        with gr.Column():
            chatbot = gr.Chatbot(value=[[None, "æ‚¨å¥½, æˆ‘æ˜¯ç»´æ™®è®ºæ–‡åŠ©æ‰‹ã€‚é™¤äº†æ™®é€šçš„å¤§æ¨¡å‹èƒ½åŠ›ä»¥å¤–ï¼Œæˆ‘è¿˜ç‰¹åˆ«äº†è§£æ¥è‡ªç»´æ™®çš„å­¦æœ¯è®ºæ–‡å“¦ï¼"]], scale=35, height=600)
            message = gr.Textbox(placeholder="ä½ èƒ½å¸®æˆ‘æ‰¾ä¸€äº›æœ‰å…³æœºå™¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹é¢çš„è®ºæ–‡å—", lines=1, max_lines=20)
            gr.Examples(
                [["åŠç›‘ç£å­¦ä¹ çš„è®ºæ–‡æœ‰å“ªäº›ï¼Ÿ"], ["è¯·æ¨è3ç¯‡å¼ºåŒ–å­¦ä¹ å¤šæ™ºèƒ½ä½“çš„è®ºæ–‡"], ["è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ "]],
                inputs=[message],
                outputs=[message],
                label="ç¤ºä¾‹è¾“å…¥",
            )
            with gr.Row():
                submit = gr.Button("ğŸš€ æäº¤", variant="primary", scale=1)
                clear = gr.Button("æ¸…é™¤", variant="primary", scale=1)
            # é»˜è®¤æ—¥å¿—å¯è§
            log = gr.Textbox(value="å½“å‰è½®æ¬¡æ—¥å¿—", visible=True)

        message.submit(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
            prediction, inputs=[chatbot], outputs=[chatbot, log]
        )
        submit.click(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
            prediction, inputs=[chatbot], outputs=[chatbot, log]
        )
        clear.click(
            lambda _: ([[None, "æ‚¨å¥½, æˆ‘æ˜¯ç»´æ™®è®ºæ–‡åŠ©æ‰‹ã€‚é™¤äº†æ™®é€šçš„å¤§æ¨¡å‹èƒ½åŠ›ä»¥å¤–ï¼Œæˆ‘è¿˜ç‰¹åˆ«äº†è§£æ¥è‡ªç»´æ™®çš„å­¦æœ¯è®ºæ–‡å“¦ï¼"]]), inputs=[clear], outputs=[chatbot]
        )

    demo.queue(concurrency_count=1)
    demo.launch(server_name=args.serving_name, server_port=args.serving_port, debug=True)


if "__main__" == __name__:
    launch_ui()
