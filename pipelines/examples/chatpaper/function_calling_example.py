# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json

import erniebot
import gradio as gr
from prompt_utils import functions, get_parse_args

from pipelines.document_stores import BaiduElasticsearchDocumentStore
from pipelines.nodes import EmbeddingRetriever
from pipelines.pipelines import Pipeline

args = get_parse_args()
erniebot.api_type = "qianfan"
erniebot.ak = args.api_key
erniebot.sk = args.secret_key

document_store_with_docs = BaiduElasticsearchDocumentStore(
    host=args.host,
    port=args.port,
    username=args.username,
    password=args.password,
    embedding_dim=args.embedding_dim,
    similarity="dot_prod",
    vector_type="bpack_vector",
    search_fields=["content", "meta"],
    index=args.abstract_index_name,
)
dpr_retriever = EmbeddingRetriever(
    document_store=document_store_with_docs,
    retriever_batch_size=args.retriever_batch_size,
    api_key=args.embedding_api_key,
    embed_title=args.embed_title,
    secret_key=args.embedding_secret_key,
)

pipeline = Pipeline()
pipeline.add_node(component=dpr_retriever, name="DenseRetriever", inputs=["Query"])


def search_multi_paper(query, top_k=3):
    prediction = pipeline.run(
        query=query,
        params={
            "DenseRetriever": {
                "top_k": top_k,
                "index": args.abstract_index_name,
            },
        },
    )

    documents = []
    for doc in prediction["documents"]:
        documents.append(
            {
                "document": doc.content,
                "key_words": doc.meta["key_words"],
                "title": doc.meta["title"],
            }
        )
    return {"documents": documents}


def search_single_paper(query, title):
    filters = {
        "$and": {
            "title": {"$eq": title},
        }
    }
    prediction = pipeline.run(
        query=query,
        params={
            "DenseRetriever": {
                "top_k": 3,
                "index": args.full_text_index_name,
                "filters": filters,
            },
        },
    )

    documents = []
    for doc in prediction["documents"]:
        documents.append(
            {
                "document": doc.content,
                "key_words": doc.meta["key_words"],
                "title": doc.meta["title"],
            }
        )

    return {"documents": documents}


def history_transform(history=[]):
    messages = []
    if len(history) < 2:
        return messages

    for turn_idx in range(1, len(history)):
        messages.extend(
            [{"role": "user", "content": history[turn_idx][0]}, {"role": "assistant", "content": history[turn_idx][1]}]
        )
    return messages


def prediction(history):
    logs = []
    query = history.pop()[0]

    if query == "":
        return history, "æ³¨æ„ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©º"
    for turn_idx in range(len(history)):
        if history[turn_idx][0] is not None:
            history[turn_idx][0] = history[turn_idx][0].replace("<br>", "")
        if history[turn_idx][1] is not None:
            history[turn_idx][1] = history[turn_idx][1].replace("<br>", "")

    messages = history_transform(history)
    messages.append({"role": "user", "content": query})
    logs.append(f"Function Callçš„è¾“å…¥: {messages}")
    # Step 1, decide whether we need function call
    resp_stream = erniebot.ChatCompletion.create(
        model="ernie-bot-3.5", messages=messages, functions=functions, stream=True
    )
    # Step 2: execute command
    stream_output = ""
    output_response = ""
    function_flag = False
    for resp in resp_stream:
        if not hasattr(resp, "function_call"):
            if not function_flag:
                logs.append("Function Callæœªè§¦å‘")
                function_flag = True
            stream_output += resp["result"]
            yield history + [[query, stream_output]], "\n".join(logs)

        else:
            # Function Call triggered
            output_response = resp
            break

    # 2.1: execute function calling
    if hasattr(output_response, "function_call"):
        function_call = output_response.function_call
        logs.append(f"Function Callå·²è§¦å‘: {function_call}")
        name2function = {"search_multi_paper": search_multi_paper, "search_single_paper": search_single_paper}
        func = name2function[function_call["name"]]
        func_args = json.loads(function_call["arguments"])
        res = func(**func_args)
        # å¯¹äºå¤šç¯‡è®ºæ–‡æ£€ç´¢åŠ å…¥æ¶¦è‰²prompt
        if function_call["name"] == "search_multi_paper":
            res["prompt"] = "è¯·æ ¹æ®è®ºæ–‡æ£€ç´¢å·¥å…·çš„ç»“æœè¿”å›æ¯ç¯‡è®ºæ–‡çš„æ ‡é¢˜ï¼ˆåŠ ç²—ï¼‰, å†…å®¹ä»¥åŠå…³é”®è¯ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€çš„æ–¹å¼è¾“å‡ºï¼Œä¸è¦ä½¿ç”¨jsonæˆ–è€…è¡¨æ ¼çš„å½¢å¼ã€‚"
        logs.append(f"Function Callè°ƒç”¨ç»“æœ: {res}")
        # Step 3: return msg to erniebot
        messages.append({"role": "assistant", "content": None, "function_call": function_call})
        messages.append(
            {"role": "function", "name": function_call["name"], "content": json.dumps(res, ensure_ascii=False)}
        )
        response = erniebot.ChatCompletion.create(model="ernie-bot-3.5", messages=messages, stream=True)
        stream_output = ""
        for character in response:
            result = character["result"]
            stream_output += result
            yield history + [[query, stream_output]], "\n".join(logs)

    history.append([query, stream_output])
    return history, "\n".join(logs)


def add_message_chatbot(messages, history):
    history.append([messages, None])
    return None, history


def launch_ui():
    with gr.Blocks(title="ç»´æ™®å°åŠ©æ‰‹", theme=gr.themes.Base()) as demo:
        gr.HTML("""<h1 align="center">ChatPaperç»´æ™®å°åŠ©æ‰‹</h1>""")
        with gr.Tab("ChatPaper"):
            with gr.Column():
                chatbot = gr.Chatbot(value=[[None, "æ‚¨å¥½, æˆ‘æ˜¯ç»´æ™®è®ºæ–‡å°åŠ©æ‰‹"]], scale=35, height=500)
                message = gr.Textbox(placeholder="ä½ èƒ½å¸®æˆ‘æ‰¾ä¸€äº›æœ‰å…³æœºå™¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹é¢çš„è®ºæ–‡å—", lines=1, max_lines=20)
                with gr.Row():
                    submit = gr.Button("ğŸš€ æäº¤", variant="primary", scale=1)
                    clear = gr.Button("æ¸…é™¤", variant="primary", scale=1)
                log = gr.Textbox(value="å½“å‰è½®æ¬¡æ—¥å¿—")
            message.submit(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
                prediction, inputs=[chatbot], outputs=[chatbot, log]
            )
            submit.click(add_message_chatbot, inputs=[message, chatbot], outputs=[message, chatbot]).then(
                prediction, inputs=[chatbot], outputs=[chatbot, log]
            )
            clear.click(lambda _: ([[None, "æ‚¨å¥½, æˆ‘æ˜¯ç»´æ™®è®ºæ–‡å°åŠ©æ‰‹"]]), inputs=[clear], outputs=[chatbot])
    demo.queue()
    demo.launch(server_name=args.serving_name, server_port=args.serving_port, debug=True)


if "__main__" == __name__:
    launch_ui()
