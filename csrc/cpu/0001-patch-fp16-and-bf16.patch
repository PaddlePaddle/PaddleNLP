From eb8a1aa95e92d5d0315006fd74d44103d058948c Mon Sep 17 00:00:00 2001
From: "395822456@qq.com" <bukejiyu>
Date: Tue, 25 Jun 2024 07:42:05 +0000
Subject: [PATCH] patch fp16 and bf16

---
 include/layers_decoder.h     | 15 +++----
 src/layers/attention.h       | 54 ++++++++++++------------
 src/layers/decoder_layer.cpp | 79 +++++++++++++++++++-----------------
 src/layers/decoder_layer.h   |  7 ++--
 4 files changed, 81 insertions(+), 74 deletions(-)

diff --git a/include/layers_decoder.h b/include/layers_decoder.h
index 34f6aa5..0da4723 100644
--- a/include/layers_decoder.h
+++ b/include/layers_decoder.h
@@ -18,12 +18,13 @@
 
 namespace xft {
 
-void invokeLayerLLaMA(DataType dt, ActivationType at, NormType nt, int batchSize, int inputSeqLen, int attHeadDim,
-        int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed, int pastSeqLen, int currentSeqLen, int step,
-        int hiddenSize, int intermediateSize, void *output, int outputStride, const void *input, int inputStride,
-        const float *ln1Gamma, const float *ln1Beta, const void *queryWeight, const void *keyWeight,
-        const void *valueWeight, const void *attnOutWeight, const float *ln2Gamma, const float *ln2Beta,
-        const void *gateWeight, const void *upWeight, const void *downWeight, const float *queryBias = nullptr,
-        const float *keyBias = nullptr, const float *valueBias = nullptr, const float *attnOutBias = nullptr);
+void invokeLayerLLaMA(DataType dt, ActivationType at, NormType nt, int layerId, int totalLayers, int batchSize,
+        int inputSeqLen, int attHeadDim, int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed,
+        int pastSeqLen, int currentSeqLen, int step, int hiddenSize, int intermediateSize, void *output,
+        int outputStride, const void *input, int inputStride, const float *ln1Gamma, const float *ln1Beta,
+        const void *queryWeight, const void *keyWeight, const void *valueWeight, const void *attnOutWeight,
+        const float *ln2Gamma, const float *ln2Beta, const void *gateWeight, const void *upWeight,
+        const void *downWeight, const float *queryBias = nullptr, const float *keyBias = nullptr,
+        const float *valueBias = nullptr, const float *attnOutBias = nullptr, const void *myqkvWeight = nullptr);
 
 } // namespace xft
\ No newline at end of file
diff --git a/src/layers/attention.h b/src/layers/attention.h
index 3c8ecb9..0690fd1 100644
--- a/src/layers/attention.h
+++ b/src/layers/attention.h
@@ -50,9 +50,7 @@ public:
         : layerId(layerId), qkpo(ctx->attHeadSize, ctx->maxPosEmbed), norm(ctx) {
 
         //todo(marvin): clear this code after all rotary_emb refactor
-        if constexpr (std::is_same<QKPO_CLS, LlamaRotaryEmbedding>::value) {
-            qkpo = LlamaRotaryEmbedding(ctx);
-        }
+        if constexpr (std::is_same<QKPO_CLS, LlamaRotaryEmbedding>::value) { qkpo = LlamaRotaryEmbedding(ctx); }
 
         // Group attention or multi-head attention (multi-head attn is a special case of group attn)
         if (ctx->attHeadNum % ctx->kvHeadNum == 0) {
@@ -82,7 +80,8 @@ public:
             const float *queryBias, const OriWeiT *keyWeight, const float *keyScale, const float *keyZero,
             const float *keyBias, const OriWeiT *valueWeight, const float *valueScale, const float *valueZero,
             const float *valueBias, const OriWeiT *attnOutWeight, const float *attnOutScale, const float *attnOutZero,
-            const float *attnOutBias, bool doLNorm, const float *gamma1, const float *beta1, bool trans = true) {
+            const float *attnOutBias, bool doLNorm, const float *gamma1, const float *beta1, bool trans = true,
+            const OriWeiT *myqkvWeight = nullptr) {
         int hiddenSize = ctx->hiddenSize;
         int headSize = ctx->attHeadSize;
 
@@ -93,31 +92,34 @@ public:
         int responsibleCols = qResponsibleCols + 2 * kvResponsibleCols;
 
         constexpr int sizeFactor = std::is_same_v<OriWeiT, uint4x2_t> ? 2 : 1;
-
         OriWeiT *concatBuf = (OriWeiT *)malloc(hiddenSize * responsibleCols * sizeof(OriWeiT) / sizeFactor);
-        if (trans) {
-            memcpy(concatBuf, queryWeight + this->startQHead * headSize * hiddenSize / sizeFactor,
-                    hiddenSize * qResponsibleCols * sizeof(OriWeiT) / sizeFactor);
-            memcpy(concatBuf + hiddenSize * qResponsibleCols / sizeFactor,
-                    keyWeight + this->startKVHead * headSize * hiddenSize / sizeFactor,
-                    hiddenSize * kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
-            memcpy(concatBuf + hiddenSize * (qResponsibleCols + kvResponsibleCols) / sizeFactor,
-                    valueWeight + this->startKVHead * headSize * hiddenSize / sizeFactor,
-                    hiddenSize * kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+        if (myqkvWeight != nullptr) {
+            memcpy(concatBuf, myqkvWeight, hiddenSize * responsibleCols * sizeof(OriWeiT) / sizeFactor);
         } else {
-            int qkvStride = (ctx->attHeadNum + ctx->kvHeadNum + ctx->kvHeadNum) * ctx->attHeadSize;
+            if (trans) {
+                memcpy(concatBuf, queryWeight + this->startQHead * headSize * hiddenSize / sizeFactor,
+                        hiddenSize * qResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                memcpy(concatBuf + hiddenSize * qResponsibleCols / sizeFactor,
+                        keyWeight + this->startKVHead * headSize * hiddenSize / sizeFactor,
+                        hiddenSize * kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                memcpy(concatBuf + hiddenSize * (qResponsibleCols + kvResponsibleCols) / sizeFactor,
+                        valueWeight + this->startKVHead * headSize * hiddenSize / sizeFactor,
+                        hiddenSize * kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+            } else {
+                int qkvStride = (ctx->attHeadNum + ctx->kvHeadNum + ctx->kvHeadNum) * ctx->attHeadSize;
 #pragma omp parallel for
-            for (int i = 0; i < hiddenSize; ++i) {
-                memcpy(concatBuf + i * responsibleCols / sizeFactor,
-                        queryWeight + i * qkvStride / sizeFactor + this->startQHead * headSize / sizeFactor,
-                        qResponsibleCols * sizeof(OriWeiT) / sizeFactor);
-                memcpy(concatBuf + i * responsibleCols / sizeFactor + qResponsibleCols / sizeFactor,
-                        keyWeight + i * qkvStride / sizeFactor + this->startKVHead * headSize / sizeFactor,
-                        kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
-                memcpy(concatBuf + i * responsibleCols / sizeFactor + qResponsibleCols / sizeFactor
-                                + kvResponsibleCols / sizeFactor,
-                        valueWeight + i * qkvStride / sizeFactor + this->startKVHead * headSize / sizeFactor,
-                        kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                for (int i = 0; i < hiddenSize; ++i) {
+                    memcpy(concatBuf + i * responsibleCols / sizeFactor,
+                            queryWeight + i * qkvStride / sizeFactor + this->startQHead * headSize / sizeFactor,
+                            qResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                    memcpy(concatBuf + i * responsibleCols / sizeFactor + qResponsibleCols / sizeFactor,
+                            keyWeight + i * qkvStride / sizeFactor + this->startKVHead * headSize / sizeFactor,
+                            kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                    memcpy(concatBuf + i * responsibleCols / sizeFactor + qResponsibleCols / sizeFactor
+                                    + kvResponsibleCols / sizeFactor,
+                            valueWeight + i * qkvStride / sizeFactor + this->startKVHead * headSize / sizeFactor,
+                            kvResponsibleCols * sizeof(OriWeiT) / sizeFactor);
+                }
             }
         }
         float *concatScale = nullptr;
diff --git a/src/layers/decoder_layer.cpp b/src/layers/decoder_layer.cpp
index 02f13cb..6fe5d58 100644
--- a/src/layers/decoder_layer.cpp
+++ b/src/layers/decoder_layer.cpp
@@ -27,13 +27,14 @@
 namespace xft {
 
 template <typename DataT, typename NormT>
-void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int batchSize, int inputSeqLen, int attHeadDim,
-        int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed, int pastSeqLen, int currentSeqLen, int step,
-        int hiddenSize, int intermediateSize, void *output, int outputStride, const void *input, int inputStride,
-        const float *ln1Gamma, const float *ln1Beta, const void *queryWeight, const void *keyWeight,
-        const void *valueWeight, const void *attnOutWeight, const float *ln2Gamma, const float *ln2Beta,
-        const void *gateWeight, const void *upWeight, const void *downWeight, const float *queryBias,
-        const float *keyBias, const float *valueBias, const float *attnOutBias) {
+void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int layerId, int totalLayers, int batchSize,
+        int inputSeqLen, int attHeadDim, int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed,
+        int pastSeqLen, int currentSeqLen, int step, int hiddenSize, int intermediateSize, void *output,
+        int outputStride, const void *input, int inputStride, const float *ln1Gamma, const float *ln1Beta,
+        const void *queryWeight, const void *keyWeight, const void *valueWeight, const void *attnOutWeight,
+        const float *ln2Gamma, const float *ln2Beta, const void *gateWeight, const void *upWeight,
+        const void *downWeight, const float *queryBias, const float *keyBias, const float *valueBias,
+        const float *attnOutBias, const void *myqkvWeight) {
 
     // TODO: will deprecate attention mask in future, so need to change this
     auto prepareAttnMask = [&](DecoderContext *ctx, int step) {
@@ -109,7 +110,7 @@ void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int batchSize,
         ctx = new DecoderContext(1, hiddenSize, attHeadDim, attHeadNum, kvHeadNum, intermediateSize, actType, 1e-6, 0,
                 0, maxPositions, maxPosEmbed, -1, 0, 1, mmHelper);
         if (kvCacheMgr != nullptr) delete kvCacheMgr;
-        kvCacheMgr = new KVCacheManager<float16_t>(1);
+        kvCacheMgr = new KVCacheManager<float16_t>(totalLayers);
     }
 
     // create hash key and value: if hidden and intermediateSize is changed , then memory pointer is also changed.
@@ -127,7 +128,8 @@ void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int batchSize,
                 nullptr, nullptr, keyBias, (const float *)valueWeight, nullptr, nullptr, valueBias,
                 (const float *)attnOutWeight, nullptr, nullptr, attnOutBias, ln1Gamma, ln1Beta,
                 (const float *)gateWeight, nullptr, nullptr, nullptr, (const float *)upWeight, nullptr, nullptr,
-                nullptr, ln2Gamma, ln2Beta, (const float *)downWeight, nullptr, nullptr, false);
+                nullptr, ln2Gamma, ln2Beta, (const float *)downWeight, nullptr, nullptr, false,
+                (const float *)myqkvWeight);
         llama_layer_hub[llama_layer_key] = llama_layer;
         printf(">> create llama_layer_key: %s\n", llama_layer_key.c_str());
     } else {
@@ -142,8 +144,8 @@ void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int batchSize,
     int workers = 1;
     int headsPerSplit = (ctx->kvHeadNum + workers - 1) / workers;
     kvCacheMgr->resize(maxPositions, batchSize, headsPerSplit, attHeadDim);
-    KVCacheTensor<float16_t> &presentKey = kvCacheMgr->getKey(0);
-    KVCacheTensor<float16_t> &presentValue = kvCacheMgr->getValue(0);
+    KVCacheTensor<float16_t> &presentKey = kvCacheMgr->getKey(layerId);
+    KVCacheTensor<float16_t> &presentValue = kvCacheMgr->getValue(layerId);
 
     float *attnOut = (float *)(ctx->tmpBuf.Data());
 
@@ -159,45 +161,46 @@ void LayerLLaMAImpl(DataType dt, ActivationType at, NormType nt, int batchSize,
     llama_layer->forwardFFN(ctx, attnOut, (float *)output, inputStride, outputStride, true);
 }
 
-void invokeLayerLLaMA(DataType dt, ActivationType at, NormType nt, int batchSize, int inputSeqLen, int attHeadDim,
-        int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed, int pastSeqLen, int currentSeqLen, int step,
-        int hiddenSize, int intermediateSize, void *output, int outputStride, const void *input, int inputStride,
-        const float *ln1Gamma, const float *ln1Beta, const void *queryWeight, const void *keyWeight,
-        const void *valueWeight, const void *attnOutWeight, const float *ln2Gamma, const float *ln2Beta,
-        const void *gateWeight, const void *upWeight, const void *downWeight, const float *queryBias,
-        const float *keyBias, const float *valueBias, const float *attnOutBias) {
+void invokeLayerLLaMA(DataType dt, ActivationType at, NormType nt, int layerId, int totalLayers, int batchSize,
+        int inputSeqLen, int attHeadDim, int attHeadNum, int kvHeadNum, int maxPositions, int maxPosEmbed,
+        int pastSeqLen, int currentSeqLen, int step, int hiddenSize, int intermediateSize, void *output,
+        int outputStride, const void *input, int inputStride, const float *ln1Gamma, const float *ln1Beta,
+        const void *queryWeight, const void *keyWeight, const void *valueWeight, const void *attnOutWeight,
+        const float *ln2Gamma, const float *ln2Beta, const void *gateWeight, const void *upWeight,
+        const void *downWeight, const float *queryBias, const float *keyBias, const float *valueBias,
+        const float *attnOutBias, const void *myqkvWeight) {
     static std::mutex mutex;
     std::lock_guard<std::mutex> lock(mutex);
 
     if (dt == DataType::bf16) {
         if (nt == NormType::RMS)
-            LayerLLaMAImpl<bfloat16_t, RmsNorm>(dt, at, nt, batchSize, inputSeqLen, attHeadDim, attHeadNum, kvHeadNum,
-                    maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize, intermediateSize, output,
-                    outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight, keyWeight, valueWeight,
-                    attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight, queryBias, keyBias, valueBias,
-                    attnOutBias);
+            LayerLLaMAImpl<bfloat16_t, RmsNorm>(dt, at, nt, layerId, totalLayers, batchSize, inputSeqLen, attHeadDim,
+                    attHeadNum, kvHeadNum, maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize,
+                    intermediateSize, output, outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight,
+                    keyWeight, valueWeight, attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight,
+                    queryBias, keyBias, valueBias, attnOutBias, myqkvWeight);
         else if (nt == NormType::LN) {
-            LayerLLaMAImpl<bfloat16_t, LayerNorm>(dt, at, nt, batchSize, inputSeqLen, attHeadDim, attHeadNum, kvHeadNum,
-                    maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize, intermediateSize, output,
-                    outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight, keyWeight, valueWeight,
-                    attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight, queryBias, keyBias, valueBias,
-                    attnOutBias);
+            LayerLLaMAImpl<bfloat16_t, LayerNorm>(dt, at, nt, layerId, totalLayers, batchSize, inputSeqLen, attHeadDim,
+                    attHeadNum, kvHeadNum, maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize,
+                    intermediateSize, output, outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight,
+                    keyWeight, valueWeight, attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight,
+                    queryBias, keyBias, valueBias, attnOutBias, myqkvWeight);
         } else {
             printf(">> unsupported norm type\n");
         }
     } else if (dt == DataType::fp16) {
         if (nt == NormType::RMS)
-            LayerLLaMAImpl<float16_t, RmsNorm>(dt, at, nt, batchSize, inputSeqLen, attHeadDim, attHeadNum, kvHeadNum,
-                    maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize, intermediateSize, output,
-                    outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight, keyWeight, valueWeight,
-                    attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight, queryBias, keyBias, valueBias,
-                    attnOutBias);
+            LayerLLaMAImpl<float16_t, RmsNorm>(dt, at, nt, layerId, totalLayers, batchSize, inputSeqLen, attHeadDim,
+                    attHeadNum, kvHeadNum, maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize,
+                    intermediateSize, output, outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight,
+                    keyWeight, valueWeight, attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight,
+                    queryBias, keyBias, valueBias, attnOutBias, myqkvWeight);
         else if (nt == NormType::LN) {
-            LayerLLaMAImpl<float16_t, LayerNorm>(dt, at, nt, batchSize, inputSeqLen, attHeadDim, attHeadNum, kvHeadNum,
-                    maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize, intermediateSize, output,
-                    outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight, keyWeight, valueWeight,
-                    attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight, queryBias, keyBias, valueBias,
-                    attnOutBias);
+            LayerLLaMAImpl<float16_t, LayerNorm>(dt, at, nt, layerId, totalLayers, batchSize, inputSeqLen, attHeadDim,
+                    attHeadNum, kvHeadNum, maxPositions, maxPosEmbed, pastSeqLen, currentSeqLen, step, hiddenSize,
+                    intermediateSize, output, outputStride, input, inputStride, ln1Gamma, ln1Beta, queryWeight,
+                    keyWeight, valueWeight, attnOutWeight, ln2Gamma, ln2Beta, gateWeight, upWeight, downWeight,
+                    queryBias, keyBias, valueBias, attnOutBias, myqkvWeight);
         } else {
             printf(">> unsupported norm type\n");
         }
diff --git a/src/layers/decoder_layer.h b/src/layers/decoder_layer.h
index 3cb5873..3896505 100644
--- a/src/layers/decoder_layer.h
+++ b/src/layers/decoder_layer.h
@@ -83,10 +83,10 @@ public:
             const float *fc1Scales, const float *fc1Zeros, const float *fc1Bias, const OriWeiT *fc2Weight,
             const float *fc2Scales, const float *fc2Zeros, const float *fc2Bias, const float *ln2Gamma,
             const float *ln2Beta, const OriWeiT *fc3Weight, const float *fc3Scales, const float *fc3Zeros,
-            bool trans = true) {
+            bool trans = true, const OriWeiT *myqkvWeight = nullptr) {
         attn.setWeights(ctx, queryWeight, queryScale, queryZero, queryBias, keyWeight, keyScale, keyZero, keyBias,
                 valueWeight, valueScale, valueZero, valueBias, attnOutWeight, attnOutScale, attnOutZero, attnOutBias,
-                true, ln1Gamma, ln1Beta, trans);
+                true, ln1Gamma, ln1Beta, trans, myqkvWeight);
 
         mlp.setWeights(ctx, fc1Weight, fc1Scales, fc1Zeros, fc1Bias, fc2Weight, fc2Scales, fc2Zeros, fc2Bias, ln2Gamma,
                 ln2Beta, fc3Weight, fc3Scales, fc3Zeros, trans);
@@ -114,7 +114,8 @@ public:
     }
 
     template <typename InT, typename OutT>
-    void forwardFFN(DecoderContext *ctx, InT *input, OutT *output, int iStride, int oStride, bool doLnBefore = true, int totInSeqLen = 0) {
+    void forwardFFN(DecoderContext *ctx, InT *input, OutT *output, int iStride, int oStride, bool doLnBefore = true,
+            int totInSeqLen = 0) {
         TimeLine t("Decoder.forwardFFN");
         mlp.forward(ctx, input, output, iStride, oStride, doLnBefore, totInSeqLen);
     }
-- 
2.25.1

