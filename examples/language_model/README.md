# PaddleNLP 语言模型预训练

自Transformer推出以来，类似BERT预训练模型，极大的提高了各种NLP任务下的使用效果。
如何高效的从零开始，训练出通用的预训练模型是很多用户急需的能力。

本目录为用户提供了很多预训练模型，其中部分模型提供了预训练流程。

## 模型预训练支持情况

目前PaddleNLP中，提供预训练流程支持的模型，可以根据数据流使用方式，分为两种。

- 简单版数据流：从原始文本读取数据。
    - 此方式类似于finetune流程。每次加载原始文本，tokenizer转换一遍后用于预训练流程
    - 优点：读取原始文本，处理成本小。训练和数据处理一提，无感知。
    - 缺点：转换后数据id没有落盘，每次加载要重新处理。处理的数据量级较小。
    - 改进方案：使用datasets包提供的cache功能，保存转换后的id，随机访问。
    - 注：**此类数据流程，paddlenlp预训练后续计划统一到改进方案，支持cache功能。**
- 高效版数据流：原始文本转化id保存后训练
    - 此方式适用于大规模数据集预训练，一次性将文本换id后，保存为其他格式。
    - 优点：可以支持较大规模。
    - 缺点：数据处理与训练分离，用户使用成本较高。
    - 注：**此类数据流程，paddlenlp目前主推的是npy-mmap格式，此格式功能。** hdf5格式仅供原始bert模型使用。

下表是目前预训练支持情况：

|模型|数据格式 | 预处理代码 | 大规模数据支持度 | 说明 |
|-|-|-|-|-
| [ERNIE-1.0](./ernie-1.0)| npy mmap | [链接](./data_tools) | 高 | 与[Megatron](https://github.com/NVIDIA/Megatron-LM)数据格式类似，高效，可复现性好
| [GPT](./gpt) | npy mmap| 同上 | 高 | 同上 |
| [GPT-3](./gpt-3) | npy mmap | 同上 | 高 | 同上
| [BERT](./bert) | hdf5 | [链接](./pretraining_data_prepare)| 高  | 数据格式与官方Bert一致 |
| [RoBERTa](./roberta) | txt转datasets cache | 无 | 中 | 数据处理转换为id后，cache保存，加载快 |
| [ELECTRA](./electra) | 文本txt | 无 | 低 | 采用原始文本读取，每次load需要处理一遍数据，类似finetune流程 |
| [BIG-BIRD](./bigbird) | 文本txt | 无 | 低 | 同上
| [ConvBert](./convbert) | 文本txt | 无 | 低 | 同上


其中，ERNIE-1.0 预训练功能支持较为完善。训练日志可视化，断点重启等功能支持较为完备。用户可以优先参考。

## RoadMap
后续paddlenlp预训练方向，计划做如下发展改进：

- **功能完善** 简单版数据流，全面支持datasets模块的cache功能。
- **预训练权重复现** 提供大部分模型，paddlenlp复现的权重效果。
- **预料，教程** 开放或集成更多预训练预料，提供更多完整训练教程
- **预训练流程新增**，更多模型开放预训练流程，敬请期待。
