{
  "attention_probs_dropout_prob": 0.0, 
  "hidden_dropout_prob": 0.0, 
  "pre_encoder_cmd": "n",
  "preprocess_cmd": "",
  "postprocess_cmd": "an",
  "hidden_act": "gelu.approximate", 
  "emb_size": 512, 
  "hidden_size": 12288, 
  "initializer_range": 0.02, 
  "max_position_embeddings": 512, 
  "num_attention_heads": 96, 
  "num_hidden_layers": 96,
  "sent_type_vocab_size": 4, 
  "task_type_vocab_size": 16, 
  "vocab_size": 30000,
  "epsilon": 1e-12,
  "param_share": "outer_share",
  "emb_mapping_in": true,
  "n_layer_per_block": 1
}
