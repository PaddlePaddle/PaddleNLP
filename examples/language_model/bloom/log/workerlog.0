/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:22:21,430] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:22:21,430] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:22:21,431] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:22:21,432] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:22:21,433] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:22:21,434] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:22:21.435717 29537 tcp_utils.cc:181] The server starts to listen on IP_ANY:42203
I0322 18:22:21.435973 29537 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42203
W0322 18:22:23.998164 29537 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:22:24.002604 29537 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:22:26,332] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:22:28,389] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:22:28,981] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:22:28,982] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:22:29,963] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:22:29,964] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 18:22:30,258] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:22:30,260] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:22:30,290] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:22:30,333] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:22:30,334] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:22:31,802] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:23:08,549] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:23:08,549] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:23:08,550] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:23:08,551] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:23:08,552] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:23:08,553] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:23:08,554] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:23:08,554] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:23:08,554] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:23:08,554] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:23:08,554] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:23:08.555243 37560 tcp_utils.cc:181] The server starts to listen on IP_ANY:42210
I0322 18:23:08.555552 37560 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42210
W0322 18:23:10.841780 37560 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:23:10.846040 37560 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:23:13,206] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:23:18,117] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:23:18,698] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:23:18,699] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:23:19,789] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:23:19,789] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:23:20,114] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:23:20,116] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:23:20,144] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:23:20,188] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:23:20,188] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:23:21,548] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:25:42,234] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:25:42,235] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:25:42,236] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:25:42,237] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:25:42,238] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:25:42,239] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:25:42.240728 22010 tcp_utils.cc:181] The server starts to listen on IP_ANY:61873
I0322 18:25:42.240955 22010 tcp_utils.cc:130] Successfully connected to 10.255.129.12:61873
W0322 18:25:44.689951 22010 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:25:44.694378 22010 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:25:46,602] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:25:48,456] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:25:49,066] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:25:49,066] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:25:50,034] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:25:50,035] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:25:50,298] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:25:50,300] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:25:50,327] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:25:50,370] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:25:50,370] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:25:52,169] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
result shape [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:28:01,725] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:28:01,726] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:28:01,727] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:28:01,728] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:28:01,729] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:28:01,730] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:28:01.731652  2581 tcp_utils.cc:181] The server starts to listen on IP_ANY:41229
I0322 18:28:01.731885  2581 tcp_utils.cc:130] Successfully connected to 10.255.129.12:41229
W0322 18:28:04.382102  2581 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:28:04.386674  2581 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:28:06,476] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:28:14,252] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:28:14,820] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:28:14,820] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:28:15,792] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:28:15,793] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:28:16,073] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:28:16,076] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:28:16,104] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:28:16,148] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:28:16,148] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:28:17,968] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
result shape [2, 285, 3072]
fused qkv [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1097, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 916, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 569, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 403, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 361, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:28:37,056] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:28:37,056] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:28:37,056] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:37,056] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:37,056] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:28:37,057] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:28:37,058] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - lora                :None[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:28:37,059] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:28:37,060] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:28:37,061] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:28:37.061874  8277 tcp_utils.cc:181] The server starts to listen on IP_ANY:63217
I0322 18:28:37.062063  8277 tcp_utils.cc:130] Successfully connected to 10.255.129.12:63217
W0322 18:28:42.335521  8277 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:28:42.339720  8277 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:28:44,628] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:28:46,434] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:28:47,018] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:28:47,018] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:28:48,093] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:28:48,093] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[2023-03-22 18:28:48,096] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:28:48,117] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:28:48,155] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:28:48,155] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:28:49,242] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:31:58,094] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:31:58,095] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:31:58,096] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:31:58,097] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:31:58,098] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:31:58,099] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:31:58.100569  2005 tcp_utils.cc:181] The server starts to listen on IP_ANY:35107
I0322 18:31:58.100852  2005 tcp_utils.cc:130] Successfully connected to 10.255.129.12:35107
W0322 18:32:00.919639  2005 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:32:00.923893  2005 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:32:03,267] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:32:05,076] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:32:05,681] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:32:05,681] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:32:06,740] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:32:06,740] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:32:07,055] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:32:07,057] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:32:07,087] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:32:07,131] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:32:07,131] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:32:07,959] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1097, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 916, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 569, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 403, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 361, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:33:37,489] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:33:37,489] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:33:37,490] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:33:37,491] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:33:37,492] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:33:37,493] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:33:37.494915 17767 tcp_utils.cc:181] The server starts to listen on IP_ANY:52621
I0322 18:33:37.495177 17767 tcp_utils.cc:130] Successfully connected to 10.255.129.12:52621
W0322 18:33:39.883049 17767 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:33:39.891885 17767 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:33:42,133] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:33:46,102] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:33:46,667] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:33:46,668] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:33:47,641] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:33:47,642] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:33:47,888] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:33:47,890] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:33:47,918] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:33:47,962] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:33:47,962] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:33:49,602] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:34:55,497] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:34:55,497] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:34:55,498] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:34:55,499] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:34:55,500] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:34:55,501] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:34:55,502] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:34:55,502] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:34:55,502] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:34:55,502] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:34:55.503381 29328 tcp_utils.cc:181] The server starts to listen on IP_ANY:39326
I0322 18:34:55.503651 29328 tcp_utils.cc:130] Successfully connected to 10.255.129.12:39326
W0322 18:34:58.111362 29328 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:34:58.115478 29328 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 18:35:00,504] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 18:35:02,358] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:35:02,886] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:35:02,886] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:35:03,944] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:35:03,945] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 238, in do_train
    model = get_lora_model(model, lora_config)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/paddlenlp/layers/lora.py", line 378, in get_lora_model
    _find_and_replace_module(model, module_name, lora_config)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/paddlenlp/layers/lora.py", line 225, in _find_and_replace_module
    has_bias=module.has_bias,
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1234, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'ColumnParallelLinear' object has no attribute 'has_bias'
I0322 18:35:04.418013 29834 tcp_store.cc:257] receive shutdown event and so quit from MasterDaemon run loop
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:17:21,490] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:17:21,490] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:17:21,491] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:17:21,492] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:17:21,493] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:17:21,494] [    INFO][0m - accumulate_steps    :4[0m
I0322 20:17:21.496239 14510 tcp_utils.cc:181] The server starts to listen on IP_ANY:60376
I0322 20:17:21.496562 14510 tcp_utils.cc:130] Successfully connected to 10.255.129.12:60376
W0322 20:17:26.634999 14510 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:17:26.639547 14510 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 20:17:28,634] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 20:17:30,818] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:17:31,385] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:17:31,385] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:17:32,382] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:17:32,382] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:17:32,632] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:17:32,634] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:17:32,661] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:17:32,704] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:17:32,704] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:17:34,110] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
[32m[2023-03-22 20:17:41,493] [    INFO][0m - global step 10, epoch: 0, loss: 6.247573853, avg_reader_cost: 1.69935 sec, avg_batch_cost: 1.86204 sec, speed: 0.54 step/s, ips_total: 4399 tokens/s, ips: 2200 tokens/s, learning rate: 1.00000e-05[0m
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:17:58,562] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:17:58,562] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:17:58,562] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:58,562] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:17:58,563] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:17:58,564] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:17:58,565] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:17:58,566] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:17:58,567] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:17:58,568] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:17:58,568] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:17:58,568] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:17:58,568] [    INFO][0m - accumulate_steps    :4[0m
I0322 20:17:58.569773 20500 tcp_utils.cc:181] The server starts to listen on IP_ANY:42922
I0322 20:17:58.570107 20500 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42922
W0322 20:18:03.781965 20500 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:18:03.786504 20500 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 20:18:05,837] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 20:18:07,616] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:18:08,275] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:18:08,275] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:18:09,380] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:18:09,381] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:18:09,644] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:18:09,646] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:18:09,673] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:18:09,716] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:18:09,716] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:18:11,502] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
[32m[2023-03-22 20:18:18,844] [    INFO][0m - global step 10, epoch: 0, loss: 6.247573471, avg_reader_cost: 1.82314 sec, avg_batch_cost: 1.98793 sec, speed: 0.50 step/s, ips_total: 4121 tokens/s, ips: 2060 tokens/s, learning rate: 1.00000e-05[0m
[32m[2023-03-22 20:18:24,045] [    INFO][0m - global step 20, epoch: 0, loss: 6.205389786, avg_reader_cost: 0.75167 sec, avg_batch_cost: 0.88629 sec, speed: 1.13 step/s, ips_total: 9243 tokens/s, ips: 4622 tokens/s, learning rate: 2.00000e-05[0m
[32m[2023-03-22 20:18:29,529] [    INFO][0m - global step 30, epoch: 0, loss: 6.209151459, avg_reader_cost: 0.76778 sec, avg_batch_cost: 0.91858 sec, speed: 1.09 step/s, ips_total: 8918 tokens/s, ips: 4459 tokens/s, learning rate: 3.00000e-05[0m
[32m[2023-03-22 20:18:35,191] [    INFO][0m - global step 40, epoch: 0, loss: 5.945185852, avg_reader_cost: 0.76291 sec, avg_batch_cost: 0.93923 sec, speed: 1.06 step/s, ips_total: 8722 tokens/s, ips: 4361 tokens/s, learning rate: 4.00000e-05[0m
[32m[2023-03-22 20:18:40,653] [    INFO][0m - global step 50, epoch: 0, loss: 5.686618042, avg_reader_cost: 0.74595 sec, avg_batch_cost: 0.90722 sec, speed: 1.10 step/s, ips_total: 9030 tokens/s, ips: 4515 tokens/s, learning rate: 5.00000e-05[0m
[32m[2023-03-22 20:18:46,582] [    INFO][0m - global step 60, epoch: 0, loss: 5.319855881, avg_reader_cost: 0.81902 sec, avg_batch_cost: 0.98270 sec, speed: 1.02 step/s, ips_total: 8336 tokens/s, ips: 4168 tokens/s, learning rate: 6.00000e-05[0m
[32m[2023-03-22 20:18:51,948] [    INFO][0m - global step 70, epoch: 0, loss: 4.724104691, avg_reader_cost: 0.76184 sec, avg_batch_cost: 0.91111 sec, speed: 1.10 step/s, ips_total: 8991 tokens/s, ips: 4496 tokens/s, learning rate: 7.00000e-05[0m
[32m[2023-03-22 20:18:58,008] [    INFO][0m - global step 80, epoch: 0, loss: 3.950780106, avg_reader_cost: 0.84662 sec, avg_batch_cost: 1.02363 sec, speed: 0.98 step/s, ips_total: 8003 tokens/s, ips: 4001 tokens/s, learning rate: 8.00000e-05[0m
[32m[2023-03-22 20:19:03,670] [    INFO][0m - global step 90, epoch: 0, loss: 3.358022308, avg_reader_cost: 0.78567 sec, avg_batch_cost: 0.95240 sec, speed: 1.05 step/s, ips_total: 8601 tokens/s, ips: 4301 tokens/s, learning rate: 9.00000e-05[0m
[32m[2023-03-22 20:19:09,405] [    INFO][0m - global step 100, epoch: 0, loss: 2.598823166, avg_reader_cost: 0.81918 sec, avg_batch_cost: 0.98056 sec, speed: 1.02 step/s, ips_total: 8354 tokens/s, ips: 4177 tokens/s, learning rate: 1.00000e-04[0m
[32m[2023-03-22 20:19:47,727] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:19:47,728] [    INFO][0m - valid step 100, batch: 123, loss: 2.301438, speed: 3.21 step/s[0m
[32m[2023-03-22 20:19:47,728] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:19:47,730] [    INFO][0m - Save model to output_generate/100/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:19:48,019] [    INFO][0m - Configuration saved in output_generate/100/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:19:57,953] [    INFO][0m - global step 110, epoch: 0, loss: 2.256070328, avg_reader_cost: 0.82859 sec, avg_batch_cost: 1.00290 sec, speed: 1.00 step/s, ips_total: 8168 tokens/s, ips: 4084 tokens/s, learning rate: 1.10000e-04[0m
[32m[2023-03-22 20:20:04,276] [    INFO][0m - global step 120, epoch: 0, loss: 1.663255692, avg_reader_cost: 1.01134 sec, avg_batch_cost: 1.16730 sec, speed: 0.86 step/s, ips_total: 7018 tokens/s, ips: 3509 tokens/s, learning rate: 1.20000e-04[0m
[32m[2023-03-22 20:20:09,566] [    INFO][0m - global step 130, epoch: 0, loss: 1.476925468, avg_reader_cost: 0.75836 sec, avg_batch_cost: 0.90158 sec, speed: 1.11 step/s, ips_total: 9086 tokens/s, ips: 4543 tokens/s, learning rate: 1.30000e-04[0m
[32m[2023-03-22 20:20:14,992] [    INFO][0m - global step 140, epoch: 0, loss: 1.412066746, avg_reader_cost: 0.76570 sec, avg_batch_cost: 0.92205 sec, speed: 1.08 step/s, ips_total: 8885 tokens/s, ips: 4442 tokens/s, learning rate: 1.40000e-04[0m
[32m[2023-03-22 20:20:20,596] [    INFO][0m - global step 150, epoch: 0, loss: 1.341370296, avg_reader_cost: 0.72894 sec, avg_batch_cost: 0.90719 sec, speed: 1.10 step/s, ips_total: 9030 tokens/s, ips: 4515 tokens/s, learning rate: 1.50000e-04[0m
[32m[2023-03-22 20:20:26,191] [    INFO][0m - global step 160, epoch: 0, loss: 1.377981091, avg_reader_cost: 0.78584 sec, avg_batch_cost: 0.94488 sec, speed: 1.06 step/s, ips_total: 8670 tokens/s, ips: 4335 tokens/s, learning rate: 1.60000e-04[0m
[32m[2023-03-22 20:20:31,204] [    INFO][0m - global step 170, epoch: 0, loss: 1.418717575, avg_reader_cost: 0.71890 sec, avg_batch_cost: 0.84999 sec, speed: 1.18 step/s, ips_total: 9638 tokens/s, ips: 4819 tokens/s, learning rate: 1.70000e-04[0m
[32m[2023-03-22 20:20:36,686] [    INFO][0m - global step 180, epoch: 0, loss: 1.497397041, avg_reader_cost: 0.77848 sec, avg_batch_cost: 0.93115 sec, speed: 1.07 step/s, ips_total: 8798 tokens/s, ips: 4399 tokens/s, learning rate: 1.80000e-04[0m
[32m[2023-03-22 20:20:43,796] [    INFO][0m - global step 190, epoch: 0, loss: 1.283533382, avg_reader_cost: 0.98558 sec, avg_batch_cost: 1.23659 sec, speed: 0.81 step/s, ips_total: 6625 tokens/s, ips: 3312 tokens/s, learning rate: 1.90000e-04[0m
[32m[2023-03-22 20:20:49,720] [    INFO][0m - global step 200, epoch: 0, loss: 1.311210346, avg_reader_cost: 0.81628 sec, avg_batch_cost: 0.99422 sec, speed: 1.01 step/s, ips_total: 8240 tokens/s, ips: 4120 tokens/s, learning rate: 2.00000e-04[0m
[32m[2023-03-22 20:21:28,182] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:21:28,182] [    INFO][0m - valid step 200, batch: 123, loss: 1.198414, speed: 3.20 step/s[0m
[32m[2023-03-22 20:21:28,182] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:21:28,184] [    INFO][0m - Save model to output_generate/200/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:21:28,463] [    INFO][0m - Configuration saved in output_generate/200/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:21:37,657] [    INFO][0m - global step 210, epoch: 0, loss: 1.196023560, avg_reader_cost: 0.69236 sec, avg_batch_cost: 0.84690 sec, speed: 1.18 step/s, ips_total: 9673 tokens/s, ips: 4836 tokens/s, learning rate: 2.10000e-04[0m
[32m[2023-03-22 20:21:43,520] [    INFO][0m - global step 220, epoch: 0, loss: 1.342715740, avg_reader_cost: 0.83694 sec, avg_batch_cost: 0.99601 sec, speed: 1.00 step/s, ips_total: 8225 tokens/s, ips: 4112 tokens/s, learning rate: 2.20000e-04[0m
[32m[2023-03-22 20:21:49,049] [    INFO][0m - global step 230, epoch: 0, loss: 1.245574284, avg_reader_cost: 0.80731 sec, avg_batch_cost: 0.96016 sec, speed: 1.04 step/s, ips_total: 8532 tokens/s, ips: 4266 tokens/s, learning rate: 2.30000e-04[0m
[32m[2023-03-22 20:21:54,592] [    INFO][0m - global step 240, epoch: 0, loss: 1.190822315, avg_reader_cost: 0.77007 sec, avg_batch_cost: 0.93412 sec, speed: 1.07 step/s, ips_total: 8770 tokens/s, ips: 4385 tokens/s, learning rate: 2.40000e-04[0m
[32m[2023-03-22 20:21:59,978] [    INFO][0m - global step 250, epoch: 0, loss: 1.218515778, avg_reader_cost: 0.73493 sec, avg_batch_cost: 0.90167 sec, speed: 1.11 step/s, ips_total: 9085 tokens/s, ips: 4543 tokens/s, learning rate: 2.50000e-04[0m
[32m[2023-03-22 20:22:07,002] [    INFO][0m - global step 260, epoch: 0, loss: 1.288566017, avg_reader_cost: 1.06594 sec, avg_batch_cost: 1.22942 sec, speed: 0.81 step/s, ips_total: 6663 tokens/s, ips: 3332 tokens/s, learning rate: 2.60000e-04[0m
[32m[2023-03-22 20:22:12,514] [    INFO][0m - global step 270, epoch: 0, loss: 1.196944141, avg_reader_cost: 0.74291 sec, avg_batch_cost: 0.91271 sec, speed: 1.10 step/s, ips_total: 8975 tokens/s, ips: 4488 tokens/s, learning rate: 2.70000e-04[0m
[32m[2023-03-22 20:22:17,891] [    INFO][0m - global step 280, epoch: 0, loss: 1.295417976, avg_reader_cost: 0.73535 sec, avg_batch_cost: 0.90319 sec, speed: 1.11 step/s, ips_total: 9070 tokens/s, ips: 4535 tokens/s, learning rate: 2.80000e-04[0m
[32m[2023-03-22 20:22:23,449] [    INFO][0m - global step 290, epoch: 0, loss: 1.236049747, avg_reader_cost: 0.78647 sec, avg_batch_cost: 0.93912 sec, speed: 1.06 step/s, ips_total: 8723 tokens/s, ips: 4362 tokens/s, learning rate: 2.90000e-04[0m
[32m[2023-03-22 20:22:28,833] [    INFO][0m - global step 300, epoch: 0, loss: 1.405035019, avg_reader_cost: 0.74672 sec, avg_batch_cost: 0.90621 sec, speed: 1.10 step/s, ips_total: 9040 tokens/s, ips: 4520 tokens/s, learning rate: 3.00000e-04[0m
[32m[2023-03-22 20:23:08,092] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:23:08,092] [    INFO][0m - valid step 300, batch: 123, loss: 1.134963, speed: 3.13 step/s[0m
[32m[2023-03-22 20:23:08,092] [    INFO][0m - ------------------------------------------------------------[0m
[32m[2023-03-22 20:23:08,094] [    INFO][0m - Save model to output_generate/300/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:23:08,400] [    INFO][0m - Configuration saved in output_generate/300/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:23:17,825] [    INFO][0m - global step 310, epoch: 0, loss: 1.301109123, avg_reader_cost: 0.71846 sec, avg_batch_cost: 0.88817 sec, speed: 1.13 step/s, ips_total: 9223 tokens/s, ips: 4612 tokens/s, learning rate: 3.10000e-04[0m
[32m[2023-03-22 20:23:23,208] [    INFO][0m - global step 320, epoch: 0, loss: 1.280304432, avg_reader_cost: 0.75970 sec, avg_batch_cost: 0.90236 sec, speed: 1.11 step/s, ips_total: 9078 tokens/s, ips: 4539 tokens/s, learning rate: 3.20000e-04[0m
[32m[2023-03-22 20:23:28,688] [    INFO][0m - global step 330, epoch: 0, loss: 1.120679188, avg_reader_cost: 0.75981 sec, avg_batch_cost: 0.92867 sec, speed: 1.08 step/s, ips_total: 8821 tokens/s, ips: 4411 tokens/s, learning rate: 3.30000e-04[0m
[32m[2023-03-22 20:23:34,258] [    INFO][0m - global step 340, epoch: 0, loss: 1.155190849, avg_reader_cost: 0.76614 sec, avg_batch_cost: 0.92934 sec, speed: 1.08 step/s, ips_total: 8815 tokens/s, ips: 4407 tokens/s, learning rate: 3.40000e-04[0m
[32m[2023-03-22 20:23:40,031] [    INFO][0m - global step 350, epoch: 0, loss: 1.238403320, avg_reader_cost: 0.80526 sec, avg_batch_cost: 0.97520 sec, speed: 1.03 step/s, ips_total: 8400 tokens/s, ips: 4200 tokens/s, learning rate: 3.50000e-04[0m
[32m[2023-03-22 20:23:45,689] [    INFO][0m - global step 360, epoch: 0, loss: 1.170107651, avg_reader_cost: 0.76244 sec, avg_batch_cost: 0.93271 sec, speed: 1.07 step/s, ips_total: 8783 tokens/s, ips: 4392 tokens/s, learning rate: 3.60000e-04[0m
[32m[2023-03-22 20:23:51,557] [    INFO][0m - global step 370, epoch: 0, loss: 1.341378307, avg_reader_cost: 0.81418 sec, avg_batch_cost: 0.98734 sec, speed: 1.01 step/s, ips_total: 8297 tokens/s, ips: 4149 tokens/s, learning rate: 3.70000e-04[0m
[32m[2023-03-22 20:23:57,378] [    INFO][0m - global step 380, epoch: 0, loss: 0.991841030, avg_reader_cost: 0.79794 sec, avg_batch_cost: 0.96269 sec, speed: 1.04 step/s, ips_total: 8510 tokens/s, ips: 4255 tokens/s, learning rate: 3.80000e-04[0m
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:24:29,605] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:24:29,606] [    INFO][0m - micro_batch_size    :4[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:24:29,607] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:24:29,608] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:24:29,609] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:24:29,610] [    INFO][0m - accumulate_steps    :2[0m
I0322 20:24:29.611439 38179 tcp_utils.cc:181] The server starts to listen on IP_ANY:35974
I0322 20:24:29.611625 38179 tcp_utils.cc:130] Successfully connected to 10.255.129.12:35974
W0322 20:24:34.910676 38179 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:24:34.915158 38179 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 20:24:37,182] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 192, in do_train
    raise ValueError(
ValueError: Output directory (output_generate) already exists and is not empty. Use --overwrite_output_dir to overcome.
I0322 20:24:39.322660 39616 tcp_store.cc:257] receive shutdown event and so quit from MasterDaemon run loop
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
usage: finetune_generation.py [-h] --model_type MODEL_TYPE
                              --model_name_or_path MODEL_NAME_OR_PATH
                              [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH]
                              --input_dir INPUT_DIR --output_dir OUTPUT_DIR
                              [--split SPLIT]
                              [--global_batch_size GLOBAL_BATCH_SIZE]
                              [--local_batch_size LOCAL_BATCH_SIZE]
                              [--micro_batch_size MICRO_BATCH_SIZE]
                              [--weight_decay WEIGHT_DECAY]
                              [--grad_clip GRAD_CLIP] [--max_lr MAX_LR]
                              [--min_lr MIN_LR] [--warmup_rate WARMUP_RATE]
                              [--adam_beta1 ADAM_BETA1]
                              [--adam_beta2 ADAM_BETA2]
                              [--adam_epsilon ADAM_EPSILON]
                              [--num_train_epochs NUM_TRAIN_EPOCHS]
                              [--max_steps MAX_STEPS]
                              [--save_steps SAVE_STEPS]
                              [--decay_steps DECAY_STEPS]
                              [--logging_freq LOGGING_FREQ]
                              [--eval_freq EVAL_FREQ]
                              [--eval_iters EVAL_ITERS]
                              [--fuse_transformer FUSE_TRANSFORMER]
                              [--sharding_degree SHARDING_DEGREE]
                              [--dp_degree DP_DEGREE] [--mp_degree MP_DEGREE]
                              [--pp_degree PP_DEGREE]
                              [--use_recompute [USE_RECOMPUTE]]
                              [--lora [LORA]]
                              [--sharding_stage SHARDING_STAGE]
                              [--sharding_offload [SHARDING_OFFLOAD]]
                              [--use_pure_fp16 [USE_PURE_FP16]]
                              [--scale_loss SCALE_LOSS]
                              [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]
                              [--attention_probs_dropout_prob ATTENTION_PROBS_DROPOUT_PROB]
                              [--to_static]
                              [--save_total_limit SAVE_TOTAL_LIMIT]
                              [--seed SEED]
                              [--check_accuracy [CHECK_ACCURACY]]
                              [--device {cpu,gpu,xpu,npu}]
                              [--lr_decay_style {cosine,linear,none}]
                              [-p PROFILER_OPTIONS]
                              [--task_name {cola,sst-2,mrpc,sts-b,qqp,mnli,qnli,rte}]
                              [--dataset_name DATASET_NAME]
                              [--max_seq_length MAX_SEQ_LENGTH]
                              [--max_source_length MAX_SOURCE_LENGTH]
                              [--max_target_length MAX_TARGET_LENGTH]
finetune_generation.py: error: unrecognized arguments: --overwrite_output_dir
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:25:31,669] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:25:31,669] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:25:31,669] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:25:31,669] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:25:31,669] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:25:31,669] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - micro_batch_size    :4[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:25:31,670] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:25:31,671] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:25:31,672] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:25:31,673] [    INFO][0m - accumulate_steps    :2[0m
I0322 20:25:31.674693  7747 tcp_utils.cc:181] The server starts to listen on IP_ANY:36872
I0322 20:25:31.674896  7747 tcp_utils.cc:130] Successfully connected to 10.255.129.12:36872
W0322 20:25:34.265812  7747 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:25:34.270380  7747 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.
[2023-03-22 20:25:36,205] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], check/clip group: [0, 1]
[32m[2023-03-22 20:25:38,305] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:25:38,897] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:25:38,898] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:25:39,942] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:25:39,942] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:25:40,191] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:25:40,193] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:25:40,220] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:25:40,263] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:25:40,263] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:25:41,569] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
[32m[2023-03-22 20:25:48,478] [    INFO][0m - global step 10, epoch: 0, loss: 6.224243546, avg_reader_cost: 0.57402 sec, avg_batch_cost: 0.86268 sec, speed: 1.16 step/s, ips_total: 9496 tokens/s, ips: 4748 tokens/s, learning rate: 1.00000e-05[0m
[32m[2023-03-22 20:25:52,527] [    INFO][0m - global step 20, epoch: 0, loss: 6.191883087, avg_reader_cost: 0.15606 sec, avg_batch_cost: 0.40499 sec, speed: 2.47 step/s, ips_total: 20228 tokens/s, ips: 10114 tokens/s, learning rate: 2.00000e-05[0m
[32m[2023-03-22 20:25:57,289] [    INFO][0m - global step 30, epoch: 0, loss: 6.192108536, avg_reader_cost: 0.20000 sec, avg_batch_cost: 0.47608 sec, speed: 2.10 step/s, ips_total: 17207 tokens/s, ips: 8604 tokens/s, learning rate: 3.00000e-05[0m
[32m[2023-03-22 20:26:02,378] [    INFO][0m - global step 40, epoch: 0, loss: 5.941948318, avg_reader_cost: 0.20290 sec, avg_batch_cost: 0.50904 sec, speed: 1.96 step/s, ips_total: 16093 tokens/s, ips: 8047 tokens/s, learning rate: 4.00000e-05[0m
[32m[2023-03-22 20:26:07,097] [    INFO][0m - global step 50, epoch: 0, loss: 5.673814392, avg_reader_cost: 0.17276 sec, avg_batch_cost: 0.47135 sec, speed: 2.12 step/s, ips_total: 17380 tokens/s, ips: 8690 tokens/s, learning rate: 5.00000e-05[0m
[32m[2023-03-22 20:26:12,247] [    INFO][0m - global step 60, epoch: 0, loss: 5.307836533, avg_reader_cost: 0.20143 sec, avg_batch_cost: 0.51512 sec, speed: 1.94 step/s, ips_total: 15903 tokens/s, ips: 7951 tokens/s, learning rate: 6.00000e-05[0m
[32m[2023-03-22 20:26:17,038] [    INFO][0m - global step 70, epoch: 0, loss: 4.704073715, avg_reader_cost: 0.18606 sec, avg_batch_cost: 0.47919 sec, speed: 2.09 step/s, ips_total: 17096 tokens/s, ips: 8548 tokens/s, learning rate: 7.00000e-05[0m
[32m[2023-03-22 20:26:21,677] [    INFO][0m - global step 80, epoch: 0, loss: 3.948439026, avg_reader_cost: 0.17223 sec, avg_batch_cost: 0.46397 sec, speed: 2.16 step/s, ips_total: 17656 tokens/s, ips: 8828 tokens/s, learning rate: 8.00000e-05[0m
