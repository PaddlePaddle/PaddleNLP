/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:22:21,712] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:22:21,713] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:22:21,714] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:22:21,715] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:22:21,716] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:22:21,717] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:22:21,717] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:22:21,717] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:22:21,717] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:22:21,717] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:22:21.718489 29540 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42203
W0322 18:22:24.494450 29540 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:22:24.498894 29540 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:22:26,232] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:22:28,064] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:22:28,598] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:22:28,598] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:22:29,583] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:22:29,584] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 18:22:29,884] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:22:29,887] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:22:29,914] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:22:29,958] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:22:29,958] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:22:31,803] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:23:08,587] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:23:08,587] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:23:08,587] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:23:08,587] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:23:08,588] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:23:08,589] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:23:08,590] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:23:08,591] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:23:08,592] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:23:08.592912 37562 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42210
W0322 18:23:11.369448 37562 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:23:11.374063 37562 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:23:13,106] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:23:14,898] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:23:15,458] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:23:15,459] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:23:16,426] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:23:16,426] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:23:16,670] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:23:16,673] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:23:16,700] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:23:16,744] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:23:16,744] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:23:21,550] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:25:42,477] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:25:42,477] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:25:42,477] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:25:42,477] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:25:42,477] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:25:42,477] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:25:42,478] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:25:42,479] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:25:42,480] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:25:42,481] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:25:42.482892 22012 tcp_utils.cc:130] Successfully connected to 10.255.129.12:61873
W0322 18:25:44.781908 22012 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:25:44.786026 22012 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:25:46,502] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:25:48,426] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:25:49,002] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:25:49,002] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:25:49,979] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:25:49,979] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:25:50,244] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:25:50,246] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:25:50,274] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:25:50,319] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:25:50,319] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:25:52,170] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
result shape [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1096, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 915, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 568, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 402, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 360, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:28:01,875] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:28:01,875] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:28:01,876] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:28:01,877] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:28:01,878] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:28:01,879] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:28:01.880744  2594 tcp_utils.cc:130] Successfully connected to 10.255.129.12:41229
W0322 18:28:04.659680  2594 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:28:04.664460  2594 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:28:06,376] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:28:08,242] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:28:08,784] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:28:08,785] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:28:09,782] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:28:09,783] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:28:10,050] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:28:10,053] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:28:10,079] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:28:10,123] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:28:10,124] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:28:17,971] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
result shape [2, 285, 3072]
fused qkv [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1097, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 916, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 569, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 403, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 361, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:28:37,010] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:28:37,011] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:28:37,012] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - lora                :None[0m
[32m[2023-03-22 18:28:37,013] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:28:37,014] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:28:37,015] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:28:37.016480  8279 tcp_utils.cc:107] Retry to connect to 10.255.129.12:63217 while the server is not yet listening.
I0322 18:28:40.016753  8279 tcp_utils.cc:130] Successfully connected to 10.255.129.12:63217
W0322 18:28:42.802599  8279 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:28:42.807126  8279 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:28:44,528] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:28:46,487] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:28:47,082] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:28:47,082] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:28:48,129] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:28:48,129] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[2023-03-22 18:28:48,132] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:28:48,152] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:28:48,191] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:28:48,192] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:28:49,242] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:31:58,452] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:31:58,452] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:31:58,452] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:31:58,452] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:31:58,452] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:31:58,452] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:31:58,453] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:31:58,454] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:31:58,455] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:31:58,456] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:31:58,457] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:31:58.457911  2007 tcp_utils.cc:130] Successfully connected to 10.255.129.12:35107
W0322 18:32:01.429978  2007 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:32:01.435530  2007 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:32:03,167] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:32:05,000] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:32:05,598] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:32:05,598] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:32:06,761] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:32:06,762] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:32:07,108] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:32:07,111] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:32:07,137] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:32:07,182] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:32:07,182] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:32:07,960] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 3072]
Traceback (most recent call last):
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 558, in <module>
    do_train(args)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/finetune_generation.py", line 422, in do_train
    loss, _ = model(**batch)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 39, in forward
    output = self._layers(*inputs, **kwargs)
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 1097, in forward
    transformer_outputs = self.bloom(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 916, in forward
    outputs = block(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 569, in forward
    attn_outputs = self.self_attention(
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/fluid/dygraph/layers.py", line 1012, in __call__
    return self.forward(*inputs, **kwargs)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 403, in forward
    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
  File "/ssd2/hesijun/workspace/PaddleNLP_sijun/examples/language_model/bloom/modeling.py", line 361, in _split_heads
    fused_qkv = fused_qkv.reshape([batch_size, seq_length, self.num_heads, 3, self.head_dim])
  File "/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/paddle/tensor/manipulation.py", line 3543, in reshape
    out = _C_ops.reshape(x, shape)
ValueError: (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [2, 285, 3072], X's size = 1751040, 'shape' is [2, 285, 8, 3, 64], the capacity of 'shape' is 875520.
  [Hint: Expected capacity == in_size, but received capacity:875520 != in_size:1751040.] (at /paddle/paddle/phi/infermeta/unary.cc:1435)

/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:33:37,642] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:33:37,643] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:33:37,644] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:33:37,645] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:33:37,646] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:33:37,647] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:33:37,647] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:33:37,647] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:33:37,647] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:33:37.648543 17769 tcp_utils.cc:130] Successfully connected to 10.255.129.12:52621
W0322 18:33:40.292321 17769 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:33:40.297077 17769 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:33:42,033] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 18:33:43,793] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 18:33:44,357] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 18:33:44,357] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 18:33:45,367] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 18:33:45,367] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
3072
[32m[2023-03-22 18:33:45,613] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 18:33:45,615] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 18:33:45,642] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 18:33:45,686] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 18:33:45,686] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 18:33:49,602] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 285, 1024]
input_mp shape [2, 285, 1024]
result_mp shape [2, 285, 1536]
input_a shape [2, 285, 4]
delta_mp shape [2, 285, 1536]
fused qkv [2, 285, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 210, 1024]
input_mp shape [2, 210, 1024]
result_mp shape [2, 210, 1536]
input_a shape [2, 210, 4]
delta_mp shape [2, 210, 1536]
fused qkv [2, 210, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 228, 1024]
input_mp shape [2, 228, 1024]
result_mp shape [2, 228, 1536]
input_a shape [2, 228, 4]
delta_mp shape [2, 228, 1536]
fused qkv [2, 228, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 117, 1024]
input_mp shape [2, 117, 1024]
result_mp shape [2, 117, 1536]
input_a shape [2, 117, 4]
delta_mp shape [2, 117, 1536]
fused qkv [2, 117, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 106, 1024]
input_mp shape [2, 106, 1024]
result_mp shape [2, 106, 1536]
input_a shape [2, 106, 4]
delta_mp shape [2, 106, 1536]
fused qkv [2, 106, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 347, 1024]
input_mp shape [2, 347, 1024]
result_mp shape [2, 347, 1536]
input_a shape [2, 347, 4]
delta_mp shape [2, 347, 1536]
fused qkv [2, 347, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 177, 1024]
input_mp shape [2, 177, 1024]
result_mp shape [2, 177, 1536]
input_a shape [2, 177, 4]
delta_mp shape [2, 177, 1536]
fused qkv [2, 177, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 499, 1024]
input_mp shape [2, 499, 1024]
result_mp shape [2, 499, 1536]
input_a shape [2, 499, 4]
delta_mp shape [2, 499, 1536]
fused qkv [2, 499, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 216, 1024]
input_mp shape [2, 216, 1024]
result_mp shape [2, 216, 1536]
input_a shape [2, 216, 4]
delta_mp shape [2, 216, 1536]
fused qkv [2, 216, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 152, 1024]
input_mp shape [2, 152, 1024]
result_mp shape [2, 152, 1536]
input_a shape [2, 152, 4]
delta_mp shape [2, 152, 1536]
fused qkv [2, 152, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 200, 1024]
input_mp shape [2, 200, 1024]
result_mp shape [2, 200, 1536]
input_a shape [2, 200, 4]
delta_mp shape [2, 200, 1536]
fused qkv [2, 200, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 138, 1024]
input_mp shape [2, 138, 1024]
result_mp shape [2, 138, 1536]
input_a shape [2, 138, 4]
delta_mp shape [2, 138, 1536]
fused qkv [2, 138, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 293, 1024]
input_mp shape [2, 293, 1024]
result_mp shape [2, 293, 1536]
input_a shape [2, 293, 4]
delta_mp shape [2, 293, 1536]
fused qkv [2, 293, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 187, 1024]
input_mp shape [2, 187, 1024]
result_mp shape [2, 187, 1536]
input_a shape [2, 187, 4]
delta_mp shape [2, 187, 1536]
fused qkv [2, 187, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 226, 1024]
input_mp shape [2, 226, 1024]
result_mp shape [2, 226, 1536]
input_a shape [2, 226, 4]
delta_mp shape [2, 226, 1536]
fused qkv [2, 226, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 157, 1024]
input_mp shape [2, 157, 1024]
result_mp shape [2, 157, 1536]
input_a shape [2, 157, 4]
delta_mp shape [2, 157, 1536]
fused qkv [2, 157, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 460, 1024]
input_mp shape [2, 460, 1024]
result_mp shape [2, 460, 1536]
input_a shape [2, 460, 4]
delta_mp shape [2, 460, 1536]
fused qkv [2, 460, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
input shape [2, 281, 1024]
input_mp shape [2, 281, 1024]
result_mp shape [2, 281, 1536]
input_a shape [2, 281, 4]
delta_mp shape [2, 281, 1536]
fused qkv [2, 281, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 18:34:55,858] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 18:34:55,858] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 18:34:55,858] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 18:34:55,858] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 18:34:55,859] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 18:34:55,860] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 18:34:55,861] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 18:34:55,862] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 18:34:55,863] [    INFO][0m - accumulate_steps    :4[0m
I0322 18:34:55.863920 29344 tcp_utils.cc:130] Successfully connected to 10.255.129.12:39326
W0322 18:34:58.679524 29344 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 18:34:58.684005 29344 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 18:35:00,404] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:17:21,449] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:17:21,449] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:17:21,450] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:17:21,451] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:17:21,452] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:17:21,453] [    INFO][0m - accumulate_steps    :4[0m
I0322 20:17:21.454881 14512 tcp_utils.cc:107] Retry to connect to 10.255.129.12:60376 while the server is not yet listening.
I0322 20:17:24.455209 14512 tcp_utils.cc:130] Successfully connected to 10.255.129.12:60376
W0322 20:17:26.813227 14512 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:17:26.817495 14512 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 20:17:28,534] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 20:17:30,440] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:17:30,993] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:17:30,993] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:17:31,953] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:17:31,953] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:17:32,205] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:17:32,207] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:17:32,235] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:17:32,281] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:17:32,281] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:17:34,113] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 285, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 210, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 228, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 117, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 106, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 347, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 177, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 499, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 152, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 200, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 138, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 293, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 187, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 226, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 157, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 460, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 281, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 139, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 246, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 522, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 225, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 261, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 216, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 184, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 333, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 272, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 215, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 251, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 396, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 523, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 158, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 190, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 213, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 91, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 257, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 107, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 231, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
fused qkv [2, 374, 1536]
[32m[2023-03-22 20:17:41,493] [    INFO][0m - global step 10, epoch: 0, loss: 6.247573853, avg_reader_cost: 1.51970 sec, avg_batch_cost: 1.68320 sec, speed: 0.59 step/s, ips_total: 4867 tokens/s, ips: 2433 tokens/s, learning rate: 1.00000e-05[0m
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 120, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
fused qkv [2, 243, 1536]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:17:58,515] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:17:58,515] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:17:58,516] [    INFO][0m - micro_batch_size    :2[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:17:58,517] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:17:58,518] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:17:58,519] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:17:58,520] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:17:58,521] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:17:58,522] [    INFO][0m - accumulate_steps    :4[0m
I0322 20:17:58.523597 20502 tcp_utils.cc:107] Retry to connect to 10.255.129.12:42922 while the server is not yet listening.
I0322 20:18:01.523950 20502 tcp_utils.cc:130] Successfully connected to 10.255.129.12:42922
W0322 20:18:04.011041 20502 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:18:04.015331 20502 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 20:18:05,737] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 20:18:07,659] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:18:08,272] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:18:08,273] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:18:09,298] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:18:09,299] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:18:09,565] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:18:09,568] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:18:09,594] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:18:09,639] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:18:09,640] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:18:11,502] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
[32m[2023-03-22 20:18:18,844] [    INFO][0m - global step 10, epoch: 0, loss: 6.247573471, avg_reader_cost: 1.52609 sec, avg_batch_cost: 1.69158 sec, speed: 0.59 step/s, ips_total: 4843 tokens/s, ips: 2421 tokens/s, learning rate: 1.00000e-05[0m
[32m[2023-03-22 20:18:24,045] [    INFO][0m - global step 20, epoch: 0, loss: 6.205389786, avg_reader_cost: 0.75047 sec, avg_batch_cost: 0.88560 sec, speed: 1.13 step/s, ips_total: 9250 tokens/s, ips: 4625 tokens/s, learning rate: 2.00000e-05[0m
[32m[2023-03-22 20:18:29,529] [    INFO][0m - global step 30, epoch: 0, loss: 6.209151459, avg_reader_cost: 0.76508 sec, avg_batch_cost: 0.91836 sec, speed: 1.09 step/s, ips_total: 8920 tokens/s, ips: 4460 tokens/s, learning rate: 3.00000e-05[0m
[32m[2023-03-22 20:18:35,191] [    INFO][0m - global step 40, epoch: 0, loss: 5.945185852, avg_reader_cost: 0.75976 sec, avg_batch_cost: 0.93395 sec, speed: 1.07 step/s, ips_total: 8771 tokens/s, ips: 4386 tokens/s, learning rate: 4.00000e-05[0m
[32m[2023-03-22 20:18:40,653] [    INFO][0m - global step 50, epoch: 0, loss: 5.686618042, avg_reader_cost: 0.74670 sec, avg_batch_cost: 0.90848 sec, speed: 1.10 step/s, ips_total: 9017 tokens/s, ips: 4509 tokens/s, learning rate: 5.00000e-05[0m
[32m[2023-03-22 20:18:46,582] [    INFO][0m - global step 60, epoch: 0, loss: 5.319855881, avg_reader_cost: 0.81719 sec, avg_batch_cost: 0.98306 sec, speed: 1.02 step/s, ips_total: 8333 tokens/s, ips: 4167 tokens/s, learning rate: 6.00000e-05[0m
[32m[2023-03-22 20:18:51,949] [    INFO][0m - global step 70, epoch: 0, loss: 4.724104691, avg_reader_cost: 0.76151 sec, avg_batch_cost: 0.91108 sec, speed: 1.10 step/s, ips_total: 8992 tokens/s, ips: 4496 tokens/s, learning rate: 7.00000e-05[0m
[32m[2023-03-22 20:18:58,009] [    INFO][0m - global step 80, epoch: 0, loss: 3.950780106, avg_reader_cost: 0.84905 sec, avg_batch_cost: 1.02785 sec, speed: 0.97 step/s, ips_total: 7970 tokens/s, ips: 3985 tokens/s, learning rate: 8.00000e-05[0m
[32m[2023-03-22 20:19:03,671] [    INFO][0m - global step 90, epoch: 0, loss: 3.358022308, avg_reader_cost: 0.78700 sec, avg_batch_cost: 0.95190 sec, speed: 1.05 step/s, ips_total: 8606 tokens/s, ips: 4303 tokens/s, learning rate: 9.00000e-05[0m
[32m[2023-03-22 20:19:09,405] [    INFO][0m - global step 100, epoch: 0, loss: 2.598823166, avg_reader_cost: 0.81351 sec, avg_batch_cost: 0.98137 sec, speed: 1.02 step/s, ips_total: 8347 tokens/s, ips: 4174 tokens/s, learning rate: 1.00000e-04[0m
[32m[2023-03-22 20:19:47,729] [    INFO][0m - Save model to output_generate/100/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:19:47,731] [    INFO][0m - Configuration saved in output_generate/100/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:19:57,953] [    INFO][0m - global step 110, epoch: 0, loss: 2.256070328, avg_reader_cost: 0.88466 sec, avg_batch_cost: 1.05861 sec, speed: 0.94 step/s, ips_total: 7738 tokens/s, ips: 3869 tokens/s, learning rate: 1.10000e-04[0m
[32m[2023-03-22 20:20:04,276] [    INFO][0m - global step 120, epoch: 0, loss: 1.663255692, avg_reader_cost: 1.01330 sec, avg_batch_cost: 1.16933 sec, speed: 0.86 step/s, ips_total: 7006 tokens/s, ips: 3503 tokens/s, learning rate: 1.20000e-04[0m
[32m[2023-03-22 20:20:09,566] [    INFO][0m - global step 130, epoch: 0, loss: 1.476925468, avg_reader_cost: 0.76013 sec, avg_batch_cost: 0.90231 sec, speed: 1.11 step/s, ips_total: 9079 tokens/s, ips: 4539 tokens/s, learning rate: 1.30000e-04[0m
[32m[2023-03-22 20:20:14,992] [    INFO][0m - global step 140, epoch: 0, loss: 1.412066746, avg_reader_cost: 0.76247 sec, avg_batch_cost: 0.91939 sec, speed: 1.09 step/s, ips_total: 8910 tokens/s, ips: 4455 tokens/s, learning rate: 1.40000e-04[0m
[32m[2023-03-22 20:20:20,595] [    INFO][0m - global step 150, epoch: 0, loss: 1.341370296, avg_reader_cost: 0.72751 sec, avg_batch_cost: 0.90432 sec, speed: 1.11 step/s, ips_total: 9059 tokens/s, ips: 4529 tokens/s, learning rate: 1.50000e-04[0m
[32m[2023-03-22 20:20:26,191] [    INFO][0m - global step 160, epoch: 0, loss: 1.377981091, avg_reader_cost: 0.77379 sec, avg_batch_cost: 0.93502 sec, speed: 1.07 step/s, ips_total: 8761 tokens/s, ips: 4381 tokens/s, learning rate: 1.60000e-04[0m
[32m[2023-03-22 20:20:31,204] [    INFO][0m - global step 170, epoch: 0, loss: 1.418717575, avg_reader_cost: 0.71782 sec, avg_batch_cost: 0.84898 sec, speed: 1.18 step/s, ips_total: 9649 tokens/s, ips: 4825 tokens/s, learning rate: 1.70000e-04[0m
[32m[2023-03-22 20:20:36,686] [    INFO][0m - global step 180, epoch: 0, loss: 1.497397041, avg_reader_cost: 0.77637 sec, avg_batch_cost: 0.93200 sec, speed: 1.07 step/s, ips_total: 8790 tokens/s, ips: 4395 tokens/s, learning rate: 1.80000e-04[0m
[32m[2023-03-22 20:20:43,796] [    INFO][0m - global step 190, epoch: 0, loss: 1.283533382, avg_reader_cost: 0.97557 sec, avg_batch_cost: 1.22881 sec, speed: 0.81 step/s, ips_total: 6667 tokens/s, ips: 3333 tokens/s, learning rate: 1.90000e-04[0m
[32m[2023-03-22 20:20:49,720] [    INFO][0m - global step 200, epoch: 0, loss: 1.311210346, avg_reader_cost: 0.81078 sec, avg_batch_cost: 0.98842 sec, speed: 1.01 step/s, ips_total: 8288 tokens/s, ips: 4144 tokens/s, learning rate: 2.00000e-04[0m
[32m[2023-03-22 20:21:28,183] [    INFO][0m - Save model to output_generate/200/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:21:28,186] [    INFO][0m - Configuration saved in output_generate/200/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:21:37,657] [    INFO][0m - global step 210, epoch: 0, loss: 1.196023560, avg_reader_cost: 0.75307 sec, avg_batch_cost: 0.90706 sec, speed: 1.10 step/s, ips_total: 9031 tokens/s, ips: 4516 tokens/s, learning rate: 2.10000e-04[0m
[32m[2023-03-22 20:21:43,520] [    INFO][0m - global step 220, epoch: 0, loss: 1.342715740, avg_reader_cost: 0.82638 sec, avg_batch_cost: 0.98671 sec, speed: 1.01 step/s, ips_total: 8302 tokens/s, ips: 4151 tokens/s, learning rate: 2.20000e-04[0m
[32m[2023-03-22 20:21:49,049] [    INFO][0m - global step 230, epoch: 0, loss: 1.245574284, avg_reader_cost: 0.80514 sec, avg_batch_cost: 0.95792 sec, speed: 1.04 step/s, ips_total: 8552 tokens/s, ips: 4276 tokens/s, learning rate: 2.30000e-04[0m
[32m[2023-03-22 20:21:54,592] [    INFO][0m - global step 240, epoch: 0, loss: 1.190822315, avg_reader_cost: 0.76853 sec, avg_batch_cost: 0.92986 sec, speed: 1.08 step/s, ips_total: 8810 tokens/s, ips: 4405 tokens/s, learning rate: 2.40000e-04[0m
[32m[2023-03-22 20:21:59,978] [    INFO][0m - global step 250, epoch: 0, loss: 1.218515778, avg_reader_cost: 0.72967 sec, avg_batch_cost: 0.89499 sec, speed: 1.12 step/s, ips_total: 9153 tokens/s, ips: 4577 tokens/s, learning rate: 2.50000e-04[0m
[32m[2023-03-22 20:22:07,003] [    INFO][0m - global step 260, epoch: 0, loss: 1.288566017, avg_reader_cost: 1.05972 sec, avg_batch_cost: 1.22181 sec, speed: 0.82 step/s, ips_total: 6705 tokens/s, ips: 3352 tokens/s, learning rate: 2.60000e-04[0m
[32m[2023-03-22 20:22:12,514] [    INFO][0m - global step 270, epoch: 0, loss: 1.196944141, avg_reader_cost: 0.74772 sec, avg_batch_cost: 0.91772 sec, speed: 1.09 step/s, ips_total: 8927 tokens/s, ips: 4463 tokens/s, learning rate: 2.70000e-04[0m
[32m[2023-03-22 20:22:17,891] [    INFO][0m - global step 280, epoch: 0, loss: 1.295417976, avg_reader_cost: 0.73572 sec, avg_batch_cost: 0.90302 sec, speed: 1.11 step/s, ips_total: 9072 tokens/s, ips: 4536 tokens/s, learning rate: 2.80000e-04[0m
[32m[2023-03-22 20:22:23,449] [    INFO][0m - global step 290, epoch: 0, loss: 1.236049747, avg_reader_cost: 0.78117 sec, avg_batch_cost: 0.93326 sec, speed: 1.07 step/s, ips_total: 8778 tokens/s, ips: 4389 tokens/s, learning rate: 2.90000e-04[0m
[32m[2023-03-22 20:22:28,833] [    INFO][0m - global step 300, epoch: 0, loss: 1.405035019, avg_reader_cost: 0.74302 sec, avg_batch_cost: 0.90543 sec, speed: 1.10 step/s, ips_total: 9048 tokens/s, ips: 4524 tokens/s, learning rate: 3.00000e-04[0m
[32m[2023-03-22 20:23:08,093] [    INFO][0m - Save model to output_generate/300/splits_mp_02_sharding_01[0m
[32m[2023-03-22 20:23:08,095] [    INFO][0m - Configuration saved in output_generate/300/splits_mp_02_sharding_01/config.json[0m
[32m[2023-03-22 20:23:17,825] [    INFO][0m - global step 310, epoch: 0, loss: 1.301109123, avg_reader_cost: 0.81489 sec, avg_batch_cost: 0.98351 sec, speed: 1.02 step/s, ips_total: 8329 tokens/s, ips: 4165 tokens/s, learning rate: 3.10000e-04[0m
[32m[2023-03-22 20:23:23,209] [    INFO][0m - global step 320, epoch: 0, loss: 1.280304432, avg_reader_cost: 0.76008 sec, avg_batch_cost: 0.90230 sec, speed: 1.11 step/s, ips_total: 9079 tokens/s, ips: 4540 tokens/s, learning rate: 3.20000e-04[0m
[32m[2023-03-22 20:23:28,689] [    INFO][0m - global step 330, epoch: 0, loss: 1.120679188, avg_reader_cost: 0.75625 sec, avg_batch_cost: 0.92780 sec, speed: 1.08 step/s, ips_total: 8829 tokens/s, ips: 4415 tokens/s, learning rate: 3.30000e-04[0m
[32m[2023-03-22 20:23:34,258] [    INFO][0m - global step 340, epoch: 0, loss: 1.155190849, avg_reader_cost: 0.76466 sec, avg_batch_cost: 0.93000 sec, speed: 1.08 step/s, ips_total: 8809 tokens/s, ips: 4404 tokens/s, learning rate: 3.40000e-04[0m
[32m[2023-03-22 20:23:40,032] [    INFO][0m - global step 350, epoch: 0, loss: 1.238403320, avg_reader_cost: 0.80530 sec, avg_batch_cost: 0.97532 sec, speed: 1.03 step/s, ips_total: 8399 tokens/s, ips: 4200 tokens/s, learning rate: 3.50000e-04[0m
[32m[2023-03-22 20:23:45,690] [    INFO][0m - global step 360, epoch: 0, loss: 1.170107651, avg_reader_cost: 0.76303 sec, avg_batch_cost: 0.93434 sec, speed: 1.07 step/s, ips_total: 8768 tokens/s, ips: 4384 tokens/s, learning rate: 3.60000e-04[0m
[32m[2023-03-22 20:23:51,557] [    INFO][0m - global step 370, epoch: 0, loss: 1.341378307, avg_reader_cost: 0.81383 sec, avg_batch_cost: 0.98879 sec, speed: 1.01 step/s, ips_total: 8285 tokens/s, ips: 4142 tokens/s, learning rate: 3.70000e-04[0m
[32m[2023-03-22 20:23:57,378] [    INFO][0m - global step 380, epoch: 0, loss: 0.991841030, avg_reader_cost: 0.79686 sec, avg_batch_cost: 0.96351 sec, speed: 1.04 step/s, ips_total: 8502 tokens/s, ips: 4251 tokens/s, learning rate: 3.80000e-04[0m
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:24:29,503] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:24:29,503] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:24:29,503] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - micro_batch_size    :4[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:24:29,504] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:24:29,505] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:24:29,506] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:24:29,507] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:24:29,508] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:24:29,508] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:24:29,508] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:24:29,508] [    INFO][0m - accumulate_steps    :2[0m
I0322 20:24:29.509174 38186 tcp_utils.cc:107] Retry to connect to 10.255.129.12:35974 while the server is not yet listening.
I0322 20:24:32.510901 38186 tcp_utils.cc:130] Successfully connected to 10.255.129.12:35974
W0322 20:24:35.370457 38186 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:24:35.375258 38186 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 20:24:37,082] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
usage: finetune_generation.py [-h] --model_type MODEL_TYPE
                              --model_name_or_path MODEL_NAME_OR_PATH
                              [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH]
                              --input_dir INPUT_DIR --output_dir OUTPUT_DIR
                              [--split SPLIT]
                              [--global_batch_size GLOBAL_BATCH_SIZE]
                              [--local_batch_size LOCAL_BATCH_SIZE]
                              [--micro_batch_size MICRO_BATCH_SIZE]
                              [--weight_decay WEIGHT_DECAY]
                              [--grad_clip GRAD_CLIP] [--max_lr MAX_LR]
                              [--min_lr MIN_LR] [--warmup_rate WARMUP_RATE]
                              [--adam_beta1 ADAM_BETA1]
                              [--adam_beta2 ADAM_BETA2]
                              [--adam_epsilon ADAM_EPSILON]
                              [--num_train_epochs NUM_TRAIN_EPOCHS]
                              [--max_steps MAX_STEPS]
                              [--save_steps SAVE_STEPS]
                              [--decay_steps DECAY_STEPS]
                              [--logging_freq LOGGING_FREQ]
                              [--eval_freq EVAL_FREQ]
                              [--eval_iters EVAL_ITERS]
                              [--fuse_transformer FUSE_TRANSFORMER]
                              [--sharding_degree SHARDING_DEGREE]
                              [--dp_degree DP_DEGREE] [--mp_degree MP_DEGREE]
                              [--pp_degree PP_DEGREE]
                              [--use_recompute [USE_RECOMPUTE]]
                              [--lora [LORA]]
                              [--sharding_stage SHARDING_STAGE]
                              [--sharding_offload [SHARDING_OFFLOAD]]
                              [--use_pure_fp16 [USE_PURE_FP16]]
                              [--scale_loss SCALE_LOSS]
                              [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]
                              [--attention_probs_dropout_prob ATTENTION_PROBS_DROPOUT_PROB]
                              [--to_static]
                              [--save_total_limit SAVE_TOTAL_LIMIT]
                              [--seed SEED]
                              [--check_accuracy [CHECK_ACCURACY]]
                              [--device {cpu,gpu,xpu,npu}]
                              [--lr_decay_style {cosine,linear,none}]
                              [-p PROFILER_OPTIONS]
                              [--task_name {cola,sst-2,mrpc,sts-b,qqp,mnli,qnli,rte}]
                              [--dataset_name DATASET_NAME]
                              [--max_seq_length MAX_SEQ_LENGTH]
                              [--max_source_length MAX_SOURCE_LENGTH]
                              [--max_target_length MAX_TARGET_LENGTH]
finetune_generation.py: error: unrecognized arguments: --overwrite_output_dir
/ssd2/hesijun/miniconda3/envs/paddle_ray_gpu/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[32m[2023-03-22 20:25:31,836] [    INFO][0m - paddle commit id    :0e92adceae06b6b7463f2dc7790ffb0601730009[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - model_type          :bloom[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - model_name_or_path  :bigscience/bloom-560m[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - tokenizer_name_or_path:bigscience/bloom-560m[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - input_dir           :old[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - output_dir          :output_generate[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - split               :949,50,1[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - global_batch_size   :8[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - local_batch_size    :8[0m
[32m[2023-03-22 20:25:31,837] [    INFO][0m - micro_batch_size    :4[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - weight_decay        :0.01[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - grad_clip           :1.0[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - max_lr              :0.0005[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - min_lr              :0.0001[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - warmup_rate         :0.01[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - adam_beta1          :0.9[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - adam_beta2          :0.999[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - adam_epsilon        :1e-08[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - num_train_epochs    :1[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - max_steps           :50000[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - save_steps          :100[0m
[32m[2023-03-22 20:25:31,838] [    INFO][0m - decay_steps         :320[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - logging_freq        :10[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - eval_freq           :100[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - eval_iters          :10[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - fuse_transformer    :False[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - sharding_degree     :1[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - dp_degree           :1[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - mp_degree           :2[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - pp_degree           :1[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - use_recompute       :None[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - lora                :True[0m
[32m[2023-03-22 20:25:31,839] [    INFO][0m - sharding_stage      :2[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - sharding_offload    :None[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - use_pure_fp16       :False[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - scale_loss          :1024.0[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - hidden_dropout_prob :0.1[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - attention_probs_dropout_prob:0.1[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - to_static           :False[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - save_total_limit    :3[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - seed                :1234[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - check_accuracy      :None[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - device              :gpu[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - lr_decay_style      :cosine[0m
[32m[2023-03-22 20:25:31,840] [    INFO][0m - profiler_options    :None[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - task_name           :sst-2[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - dataset_name        :squad[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - max_seq_length      :1024[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - max_source_length   :512[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - max_target_length   :512[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - test_iters          :100[0m
[32m[2023-03-22 20:25:31,841] [    INFO][0m - accumulate_steps    :2[0m
I0322 20:25:31.842644  7749 tcp_utils.cc:130] Successfully connected to 10.255.129.12:36872
W0322 20:25:34.379997  7749 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0322 20:25:34.384521  7749 gpu_resources.cc:91] device: 1, cuDNN Version: 7.6.
[2023-03-22 20:25:36,105] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1]
[32m[2023-03-22 20:25:37,864] [    INFO][0m - Model config BloomConfig {
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "BloomModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "bias_dropout_fusion": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_dropout": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "masked_softmax_fusion": true,
  "model_type": "bloom",
  "mp_degree": 1,
  "mp_rank": -1,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "offset_alibi": 100,
  "pad_token_id": 3,
  "paddlenlp_version": null,
  "pp_degree": 1,
  "pretraining_tp": 1,
  "skip_bias_add": true,
  "skip_bias_add_qkv": false,
  "slow_but_exact": false,
  "unk_token_id": 0,
  "use_pure_fp16": false,
  "use_recompute": false,
  "vocab_size": 250880
}
[0m
[33m[2023-03-22 20:25:38,419] [ WARNING][0m - Accessing `vocab_size` through `model.vocab_size` will be deprecated after v2.6.0. Instead, do `model.config.vocab_size`[0m
[33m[2023-03-22 20:25:38,419] [ WARNING][0m - Accessing `hidden_size` through `model.hidden_size` will be deprecated after v2.6.0. Instead, do `model.config.hidden_size`[0m
[32m[2023-03-22 20:25:39,382] [    INFO][0m - All model checkpoint weights were used when initializing BloomForCausalLM.
[0m
[33m[2023-03-22 20:25:39,382] [ WARNING][0m - Some weights of BloomForCausalLM were not initialized from the model checkpoint at /ssd2/hesijun/.paddlenlp/models/bigscience/bloom-560m/splits_mp_02_sharding_01 and are newly initialized: ['lm_head.decoder_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-03-22 20:25:39,629] [    INFO][0m - Frozen parameters: 4.08e+08 || Trainable parameters:2.46e+05 || Total parameters:4.08e+08|| Trainable:0.06%[0m
[2023-03-22 20:25:39,631] [    INFO] tensor_parallel.py:31 - start broadcast mp parameters
[2023-03-22 20:25:39,657] [    INFO] tensor_parallel.py:38 - start broadcast dp parameters
[2023-03-22 20:25:39,700] [    INFO] tensor_parallel.py:41 - mp's parameters is ready
[2023-03-22 20:25:39,701] [ WARNING] hybrid_parallel_optimizer.py:193 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[33m[2023-03-22 20:25:41,573] [ WARNING][0m - Accessing `mp_rank` through `model.mp_rank` will be deprecated after v2.6.0. Instead, do `model.config.mp_rank`[0m
[32m[2023-03-22 20:25:48,478] [    INFO][0m - global step 10, epoch: 0, loss: 6.224243546, avg_reader_cost: 0.52411 sec, avg_batch_cost: 0.81055 sec, speed: 1.23 step/s, ips_total: 10107 tokens/s, ips: 5053 tokens/s, learning rate: 1.00000e-05[0m
[32m[2023-03-22 20:25:52,527] [    INFO][0m - global step 20, epoch: 0, loss: 6.191883087, avg_reader_cost: 0.15592 sec, avg_batch_cost: 0.40496 sec, speed: 2.47 step/s, ips_total: 20229 tokens/s, ips: 10115 tokens/s, learning rate: 2.00000e-05[0m
[32m[2023-03-22 20:25:57,290] [    INFO][0m - global step 30, epoch: 0, loss: 6.192108536, avg_reader_cost: 0.20068 sec, avg_batch_cost: 0.47634 sec, speed: 2.10 step/s, ips_total: 17198 tokens/s, ips: 8599 tokens/s, learning rate: 3.00000e-05[0m
[32m[2023-03-22 20:26:02,378] [    INFO][0m - global step 40, epoch: 0, loss: 5.941948318, avg_reader_cost: 0.20405 sec, avg_batch_cost: 0.50896 sec, speed: 1.96 step/s, ips_total: 16096 tokens/s, ips: 8048 tokens/s, learning rate: 4.00000e-05[0m
[32m[2023-03-22 20:26:07,097] [    INFO][0m - global step 50, epoch: 0, loss: 5.673814392, avg_reader_cost: 0.17319 sec, avg_batch_cost: 0.47202 sec, speed: 2.12 step/s, ips_total: 17355 tokens/s, ips: 8678 tokens/s, learning rate: 5.00000e-05[0m
[32m[2023-03-22 20:26:12,247] [    INFO][0m - global step 60, epoch: 0, loss: 5.307836533, avg_reader_cost: 0.20158 sec, avg_batch_cost: 0.51512 sec, speed: 1.94 step/s, ips_total: 15903 tokens/s, ips: 7952 tokens/s, learning rate: 6.00000e-05[0m
[32m[2023-03-22 20:26:17,038] [    INFO][0m - global step 70, epoch: 0, loss: 4.704073715, avg_reader_cost: 0.18672 sec, avg_batch_cost: 0.47921 sec, speed: 2.09 step/s, ips_total: 17095 tokens/s, ips: 8547 tokens/s, learning rate: 7.00000e-05[0m
[32m[2023-03-22 20:26:21,677] [    INFO][0m - global step 80, epoch: 0, loss: 3.948439026, avg_reader_cost: 0.17231 sec, avg_batch_cost: 0.46399 sec, speed: 2.16 step/s, ips_total: 17656 tokens/s, ips: 8828 tokens/s, learning rate: 8.00000e-05[0m
