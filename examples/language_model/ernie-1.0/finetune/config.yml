# Default Args for all dataset 
# You can overwrite the configs in each dataset.
DefaultArgs:
    learning_rate: 0.00005
    num_train_epochs: 3
    batch_size: 64
    max_seq_length: 128
    weight_decay: 0.0
    logging_steps: 10
    valid_steps: 100
    minimum_valid_times: 20 # If under valid_steps, the valid time is less then 20, the config of valid_steps will be changed.
    max_steps: -1
    warmup_steps: 0
    metric: "Accuracy"
    split: "train dev"

# Datasets which used for sequence classfication
SequenceClassification:
    clue afqmc: 
        num_train_epochs: 4
    clue tnews:
        num_train_epochs: 4
    clue iflytek:
        num_train_epochs: 8
    clue ocnli:
        num_train_epochs: 8
    clue cmnli: 
        learning_rate: 1e-4, 5e-5, 1e-5
        num_train_epochs: 3
    clue wsc: 
        num_train_epochs: 50
    clue csl:
        num_train_epochs: 10
        max_seq_length: 256
        batch_size: 32
    xnli_cn:
        learning_rate: 0.00005
        num_train_epochs: 3
    chnsenticorp_v2:
        learning_rate: 0.00001
        num_train_epochs: 5

# Datasets which used for token classfication
TokenClassification:
    peoples_daily_ner:
        num_train_epochs: 5
    msra_ner:
        num_train_epochs: 3

# Datasets which used for question answersing
QuestionAnswering:
    cmrc2018:
        num_train_epochs: 1
        batch_size: 12
        max_seq_length: 384
    dureader_nlp:
        num_train_epochs: 1
        batch_size: 12
        max_seq_length: 384
    dureader_robust:
        num_train_epochs: 1
        batch_size: 12
        max_seq_length: 384
    dlbp:
        num_train_epochs: 1
        batch_size: 12
        max_seq_length: 384