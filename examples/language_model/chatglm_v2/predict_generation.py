# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
os.environ["CUDA_VISIBLE_DEVICES"]="5"

import paddle
from paddle.distributed import fleet

from paddlenlp.transformers import (
    ChatGLMv2Config,
    ChatGLMv2Tokenizer,
)

from paddlenlp.transformers import ChatGLMv2ForCausalLM

def parse_arguments():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", default="THUDM/chatglm2-6b", help="The directory of model.")
    parser.add_argument("--batch_size", type=int, default=2, help="The batch size of data.")
    parser.add_argument("--src_length", type=int, default=1280, help="The batch size of data.")
    parser.add_argument("--tgt_length", type=int, default=1280, help="The batch size of data.")
    return parser.parse_args()


def batchfy_text(texts, batch_size):
    batch_texts = []
    batch_start = 0
    while batch_start < len(texts):
        batch_texts += [texts[batch_start : min(batch_start + batch_size, len(texts))]]
        batch_start += batch_size
    return batch_texts


class Predictor(object):
    def __init__(self, args=None, tokenizer=None, model=None, **kwargs):
        if args is None:
            self.tokenizer = tokenizer
            self.model = model
            self.src_length = kwargs["src_length"]
            self.tgt_length = kwargs["tgt_length"]
        else:
            self.tokenizer = ChatGLMv2Tokenizer.from_pretrained(args.model_name_or_path)
            self.batch_size = args.batch_size
            self.args = args
            self.src_length = self.args.src_length
            self.tgt_length = self.args.tgt_length

            tensor_parallel_degree = paddle.distributed.get_world_size()
            tensor_parallel_rank = 0
            if tensor_parallel_degree > 1:
                strategy = fleet.DistributedStrategy()
                strategy.hybrid_configs = {
                    "dp_degree": 1,
                    "mp_degree": tensor_parallel_degree,
                    "pp_degree": 1,
                    "sharding_degree": 1,
                }
                fleet.init(is_collective=True, strategy=strategy)
                hcg = fleet.get_hybrid_communicate_group()
                tensor_parallel_rank = hcg.get_model_parallel_rank()

            config = ChatGLMv2Config.from_pretrained(args.model_name_or_path)
            dtype = config.dtype if config.dtype is not None else config.paddle_dtype

            self.model = ChatGLMv2ForCausalLM.from_pretrained(
                args.model_name_or_path,
                tensor_parallel_degree=tensor_parallel_degree,
                tensor_parallel_rank=tensor_parallel_rank,
                dtype=dtype,
            )
        self.model.eval()

    def preprocess(self, input_text):
        inputs = self.tokenizer(
            input_text,
            return_tensors="pd",
            padding=True,
            max_length=self.src_length,
            truncation=True,
            truncation_side="left",
        )
        return inputs

    def infer(self, inputs):

        # static_model = paddle.jit.to_static(
        #     self.model.encoder.layers[0],
        #     input_spec=[
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  # input_ids
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  # attention_mask
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  #position_ids
        #     ],
            
        # )
        # paddle.jit.save(static_model, "./static_model/inference")

        # static_model = paddle.jit.to_static(
        #     self.model.haha,
        #     input_spec=[
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  # input_ids
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  # attention_mask
        #         paddle.static.InputSpec(shape=[None, None], dtype="int64"),  #position_ids
        #     ],
            
        # )
        # paddle.jit.save(static_model, "./static_model/inference")
        # #exit(0)
        # print(self.tgt_length)
        # print(self.tokenizer.bos_token_id)
        # print(self.tokenizer.eos_token_id)
        # print(self.tokenizer.pad_token_id)
        result = self.model.generate(
            **inputs,
            decode_strategy="sampling",
            top_k=1,
            max_length=self.tgt_length,
            bos_token_id=self.tokenizer.bos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            use_cache=False,
        )
        result = result[0]
        return result

    def postprocess(self, infer_data):
        result = []
        for x in infer_data.tolist():
            res = self.tokenizer.decode(x, skip_special_tokens=True)
            res = res.strip("\n")
            result.append(res)
        out_dict = {"result": result}
        return out_dict

    def predict(self, texts):
        input_map = self.preprocess(texts)
        #print(input_map)
        infer_result = self.infer(input_map)
        output = self.postprocess(infer_result)
        return output


if __name__ == "__main__":
    args = parse_arguments()
    predictor = Predictor(args)
    all_texts = [
        #"[Round 0]\né—®ï¼šä½ å¥½\nç­”ï¼šä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n[Round 1]\né—®ï¼šæ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\nç­”ï¼š",
    "<0>å±±ä¸œå¸ˆèŒƒå¤§å­¦åšå£«å­¦ä½è®ºæ–‡##129,16,199,21</0><1>æ„â€œå¤§å˜â€çš„æˆè¯­ã€‚å¼‚ä½“æˆè¯­çš„è¿ç”¨ä½“ç°å‡ºè¿™ä¸€æ—¶æœŸå¾ˆå¤šæˆè¯­çš„æ„æˆè¦ç´ å¹¶ä¸ååˆ†å›ºå®šï¼Œ##36,30,287,35</1><2>æˆè¯­ç»“æ„ä¹Ÿå¹¶ä¸ååˆ†ç¨³å›ºï¼Œå‘ˆç°å‡ºæˆè¯­åœ¨ç»“æ„ä¸Šä¸æ–­å‘ç”Ÿæ¼”å˜çš„è¿‡æ¸¡æ€§ç‰¹å¾ï¼Œä»¥åŠè¿‘ä»£æ±‰##36,39,291,44</2><3>è¯­è¯æ±‡å‘ç°ä»£æ±‰è¯­è¯æ±‡å‘å±•è¿‡ç¨‹ä¸­çš„è¿‡æ¸¡æ€§ç‰¹å¾ã€‚##36,49,177,54</3><4>æœ¬ç« å°ç»“##146,62,180,69</4><5>æœ¬ç« æˆ‘ä»¬åœ¨å€Ÿé‰´å‰äººå¯¹æˆè¯­çš„ç ”ç©¶æˆæœä¸ç•Œå®šçš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†å¯¹æˆè¯­çš„ç•Œå®šï¼šæˆè¯­##48,77,291,83</5><6>æ˜¯äººä»¬é•¿æœŸä»¥æ¥ç›¸æ²¿ä¹ ç”¨çš„ï¼Œæ„ä¹‰ç›¸å¯¹å®Œæ•´ã€ç»“æ„ç›¸å¯¹ç¨³å®šçš„å›ºå®šè¯ç»„ã€‚æˆè¯­çš„å…¸å‹è¡¨ç°##36,87,291,92</6><7>å½¢å¼ä¸ºå››å­—ç»“æ„ï¼Œå¤šå…·æœ‰ä¹¦é¢è¯­è‰²å½©ã€‚è¿›è€Œæå‡ºäº†å¼‚ä½“æˆè¯­çš„å®šä¹‰ï¼šå¼‚ä½“æˆè¯­æ˜¯æ•´ä½“æ„ä¹‰##36,96,290,102</7><8>åŸºæœ¬ç›¸åŒï¼Œå½¢å¼ä¸Šè‡³å°‘æœ‰ä¸€ä¸ªç›¸åŒçš„æ„æˆè¦ç´ å¹¶ä¸”å…¶ä»–ç›¸å¼‚è¦ç´ å­˜åœ¨ç€äº’ç›¸æ›¿æ¢çš„æ„ä¹‰å…³##36,106,291,111</8><9>ç³»ï¼Œå…·æœ‰ç›¸åŒè¯­æ³•æ€§è´¨ï¼Œç»“æ„ç¨³å®šçš„ä¸€ç»„è¯æ±‡ç±»èšã€‚æ¸…æœ«æ°‘åˆæ—¶æœŸå‡ºç°åœ¨ç™½è¯æŠ¥åˆŠä¸­çš„å¼‚##36,115,291,121</9><10>ä½“æˆè¯­ï¼Œä¾¿æ˜¯æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­ã€‚æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­ä¸­åœ¨å½“æ—¶ä½¿ç”¨é¢‘ç‡æœ€é«˜çš„å˜ä½“æˆ‘ä»¬ç§°##36,125,291,130</10><11>ä¸ºæ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­çš„é€šä½“ï¼Œå…¶ä»–ç§°ä¸ºæ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­çš„å˜ä½“ã€‚##36,135,223,140</11><12>æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­å¯ä»¥ä»å½¢å¼ã€è¯­ä¹‰ã€ç»“æ„ç­‰æ–¹é¢åˆ†ç±»åˆ†æï¼Œæ€»ç»“åˆ†å¸ƒç‰¹å¾ã€‚æ¸…æœ«æ°‘##48,144,291,149</12><13>åˆå¼‚ä½“æˆè¯­åœ¨æ•°é‡ä¸åˆ†å¸ƒä¸Šå…·æœ‰ç¹å¤æ€§ï¼Œåœ¨ç±»å‹ä¸Šå…·æœ‰å¤šæ ·åŒ–ç‰¹å¾ï¼Œåœ¨é£æ ¼è‰²å½©ä¸Šå…·æœ‰ç™½##35,153,290,159</13><14>è¯åŒ–ç‰¹å¾ï¼Œåœ¨æ•´ä½“è¿ç”¨ä¸Šå…·æœ‰è¿‡æ¸¡æ€§ç‰¹å¾ç­‰ã€‚##36,163,164,168</14><15>43##36,296,44,300</15><16>ä¸‡æ–¹æ•°æ®##26,310,48,315</16>",
        "[Round 0]\né—®ï¼šä½ å¥½\nç­”ï¼šä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n[Round 1]\né—®ï¼šæ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\nç­”ï¼š",
     #"<0>å±±ä¸œå¸ˆèŒƒå¤§å­¦åšå£«å­¦ä½è®ºæ–‡##129,16,199,21</0><1>æ„â€œå¤§å˜â€çš„æˆè¯­ã€‚å¼‚ä½“æˆè¯­çš„è¿ç”¨ä½“ç°å‡ºè¿™ä¸€æ—¶æœŸå¾ˆå¤šæˆè¯­çš„æ„æˆè¦ç´ å¹¶ä¸ååˆ†å›ºå®šï¼Œ##36,30,287,35</1><2>æˆè¯­ç»“æ„ä¹Ÿå¹¶ä¸ååˆ†ç¨³å›ºï¼Œå‘ˆç°å‡ºæˆè¯­åœ¨ç»“æ„ä¸Šä¸æ–­å‘ç”Ÿæ¼”å˜çš„è¿‡æ¸¡æ€§ç‰¹å¾ï¼Œä»¥åŠè¿‘ä»£æ±‰##36,39,291,44</2><3>è¯­è¯æ±‡å‘ç°ä»£æ±‰è¯­è¯æ±‡å‘å±•è¿‡ç¨‹ä¸­çš„è¿‡æ¸¡æ€§ç‰¹å¾ã€‚##36,49,177,54</3><4>æœ¬ç« å°ç»“##146,62,180,69</4><5>æœ¬ç« æˆ‘ä»¬åœ¨å€Ÿé‰´å‰äººå¯¹æˆè¯­çš„ç ”ç©¶æˆæœä¸ç•Œå®šçš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†å¯¹æˆè¯­çš„ç•Œå®šï¼šæˆè¯­##48,77,291,83</5><6>æ˜¯äººä»¬é•¿æœŸä»¥æ¥ç›¸æ²¿ä¹ ç”¨çš„ï¼Œæ„ä¹‰ç›¸å¯¹å®Œæ•´ã€ç»“æ„ç›¸å¯¹ç¨³å®šçš„å›ºå®šè¯ç»„ã€‚æˆè¯­çš„å…¸å‹è¡¨ç°##36,87,291,92</6><7>å½¢å¼ä¸ºå››å­—ç»“æ„ï¼Œå¤šå…·æœ‰ä¹¦é¢è¯­è‰²å½©ã€‚è¿›è€Œæå‡ºäº†å¼‚ä½“æˆè¯­çš„å®šä¹‰ï¼šå¼‚ä½“æˆè¯­æ˜¯æ•´ä½“æ„ä¹‰##36,96,290,102</7><8>åŸºæœ¬ç›¸åŒï¼Œå½¢å¼ä¸Šè‡³å°‘æœ‰ä¸€ä¸ªç›¸åŒçš„æ„æˆè¦ç´ å¹¶ä¸”å…¶ä»–ç›¸å¼‚è¦ç´ å­˜åœ¨ç€äº’ç›¸æ›¿æ¢çš„æ„ä¹‰å…³##36,106,291,111</8><9>ç³»ï¼Œå…·æœ‰ç›¸åŒè¯­æ³•æ€§è´¨ï¼Œç»“æ„ç¨³å®šçš„ä¸€ç»„è¯æ±‡ç±»èšã€‚æ¸…æœ«æ°‘åˆæ—¶æœŸå‡ºç°åœ¨ç™½è¯æŠ¥åˆŠä¸­çš„å¼‚##36,115,291,121</9><10>ä½“æˆè¯­ï¼Œä¾¿æ˜¯æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­ã€‚æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­ä¸­åœ¨å½“æ—¶ä½¿ç”¨é¢‘ç‡æœ€é«˜çš„å˜ä½“æˆ‘ä»¬ç§°##36,125,291,130</10><11>ä¸ºæ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­çš„é€šä½“ï¼Œå…¶ä»–ç§°ä¸ºæ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­çš„å˜ä½“ã€‚##36,135,223,140</11><12>æ¸…æœ«æ°‘åˆå¼‚ä½“æˆè¯­å¯ä»¥ä»å½¢å¼ã€è¯­ä¹‰ã€ç»“æ„ç­‰æ–¹é¢åˆ†ç±»åˆ†æï¼Œæ€»ç»“åˆ†å¸ƒç‰¹å¾ã€‚æ¸…æœ«æ°‘##48,144,291,149</12><13>åˆå¼‚ä½“æˆè¯­åœ¨æ•°é‡ä¸åˆ†å¸ƒä¸Šå…·æœ‰ç¹å¤æ€§ï¼Œåœ¨ç±»å‹ä¸Šå…·æœ‰å¤šæ ·åŒ–ç‰¹å¾ï¼Œåœ¨é£æ ¼è‰²å½©ä¸Šå…·æœ‰ç™½##35,153,290,159</13><14>è¯åŒ–ç‰¹å¾ï¼Œåœ¨æ•´ä½“è¿ç”¨ä¸Šå…·æœ‰è¿‡æ¸¡æ€§ç‰¹å¾ç­‰ã€‚##36,163,164,168</14><15>43##36,296,44,300</15><16>ä¸‡æ–¹æ•°æ®##26,310,48,315</16>",
    ]

    for i in range (1):
        batch_texts = batchfy_text(all_texts, args.batch_size)
        for bs, texts in enumerate(batch_texts):
            for i in range(10):
                import datetime
                starttime = datetime.datetime.now()
                
                outputs = predictor.predict(texts)

                endtime = datetime.datetime.now()
                duringtime = endtime - starttime
                print ("è€—æ—¶:",duringtime.seconds * 1000 + duringtime.microseconds / 1000.0)
            
            for text, result in zip(texts, outputs["result"]):
                print("{}\n{}".format(text, result))

    import datetime
    import time
    starttime = datetime.datetime.now()

    for i in range (1):
        batch_texts = batchfy_text(all_texts, args.batch_size)
        for bs, texts in enumerate(batch_texts):
            outputs = predictor.predict(texts)
            for text, result in zip(texts, outputs["result"]):
                print("{}\n{}".format(text, result))
    
    endtime = datetime.datetime.now()
    duringtime = endtime - starttime
    print (duringtime.seconds * 1000 + duringtime.microseconds / 1000.0)# å•ä½æ˜¯æ¯«ç§’
    # 104090
