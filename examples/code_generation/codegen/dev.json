{"code": "import argparse\nimport pprint\nimport mxnet as mx\n\nfrom ..logger import logger\nfrom ..config import config, default, generate_config\nfrom ..symbol import *\nfrom ..core import callback, metric\nfrom ..core.loader import ROIIter\nfrom ..core.module import MutableModule\nfrom ..processing.bbox_regression import add_bbox_regression_targets\nfrom ..utils.load_data import load_proposal_roidb, merge_roidb, filter_roidb\nfrom ..utils.load_model import load_param\n\n\ndef train_rcnn(network, dataset, image_set, root_path, dataset_path, frequent,\n               kvstore, work_load_list, no_flip, no_shuffle, resume, ctx,\n               pretrained, epoch, prefix, begin_epoch, end_epoch, train_shared,\n               lr, lr_step, proposal):\n    # set up config\n    config.TRAIN.BATCH_IMAGES = 2\n    config.TRAIN.BATCH_ROIS = 128\n    if proposal == 'ss':\n        config.TRAIN.BG_THRESH_LO = 0.1  # reproduce Fast R-CNN\n\n    # load symbol\n    sym = eval('get_' + network + '_rcnn')(num_classes=config.NUM_CLASSES)\n\n    # setup multi-gpu\n    batch_size = len(ctx)\n    input_batch_size = config.TRAIN.BATCH_IMAGES * batch_size\n\n    # print config\n    logger.info(pprint.pformat(config))\n\n    # load dataset and prepare imdb for training\n    image_sets = [iset for iset in image_set.split('+')]\n    roidbs = [\n        load_proposal_roidb(dataset,\n                            image_set,\n                            root_path,\n                            dataset_path,\n                            proposal=proposal,\n                            append_gt=True,\n                            flip=not no_flip) for image_set in image_sets\n    ]\n    roidb = merge_roidb(roidbs)\n    roidb = filter_roidb(roidb)\n    means, stds = add_bbox_regression_targets(roidb)\n\n    # load training data\n    train_data = ROIIter(roidb,\n                         batch_size=input_batch_size,\n                         shuffle=not no_shuffle,\n                         ctx=ctx,\n                         work_load_list=work_load_list,\n                         aspect_grouping=config.TRAIN.ASPECT_GROUPING)\n\n    # infer max shape\n    max_data_shape = [('data', (input_batch_size, 3,\n                                max([v[0] for v in config.SCALES]),\n                                max([v[1] for v in config.SCALES])))]\n    logger.info('providing maximum shape %s' % max_data_shape)\n\n    # infer shape\n    data_shape_dict = dict(train_data.provide_data + train_data.provide_label)\n    arg_shape, out_shape, aux_shape = sym.infer_shape(**data_shape_dict)\n    arg_shape_dict = dict(zip(sym.list_arguments(), arg_shape))\n    out_shape_dict = dict(zip(sym.list_outputs(), out_shape))\n    aux_shape_dict = dict(zip(sym.list_auxiliary_states(), aux_shape))\n    logger.info('output shape %s' % pprint.pformat(out_shape_dict))\n\n    # load and initialize params\n    if resume:\n        arg_params, aux_params = load_param(prefix, begin_epoch, convert=True)\n    else:\n        arg_params, aux_params = load_param(pretrained, epoch, convert=True)\n        arg_params['cls_score_weight'] = mx.random.normal(\n            0, 0.01, shape=arg_shape_dict['cls_score_weight'])\n        arg_params['cls_score_bias'] = mx.nd.zeros(\n            shape=arg_shape_dict['cls_score_bias'])\n        arg_params['bbox_pred_weight'] = mx.random.normal(\n            0, 0.001, shape=arg_shape_dict['bbox_pred_weight'])\n        arg_params['bbox_pred_bias'] = mx.nd.zeros(\n            shape=arg_shape_dict['bbox_pred_bias'])\n\n    # check parameter shapes\n    for k in sym.list_arguments():\n        if k in data_shape_dict:\n            continue\n        assert k in arg_params, k + ' not initialized'\n        assert arg_params[k].shape == arg_shape_dict[k], \\\n            'shape inconsistent for ' + k + ' inferred ' + str(arg_shape_dict[k]) + ' provided ' + str(arg_params[k].shape)\n    for k in sym.list_auxiliary_states():\n        assert k in aux_params, k + ' not initialized'\n        assert aux_params[k].shape == aux_shape_dict[k], \\\n            'shape inconsistent for ' + k + ' inferred ' + str(aux_shape_dict[k]) + ' provided ' + str(aux_params[k].shape)\n\n    # prepare training\n    # create solver\n    data_names = [k[0] for k in train_data.provide_data]\n    label_names = [k[0] for k in train_data.provide_label]\n    if train_shared:\n        fixed_param_prefix = config.FIXED_PARAMS_SHARED\n    else:\n        fixed_param_prefix = config.FIXED_PARAMS\n    mod = MutableModule(sym,\n                        data_names=data_names,\n                        label_names=label_names,\n                        logger=logger,\n                        context=ctx,\n                        work_load_list=work_load_list,\n                        max_data_shapes=max_data_shape,\n                        fixed_param_prefix=fixed_param_prefix)\n\n    # decide training params\n    # metric\n    eval_metric = metric.RCNNAccMetric()\n    cls_metric = metric.RCNNLogLossMetric()\n    bbox_metric = metric.RCNNL1LossMetric()\n    eval_metrics = mx.metric.CompositeEvalMetric()\n    for child_metric in [eval_metric, cls_metric, bbox_metric]:\n        eval_metrics.add(child_metric)\n    # callback\n    batch_end_callback = mx.callback.Speedometer(train_data.batch_size,\n                                                 frequent=frequent,\n                                                 auto_reset=False)\n    epoch_end_callback = callback.do_checkpoint(prefix, means, stds)\n    # decide learning rate\n    base_lr = lr\n    lr_factor = 0.1\n    lr_epoch = [int(epoch) for epoch in lr_step.split(',')]\n    lr_epoch_diff = [\n        epoch - begin_epoch for epoch in lr_epoch if epoch > begin_epoch\n    ]\n    lr = base_lr * (lr_factor**(len(lr_epoch) - len(lr_epoch_diff)))\n    lr_iters = [\n        int(epoch * len(roidb) / batch_size) for epoch in lr_epoch_diff\n    ]\n    logger.info('lr %f lr_epoch_diff %s lr_iters %s' %\n                (lr, lr_epoch_diff, lr_iters))\n    lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(lr_iters, lr_factor)\n    # optimizer\n    optimizer_params = {\n        'momentum': 0.9,\n        'wd': 0.0005,\n        'learning_rate': lr,\n        'lr_scheduler': lr_scheduler,\n        'rescale_grad': (1.0 / batch_size),\n        'clip_gradient': 5\n    }\n\n    # train\n    mod.fit(train_data,\n            eval_metric=eval_metrics,\n            epoch_end_callback=epoch_end_callback,\n            batch_end_callback=batch_end_callback,\n            kvstore=kvstore,\n            optimizer='sgd',\n            optimizer_params=optimizer_params,\n            arg_params=arg_params,\n            aux_params=aux_params,\n            begin_epoch=begin_epoch,\n            num_epoch=end_epoch)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a Fast R-CNN Network')\n    # general\n    parser.add_argument('--network',\n                        help='network name',\n                        default=default.network,\n                        type=str)\n    parser.add_argument('--dataset',\n                        help='dataset name',\n                        default=default.dataset,\n                        type=str)\n    args, rest = parser.parse_known_args()\n    generate_config(args.network, args.dataset)\n    parser.add_argument('--image_set',\n                        help='image_set name',\n                        default=default.image_set,\n                        type=str)\n    parser.add_argument('--root_path',\n                        help='output data folder',\n                        default=default.root_path,\n                        type=str)\n    parser.add_argument('--dataset_path',\n                        help='dataset path',\n                        default=default.dataset_path,\n                        type=str)\n    # training\n    parser.add_argument('--frequent',\n                        help='frequency of logging',\n                        default=default.frequent,\n                        type=int)\n    parser.add_argument('--kvstore',\n                        help='the kv-store type',\n                        default=default.kvstore,\n                        type=str)\n    parser.add_argument('--work_load_list',\n                        help='work load for different devices',\n                        default=None,\n                        type=list)\n    parser.add_argument('--no_flip',\n                        help='disable flip images',\n                        action='store_true')\n    parser.add_argument('--no_shuffle',\n                        help='disable random shuffle',\n                        action='store_true')\n    parser.add_argument('--resume',\n                        help='continue training',\n                        action='store_true')\n    # rcnn\n    parser.add_argument('--gpus',\n                        help='GPU device to train with',\n                        default='0',\n                        type=str)\n    parser.add_argument('--pretrained',\n                        help='pretrained model prefix',\n                        default=default.pretrained,\n                        type=str)\n    parser.add_argument('--pretrained_epoch',\n                        help='pretrained model epoch',\n                        default=default.pretrained_epoch,\n                        type=int)\n    parser.add_argument('--prefix',\n                        help='new model prefix',\n                        default=default.rcnn_prefix,\n                        type=str)\n    parser.add_argument('--begin_epoch',\n                        help='begin epoch of training',\n                        default=0,\n                        type=int)\n    parser.add_argument('--end_epoch',\n                        help='end epoch of training',\n                        default=default.rcnn_epoch,\n                        type=int)\n    parser.add_argument('--lr',\n                        help='base learning rate',\n                        default=default.rcnn_lr,\n                        type=float)\n    parser.add_argument('--lr_step',\n                        help='learning rate steps (in epoch)',\n                        default=default.rcnn_lr_step,\n                        type=str)\n    parser.add_argument('--train_shared',\n                        help='second round train shared params',\n                        action='store_true')\n    parser.add_argument('--proposal',\n                        help='can be ss for selective search or rpn',\n                        default='rpn',\n                        type=str)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    logger.info('Called with argument: %s' % args)\n    ctx = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n    train_rcnn(args.network,\n               args.dataset,\n               args.image_set,\n               args.root_path,\n               args.dataset_path,\n               args.frequent,\n               args.kvstore,\n               args.work_load_list,\n               args.no_flip,\n               args.no_shuffle,\n               args.resume,\n               ctx,\n               args.pretrained,\n               args.pretrained_epoch,\n               args.prefix,\n               args.begin_epoch,\n               args.end_epoch,\n               train_shared=args.train_shared,\n               lr=args.lr,\n               lr_step=args.lr_step,\n               proposal=args.proposal)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"code": "#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport traceback\nimport paddle\nfrom ...utils.registry import Registry\n\nTRANSFORMS = Registry(\"TRANSFORMS\")\n\n\nclass Compose(object):\n    \"\"\"\n    Composes several transforms together use for composing list of transforms\n    together for a dataset transform.\n    Args:\n        transforms (list): List of transforms to compose.\n    Returns:\n        A compose object which is callable, __call__ for this Compose\n        object will call each given :attr:`transforms` sequencely.\n    \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, data):\n        for f in self.transforms:\n            try:\n                data = f(data)\n            except Exception as e:\n                print(f)\n                stack_info = traceback.format_exc()\n                print(\"fail to perform transform [{}] with error: \"\n                      \"{} and stack:\\n{}\".format(f, e, str(stack_info)))\n                raise e\n        return data\n\n\ndef build_transforms(cfg):\n    transforms = []\n\n    for trans_cfg in cfg:\n        temp_trans_cfg = copy.deepcopy(trans_cfg)\n        name = temp_trans_cfg.pop('name')\n        transforms.append(TRANSFORMS.get(name)(**temp_trans_cfg))\n\n    transforms = Compose(transforms)\n    return transforms\n"}
{"code": "# -*- coding: UTF-8 -*-\n#   Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\nfrom functools import partial\n\nimport numpy as np\nimport paddle\nfrom paddle.static import InputSpec\nfrom paddlenlp.data import Pad, Tuple, Stack\nfrom paddlenlp.metrics import ChunkEvaluator\n\nfrom data import load_dataset, load_vocab, convert_example, parse_result\nfrom model import BiGruCrf\n\n# yapf: disable\nparser = argparse.ArgumentParser(__doc__)\nparser.add_argument(\"--data_dir\", type=str, default=None, help=\"The folder where the dataset is located.\")\nparser.add_argument(\"--init_checkpoint\", type=str, default=None, help=\"Path to init model.\")\nparser.add_argument(\"--batch_size\", type=int, default=300, help=\"The number of sequences contained in a mini-batch.\")\nparser.add_argument(\"--max_seq_len\", type=int, default=64, help=\"Number of words of the longest seqence.\")\nparser.add_argument(\"--device\", default=\"gpu\", type=str, choices=[\"cpu\", \"gpu\"] ,help=\"The device to select to train the model, is must be cpu/gpu.\")\nparser.add_argument(\"--emb_dim\", type=int, default=128, help=\"The dimension in which a word is embedded.\")\nparser.add_argument(\"--hidden_size\", type=int, default=128, help=\"The number of hidden nodes in the GRU layer.\")\nargs = parser.parse_args()\n# yapf: enable\n\n\ndef infer(args):\n    paddle.set_device(args.device)\n\n    # create dataset.\n    infer_ds = load_dataset(datafiles=(os.path.join(args.data_dir,\n                                                    'infer.tsv')))\n    word_vocab = load_vocab(os.path.join(args.data_dir, 'word.dic'))\n    label_vocab = load_vocab(os.path.join(args.data_dir, 'tag.dic'))\n    # q2b.dic is used to replace DBC case to SBC case\n    normlize_vocab = load_vocab(os.path.join(args.data_dir, 'q2b.dic'))\n\n    trans_func = partial(\n        convert_example,\n        max_seq_len=args.max_seq_len,\n        word_vocab=word_vocab,\n        label_vocab=label_vocab,\n        normlize_vocab=normlize_vocab)\n    infer_ds.map(trans_func)\n\n    batchify_fn = lambda samples, fn=Tuple(\n        Pad(axis=0, pad_val=0, dtype='int64'),  # word_ids\n        Stack(dtype='int64'),  # length\n    ): fn(samples)\n\n    # Create sampler for dataloader\n    infer_sampler = paddle.io.BatchSampler(\n        dataset=infer_ds,\n        batch_size=args.batch_size,\n        shuffle=False,\n        drop_last=False)\n    infer_loader = paddle.io.DataLoader(\n        dataset=infer_ds,\n        batch_sampler=infer_sampler,\n        return_list=True,\n        collate_fn=batchify_fn)\n\n    # Define the model network\n    model = BiGruCrf(args.emb_dim, args.hidden_size,\n                     len(word_vocab), len(label_vocab))\n\n    # Load the model and start predicting\n    model_dict = paddle.load(args.init_checkpoint)\n    model.load_dict(model_dict)\n\n    model.eval()\n    results = []\n    for batch in infer_loader:\n        token_ids, length = batch\n        preds = model(token_ids, length)\n        result = parse_result(token_ids.numpy(),\n                              preds.numpy(),\n                              length.numpy(), word_vocab, label_vocab)\n        results += result\n\n    sent_tags = []\n    for sent, tags in results:\n        sent_tag = ['(%s, %s)' % (ch, tag) for ch, tag in zip(sent, tags)]\n        sent_tags.append(''.join(sent_tag))\n\n    file_path = \"results.txt\"\n    with open(file_path, \"w\", encoding=\"utf8\") as fout:\n        fout.write(\"\\n\".join(sent_tags))\n\n    # Print some examples\n    print(\n        \"The results have been saved in the file: %s, some examples are shown below: \"\n        % file_path)\n    print(\"\\n\".join(sent_tags[:10]))\n\n\nif __name__ == '__main__':\n    infer(args)\n"}
{"code": "# Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\n\nimport os\n\nimport paddle.fluid as fluid\n\n\ndef nccl2_prepare(trainer_id, startup_prog, main_prog):\n    config = fluid.DistributeTranspilerConfig()\n    config.mode = \"nccl2\"\n    t = fluid.DistributeTranspiler(config=config)\n    t.transpile(\n        trainer_id,\n        trainers=os.environ.get('PADDLE_TRAINER_ENDPOINTS'),\n        current_endpoint=os.environ.get('PADDLE_CURRENT_ENDPOINT'),\n        startup_program=startup_prog,\n        program=main_prog)\n\n\ndef prepare_for_multi_process(exe, build_strategy, startup_prog, main_prog):\n    trainer_id = int(os.environ.get('PADDLE_TRAINER_ID', 0))\n    num_trainers = int(os.environ.get('PADDLE_TRAINERS_NUM', 1))\n    if num_trainers < 2:\n        return\n    build_strategy.num_trainers = num_trainers\n    build_strategy.trainer_id = trainer_id\n    nccl2_prepare(trainer_id, startup_prog, main_prog)\n"}
{"code": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport sys\nimport os\nimport errno\nimport paddle\n\n\ndef get_check_global_params(mode):\n    check_params = [\n        'use_gpu', 'max_text_length', 'image_shape', 'image_shape',\n        'character_type', 'loss_type'\n    ]\n    if mode == \"train_eval\":\n        check_params = check_params + [\n            'train_batch_size_per_card', 'test_batch_size_per_card'\n        ]\n    elif mode == \"test\":\n        check_params = check_params + ['test_batch_size_per_card']\n    return check_params\n\n\ndef check_gpu(use_gpu):\n    \"\"\"\n    Log error and exit when set use_gpu=true in paddlepaddle\n    cpu version.\n    \"\"\"\n    err = \"Config use_gpu cannot be set as true while you are \" \\\n          \"using paddlepaddle cpu version ! \\nPlease try: \\n\" \\\n          \"\\t1. Install paddlepaddle-gpu to run model on GPU \\n\" \\\n          \"\\t2. Set use_gpu as false in config file to run \" \\\n          \"model on CPU\"\n    if use_gpu:\n        try:\n            if not paddle.is_compiled_with_cuda():\n                print(err)\n                sys.exit(1)\n        except:\n            print(\"Fail to check gpu state.\")\n            sys.exit(1)\n\n\ndef _mkdir_if_not_exist(path, logger):\n    \"\"\"\n    mkdir if not exists, ignore the exception when multiprocess mkdir together\n    \"\"\"\n    if not os.path.exists(path):\n        try:\n            os.makedirs(path)\n        except OSError as e:\n            if e.errno == errno.EEXIST and os.path.isdir(path):\n                logger.warning(\n                    'be happy if some process has already created {}'.format(\n                        path))\n            else:\n                raise OSError('Failed to mkdir {}'.format(path))\n"}
{"code": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#   \n# Licensed under the Apache License, Version 2.0 (the \"License\");   \n# you may not use this file except in compliance with the License.  \n# You may obtain a copy of the License at   \n#   \n#     http://www.apache.org/licenses/LICENSE-2.0    \n#   \n# Unless required by applicable law or agreed to in writing, software   \n# distributed under the License is distributed on an \"AS IS\" BASIS, \n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n# See the License for the specific language governing permissions and   \n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom ppdet.core.workspace import register, create\nfrom .meta_arch import BaseArch\n\n__all__ = ['S2ANet']\n\n\n@register\nclass S2ANet(BaseArch):\n    __category__ = 'architecture'\n    __inject__ = [\n        's2anet_head',\n        's2anet_bbox_post_process',\n    ]\n\n    def __init__(self, backbone, neck, s2anet_head, s2anet_bbox_post_process):\n        \"\"\"\n        S2ANet, see https://arxiv.org/pdf/2008.09397.pdf\n\n        Args:\n            backbone (object): backbone instance\n            neck (object): `FPN` instance\n            s2anet_head (object): `S2ANetHead` instance\n            s2anet_bbox_post_process (object): `S2ANetBBoxPostProcess` instance\n        \"\"\"\n        super(S2ANet, self).__init__()\n        self.backbone = backbone\n        self.neck = neck\n        self.s2anet_head = s2anet_head\n        self.s2anet_bbox_post_process = s2anet_bbox_post_process\n\n    @classmethod\n    def from_config(cls, cfg, *args, **kwargs):\n        backbone = create(cfg['backbone'])\n        kwargs = {'input_shape': backbone.out_shape}\n        neck = cfg['neck'] and create(cfg['neck'], **kwargs)\n\n        out_shape = neck and neck.out_shape or backbone.out_shape\n        kwargs = {'input_shape': out_shape}\n        s2anet_head = create(cfg['s2anet_head'], **kwargs)\n        s2anet_bbox_post_process = create(cfg['s2anet_bbox_post_process'],\n                                          **kwargs)\n\n        return {\n            'backbone': backbone,\n            'neck': neck,\n            \"s2anet_head\": s2anet_head,\n            \"s2anet_bbox_post_process\": s2anet_bbox_post_process,\n        }\n\n    def _forward(self):\n        body_feats = self.backbone(self.inputs)\n        if self.neck is not None:\n            body_feats = self.neck(body_feats)\n        self.s2anet_head(body_feats)\n        if self.training:\n            loss = self.s2anet_head.get_loss(self.inputs)\n            total_loss = paddle.add_n(list(loss.values()))\n            loss.update({'loss': total_loss})\n            return loss\n        else:\n            im_shape = self.inputs['im_shape']\n            scale_factor = self.inputs['scale_factor']\n            nms_pre = self.s2anet_bbox_post_process.nms_pre\n            pred_scores, pred_bboxes = self.s2anet_head.get_prediction(nms_pre)\n\n            # post_process\n            pred_bboxes, bbox_num = self.s2anet_bbox_post_process(pred_scores,\n                                                                  pred_bboxes)\n            # rescale the prediction back to origin image\n            pred_bboxes = self.s2anet_bbox_post_process.get_pred(\n                pred_bboxes, bbox_num, im_shape, scale_factor)\n\n            # output\n            output = {'bbox': pred_bboxes, 'bbox_num': bbox_num}\n            return output\n\n    def get_loss(self, ):\n        loss = self._forward()\n        return loss\n\n    def get_pred(self):\n        output = self._forward()\n        return output\n"}
{"code": "from data.compose import ComposeDataset, ProportionalComposeDataset\nfrom .berkeley import BerkeleyDataset\nfrom .coco import CocoDataset\nfrom .davis import DavisDataset\nfrom .grabcut import GrabCutDataset\nfrom .coco_lvis import CocoLvisDataset\nfrom .lvis import LvisDataset\nfrom .openimages import OpenImagesDataset\nfrom .sbd import SBDDataset, SBDEvaluationDataset\nfrom .images_dir import ImagesDirDataset\nfrom .ade20k import ADE20kDataset\nfrom .pascalvoc import PascalVocDataset\nfrom .human import HumanDataset"}
{"code": ""}
{"code": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\nfrom ..registry import BACKBONES\nfrom ..weight_init import weight_init_\n\n\ndef zero(x):\n    return 0\n\n\ndef iden(x):\n    return x\n\n\ndef einsum(x, A):\n    \"\"\"paddle.einsum will be implemented in release/2.2.\n    \"\"\"\n    x = x.transpose((0, 2, 3, 1, 4))\n    n, c, t, k, v = x.shape\n    k2, v2, w = A.shape\n    assert (k == k2 and v == v2), \"Args of einsum not match!\"\n    x = x.reshape((n, c, t, k * v))\n    A = A.reshape((k * v, w))\n    y = paddle.matmul(x, A)\n    return y\n\n\ndef get_hop_distance(num_node, edge, max_hop=1):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[j, i] = 1\n        A[i, j] = 1\n\n    # compute hop steps\n    hop_dis = np.zeros((num_node, num_node)) + np.inf\n    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n    arrive_mat = (np.stack(transfer_mat) > 0)\n    for d in range(max_hop, -1, -1):\n        hop_dis[arrive_mat[d]] = d\n    return hop_dis\n\n\ndef normalize_digraph(A):\n    Dl = np.sum(A, 0)\n    num_node = A.shape[0]\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i]**(-1)\n    AD = np.dot(A, Dn)\n    return AD\n\n\nclass Graph():\n    def __init__(self,\n                 layout='openpose',\n                 strategy='uniform',\n                 max_hop=1,\n                 dilation=1):\n        self.max_hop = max_hop\n        self.dilation = dilation\n\n        self.get_edge(layout)\n        self.hop_dis = get_hop_distance(self.num_node,\n                                        self.edge,\n                                        max_hop=max_hop)\n        self.get_adjacency(strategy)\n\n    def __str__(self):\n        return self.A\n\n    def get_edge(self, layout):\n        # edge is a list of [child, parent] paris\n\n        if layout == 'fsd10':\n            self.num_node = 25\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_link = [(1, 8), (0, 1), (15, 0), (17, 15), (16, 0),\n                             (18, 16), (5, 1), (6, 5), (7, 6), (2, 1), (3, 2),\n                             (4, 3), (9, 8), (10, 9), (11, 10), (24, 11),\n                             (22, 11), (23, 22), (12, 8), (13, 12), (14, 13),\n                             (21, 14), (19, 14), (20, 19)]\n            self.edge = self_link + neighbor_link\n            self.center = 8\n        elif layout == 'ntu-rgb+d':\n            self.num_node = 25\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5),\n                              (7, 6), (8, 7), (9, 21), (10, 9), (11, 10),\n                              (12, 11), (13, 1), (14, 13), (15, 14), (16, 15),\n                              (17, 1), (18, 17), (19, 18), (20, 19), (22, 23),\n                              (23, 8), (24, 25), (25, 12)]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 21 - 1\n        else:\n            raise ValueError(\"Do Not Exist This Layout.\")\n\n    def get_adjacency(self, strategy):\n        valid_hop = range(0, self.max_hop + 1, self.dilation)\n        adjacency = np.zeros((self.num_node, self.num_node))\n        for hop in valid_hop:\n            adjacency[self.hop_dis == hop] = 1\n        normalize_adjacency = normalize_digraph(adjacency)\n\n        if strategy == 'spatial':\n            A = []\n            for hop in valid_hop:\n                a_root = np.zeros((self.num_node, self.num_node))\n                a_close = np.zeros((self.num_node, self.num_node))\n                a_further = np.zeros((self.num_node, self.num_node))\n                for i in range(self.num_node):\n                    for j in range(self.num_node):\n                        if self.hop_dis[j, i] == hop:\n                            if self.hop_dis[j, self.center] == self.hop_dis[\n                                    i, self.center]:\n                                a_root[j, i] = normalize_adjacency[j, i]\n                            elif self.hop_dis[j, self.center] > self.hop_dis[\n                                    i, self.center]:\n                                a_close[j, i] = normalize_adjacency[j, i]\n                            else:\n                                a_further[j, i] = normalize_adjacency[j, i]\n                if hop == 0:\n                    A.append(a_root)\n                else:\n                    A.append(a_root + a_close)\n                    A.append(a_further)\n            A = np.stack(A)\n            self.A = A\n        else:\n            raise ValueError(\"Do Not Exist This Strategy\")\n\n\nclass ConvTemporalGraphical(nn.Layer):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 t_kernel_size=1,\n                 t_stride=1,\n                 t_padding=0,\n                 t_dilation=1):\n        super().__init__()\n\n        self.kernel_size = kernel_size\n        self.conv = nn.Conv2D(in_channels,\n                              out_channels * kernel_size,\n                              kernel_size=(t_kernel_size, 1),\n                              padding=(t_padding, 0),\n                              stride=(t_stride, 1),\n                              dilation=(t_dilation, 1))\n\n    def forward(self, x, A):\n        assert A.shape[0] == self.kernel_size\n\n        x = self.conv(x)\n        n, kc, t, v = x.shape\n        x = x.reshape((n, self.kernel_size, kc // self.kernel_size, t, v))\n        x = einsum(x, A)\n\n        return x, A\n\n\nclass st_gcn_block(nn.Layer):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 dropout=0,\n                 residual=True):\n        super(st_gcn_block, self).__init__()\n\n        assert len(kernel_size) == 2\n        assert kernel_size[0] % 2 == 1\n        padding = ((kernel_size[0] - 1) // 2, 0)\n\n        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n                                         kernel_size[1])\n\n        self.tcn = nn.Sequential(\n            nn.BatchNorm2D(out_channels),\n            nn.ReLU(),\n            nn.Conv2D(\n                out_channels,\n                out_channels,\n                (kernel_size[0], 1),\n                (stride, 1),\n                padding,\n            ),\n            nn.BatchNorm2D(out_channels),\n            nn.Dropout(dropout),\n        )\n\n        if not residual:\n            self.residual = zero\n\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = iden\n\n        else:\n            self.residual = nn.Sequential(\n                nn.Conv2D(in_channels,\n                          out_channels,\n                          kernel_size=1,\n                          stride=(stride, 1)),\n                nn.BatchNorm2D(out_channels),\n            )\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x, A):\n        res = self.residual(x)\n        x, A = self.gcn(x, A)\n        x = self.tcn(x) + res\n        return self.relu(x), A\n\n\n@BACKBONES.register()\nclass STGCN(nn.Layer):\n    \"\"\"\n    ST-GCN model from:\n    `\"Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\" <https://arxiv.org/abs/1801.07455>`_\n    Args:\n        in_channels: int, channels of vertex coordinate. 2 for (x,y), 3 for (x,y,z). Default 2.\n        edge_importance_weighting: bool, whether to use edge attention. Default True.\n        data_bn: bool, whether to use data BatchNorm. Default True.\n    \"\"\"\n    def __init__(self,\n                 in_channels=2,\n                 edge_importance_weighting=True,\n                 data_bn=True,\n                 layout='fsd10',\n                 strategy='spatial',\n                 **kwargs):\n        super(STGCN, self).__init__()\n        self.data_bn = data_bn\n        # load graph\n        self.graph = Graph(\n            layout=layout,\n            strategy=strategy,\n        )\n        A = paddle.to_tensor(self.graph.A, dtype='float32')\n        self.register_buffer('A', A)\n\n        # build networks\n        spatial_kernel_size = A.shape[0]\n        temporal_kernel_size = 9\n        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n        self.data_bn = nn.BatchNorm1D(in_channels *\n                                      A.shape[1]) if self.data_bn else iden\n        kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n        self.st_gcn_networks = nn.LayerList((\n            st_gcn_block(in_channels,\n                         64,\n                         kernel_size,\n                         1,\n                         residual=False,\n                         **kwargs0),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 128, kernel_size, 2, **kwargs),\n            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n            st_gcn_block(128, 256, kernel_size, 2, **kwargs),\n            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n        ))\n\n        # initialize parameters for edge importance weighting\n        if edge_importance_weighting:\n            self.edge_importance = nn.ParameterList([\n                self.create_parameter(\n                    shape=self.A.shape,\n                    default_initializer=nn.initializer.Constant(1))\n                for i in self.st_gcn_networks\n            ])\n        else:\n            self.edge_importance = [1] * len(self.st_gcn_networks)\n\n        self.pool = nn.AdaptiveAvgPool2D(output_size=(1, 1))\n\n    def init_weights(self):\n        \"\"\"Initiate the parameters.\n        \"\"\"\n        for layer in self.sublayers():\n            if isinstance(layer, nn.Conv2D):\n                weight_init_(layer, 'Normal', mean=0.0, std=0.02)\n            elif isinstance(layer, nn.BatchNorm2D):\n                weight_init_(layer, 'Normal', mean=1.0, std=0.02)\n            elif isinstance(layer, nn.BatchNorm1D):\n                weight_init_(layer, 'Normal', mean=1.0, std=0.02)\n\n    def forward(self, x):\n        # data normalization\n        N, C, T, V, M = x.shape\n        x = x.transpose((0, 4, 3, 1, 2))  # N, M, V, C, T\n        x = x.reshape((N * M, V * C, T))\n        if self.data_bn:\n            x.stop_gradient = False\n        x = self.data_bn(x)\n        x = x.reshape((N, M, V, C, T))\n        x = x.transpose((0, 1, 3, 4, 2))  # N, M, C, T, V\n        x = x.reshape((N * M, C, T, V))\n\n        # forward\n        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n            x, _ = gcn(x, paddle.multiply(self.A, importance))\n\n        x = self.pool(x)  # NM,C,T,V --> NM,C,1,1\n        C = x.shape[1]\n        x = paddle.reshape(x, (N, M, C, 1, 1)).mean(axis=1)  # N,C,1,1\n        return x\n"}
{"code": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is based on https://github.com/saic-vul/ritm_interactive_segmentation\nThs copyright of saic-vul/ritm_interactive_segmentation is as follows:\nMIT License [see LICENSE for details]\n\"\"\"\n\nimport paddle\nimport numpy as np\nfrom inference.clicker import Click\nfrom util.misc import get_bbox_iou, get_bbox_from_mask, expand_bbox, clamp_bbox\nfrom .base import BaseTransform\n\n\nclass ZoomIn(BaseTransform):\n    def __init__(\n        self,\n        target_size=700,\n        skip_clicks=1,\n        expansion_ratio=1.4,\n        min_crop_size=480,\n        recompute_thresh_iou=0.5,\n        prob_thresh=0.50,\n    ):\n        super().__init__()\n        self.target_size = target_size\n        self.min_crop_size = min_crop_size\n        self.skip_clicks = skip_clicks\n        self.expansion_ratio = expansion_ratio\n        self.recompute_thresh_iou = recompute_thresh_iou\n        self.prob_thresh = prob_thresh\n\n        self._input_image_shape = None\n        self._prev_probs = None\n        self._object_roi = None\n        self._roi_image = None\n\n    def transform(self, image_nd, clicks_lists):\n        assert image_nd.shape[0] == 1 and len(clicks_lists) == 1\n        self.image_changed = False\n\n        clicks_list = clicks_lists[0]\n        if len(clicks_list) <= self.skip_clicks:\n            return image_nd, clicks_lists\n\n        self._input_image_shape = image_nd.shape\n\n        current_object_roi = None\n        if self._prev_probs is not None:\n            current_pred_mask = (self._prev_probs > self.prob_thresh)[0, 0]\n            if current_pred_mask.sum() > 0:\n                current_object_roi = get_object_roi(\n                    current_pred_mask,\n                    clicks_list,\n                    self.expansion_ratio,\n                    self.min_crop_size,\n                )\n\n        if current_object_roi is None:\n            if self.skip_clicks >= 0:\n                return image_nd, clicks_lists\n            else:\n                current_object_roi = 0, image_nd.shape[2] - 1, 0, image_nd.shape[3] - 1\n\n        update_object_roi = False\n        if self._object_roi is None:\n            update_object_roi = True\n        elif not check_object_roi(self._object_roi, clicks_list):\n            update_object_roi = True\n        elif (\n            get_bbox_iou(current_object_roi, self._object_roi)\n            < self.recompute_thresh_iou\n        ):\n            update_object_roi = True\n\n        if update_object_roi:\n            self._object_roi = current_object_roi\n            self.image_changed = True\n        self._roi_image = get_roi_image_nd(image_nd, self._object_roi, self.target_size)\n\n        tclicks_lists = [self._transform_clicks(clicks_list)]\n        return self._roi_image, tclicks_lists\n\n    def inv_transform(self, prob_map):\n        if self._object_roi is None:\n            self._prev_probs = prob_map.numpy()\n            return prob_map\n\n        assert prob_map.shape[0] == 1\n        rmin, rmax, cmin, cmax = self._object_roi\n        prob_map = paddle.nn.functional.interpolate(\n            prob_map,\n            size=(rmax - rmin + 1, cmax - cmin + 1),\n            mode=\"bilinear\",\n            align_corners=True,\n        )\n\n        if self._prev_probs is not None:\n            new_prob_map = paddle.zeros(\n                shape=self._prev_probs.shape, dtype=prob_map.dtype\n            )\n            new_prob_map[:, :, rmin : rmax + 1, cmin : cmax + 1] = prob_map\n        else:\n            new_prob_map = prob_map\n\n        self._prev_probs = new_prob_map.numpy()\n\n        return new_prob_map\n\n    def check_possible_recalculation(self):\n        if (\n            self._prev_probs is None\n            or self._object_roi is not None\n            or self.skip_clicks > 0\n        ):\n            return False\n\n        pred_mask = (self._prev_probs > self.prob_thresh)[0, 0]\n        if pred_mask.sum() > 0:\n            possible_object_roi = get_object_roi(\n                pred_mask, [], self.expansion_ratio, self.min_crop_size\n            )\n            image_roi = (\n                0,\n                self._input_image_shape[2] - 1,\n                0,\n                self._input_image_shape[3] - 1,\n            )\n            if get_bbox_iou(possible_object_roi, image_roi) < 0.50:\n                return True\n        return False\n\n    def get_state(self):\n        roi_image = self._roi_image if self._roi_image is not None else None\n        return (\n            self._input_image_shape,\n            self._object_roi,\n            self._prev_probs,\n            roi_image,\n            self.image_changed,\n        )\n\n    def set_state(self, state):\n        (\n            self._input_image_shape,\n            self._object_roi,\n            self._prev_probs,\n            self._roi_image,\n            self.image_changed,\n        ) = state\n\n    def reset(self):\n        self._input_image_shape = None\n        self._object_roi = None\n        self._prev_probs = None\n        self._roi_image = None\n        self.image_changed = False\n\n    def _transform_clicks(self, clicks_list):\n        if self._object_roi is None:\n            return clicks_list\n\n        rmin, rmax, cmin, cmax = self._object_roi\n        crop_height, crop_width = self._roi_image.shape[2:]\n\n        transformed_clicks = []\n        for click in clicks_list:\n            new_r = crop_height * (click.coords[0] - rmin) / (rmax - rmin + 1)\n            new_c = crop_width * (click.coords[1] - cmin) / (cmax - cmin + 1)\n            transformed_clicks.append(click.copy(coords=(new_r, new_c)))\n        return transformed_clicks\n\n\ndef get_object_roi(pred_mask, clicks_list, expansion_ratio, min_crop_size):\n    pred_mask = pred_mask.copy()\n\n    for click in clicks_list:\n        if click.is_positive:\n            pred_mask[int(click.coords[0]), int(click.coords[1])] = 1\n\n    bbox = get_bbox_from_mask(pred_mask)\n    bbox = expand_bbox(bbox, expansion_ratio, min_crop_size)\n    h, w = pred_mask.shape[0], pred_mask.shape[1]\n    bbox = clamp_bbox(bbox, 0, h - 1, 0, w - 1)\n\n    return bbox\n\n\ndef get_roi_image_nd(image_nd, object_roi, target_size):\n    rmin, rmax, cmin, cmax = object_roi\n\n    height = rmax - rmin + 1\n    width = cmax - cmin + 1\n\n    if isinstance(target_size, tuple):\n        new_height, new_width = target_size\n    else:\n        scale = target_size / max(height, width)\n        new_height = int(round(height * scale))\n        new_width = int(round(width * scale))\n\n    with paddle.no_grad():\n        roi_image_nd = image_nd[:, :, rmin : rmax + 1, cmin : cmax + 1]\n        roi_image_nd = paddle.nn.functional.interpolate(\n            roi_image_nd,\n            size=(new_height, new_width),\n            mode=\"bilinear\",\n            align_corners=True,\n        )\n\n    return roi_image_nd\n\n\ndef check_object_roi(object_roi, clicks_list):\n    for click in clicks_list:\n        if click.is_positive:\n            if click.coords[0] < object_roi[0] or click.coords[0] >= object_roi[1]:\n                return False\n            if click.coords[1] < object_roi[2] or click.coords[1] >= object_roi[3]:\n                return False\n\n    return True\n"}
