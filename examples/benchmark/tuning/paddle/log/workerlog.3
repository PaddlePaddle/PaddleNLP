/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:48:09,933] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:48:10,485] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 16:48:10.486871 50154 tcp_utils.cc:107] Retry to connect to 10.215.195.140:60812 while the server is not yet listening.
I0619 16:48:13.487053 50154 tcp_utils.cc:130] Successfully connected to 10.215.195.140:60812
W0619 16:48:15.238752 50154 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:48:15.244779 50154 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 16:48:15,744] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:48:16,022] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:48:16,330] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:48:16,331] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:48:16,331] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:48:16,342] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:48:16,342] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:48:16,417] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:48:16,418] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,521] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:48:47,233] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:48:47,234] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Traceback (most recent call last):
  File "benchmark.py", line 136, in <module>
    main()
  File "benchmark.py", line 114, in main
    dataset = load_dataset("Chinese-Vicuna/guanaco_belle_merge_v1.0")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'Chinese-Vicuna/guanaco_belle_merge_v1.0' on the Hub (ConnectionError)
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:50:39,184] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:50:39,717] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 16:50:39.718739 51461 tcp_utils.cc:130] Successfully connected to 10.215.195.140:46550
W0619 16:50:41.245354 51461 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:50:41.250501 51461 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 16:50:41,750] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:50:42,301] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:50:42,302] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:50:42,303] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:50:42,304] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,304] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:50:42,314] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:50:42,314] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:50:42,382] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:50:42,383] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,444] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:51:11,946] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:51:11,947] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset json (/root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 145.89it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7cf1329c0e366a03.arrow
[32m[2023-06-19 16:51:20,907] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_16-50-39_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 16:51:22,448] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 16:51:22,449] [    INFO][0m - [0m
[32m[2023-06-19 16:51:22,450] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 16:51:22,455] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 16:51:22,467] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 16:51:22,514] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 16:51:22,515] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 16:51:22,515] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 16:51:22,515] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 16:51:22,521] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 16:51:24,790] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 16:51:25,842] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 16:51:26,606] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 16:51:27,239] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 16:51:27,907] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 16:51:28,524] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:28,868] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:30,852] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5
[33m[2023-06-19 16:51:31,369] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 2048.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:29:03,104] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:29:03,618] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 17:29:03.619277 65733 tcp_utils.cc:107] Retry to connect to 10.215.195.140:36267 while the server is not yet listening.
I0619 17:29:06.619421 65733 tcp_utils.cc:130] Successfully connected to 10.215.195.140:36267
W0619 17:29:08.403339 65733 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:29:08.408531 65733 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 17:29:08,947] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:29:09,285] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:29:09,634] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:29:09,634] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:29:09,645] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:29:09,645] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:29:09,723] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:29:09,724] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,786] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:29:40,155] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:29:40,155] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 508.59it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:29:48,046] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:29:48,064] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:29:48,065] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-29-03_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:29:48,066] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:29:48,067] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - [0m
[32m[2023-06-19 17:29:48,068] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:29:48,073] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:29:48,084] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:29:48,110] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:29:48,110] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:29:48,110] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 17:29:48,110] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:29:48,112] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:29:48,115] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:29:49,278] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:29:49,762] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:29:54,792] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:32:36,874] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:34:19,347] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:36:08,111] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 17:36:09,822] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4879.48
Effective Tokens per second: 2050.49
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:39:10,588] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:39:11,147] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 17:39:11.148662 70198 tcp_utils.cc:107] Retry to connect to 10.215.195.140:39512 while the server is not yet listening.
I0619 17:39:14.148866 70198 tcp_utils.cc:130] Successfully connected to 10.215.195.140:39512
W0619 17:39:15.634986 70198 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:39:15.643541 70198 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 17:39:16,147] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:39:16,718] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:39:16,719] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:39:16,719] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:39:16,720] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,720] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:39:16,730] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:39:16,730] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:39:16,853] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:39:16,854] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,915] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:39:47,298] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:39:47,298] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 493.62it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:39:55,128] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:39:55,146] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:55,147] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-39-11_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:39:55,148] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:39:55,149] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:39:55,150] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:39:55,151] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 17:39:55,151] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:39:55,151] [    INFO][0m - [0m
[32m[2023-06-19 17:39:55,151] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:39:55,155] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:39:55,166] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:39:55,192] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:39:55,192] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:39:55,192] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:39:55,193] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:39:55,195] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:39:55,197] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:39:56,391] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:39:57,044] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:39:58,037] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:39:58,352] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:45:56,907] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:45:57,439] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 17:45:57.440958 74033 tcp_utils.cc:130] Successfully connected to 10.215.195.140:42038
W0619 17:46:03.713802 74033 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:46:03.719098 74033 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 17:46:04,304] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:46:04,633] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:46:04,940] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:46:04,941] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:46:04,942] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:46:04,943] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:04,943] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:46:04,954] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:46:04,954] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:46:05,017] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:46:05,018] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:05,074] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:46:35,689] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:46:35,690] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.88it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 17:46:44,009] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:46:44,029] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:46:44,029] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:46:44,029] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:46:44,029] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:46:44,029] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:46:44,030] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-45-57_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:46:44,031] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 17:46:44,032] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:46:44,033] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - [0m
[32m[2023-06-19 17:46:44,034] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:46:44,038] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:46:44,050] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:46:44,076] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:46:44,076] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:46:44,077] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:46:44,077] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:46:44,079] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:46:44,157] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:46:45,372] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:46:46,036] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:46:47,030] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:46:47,347] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 17:52:06,677] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6995.56
Effective Tokens per second: 2420.04
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:03:14,613] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:03:15,145] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 18:03:15.146406 81939 tcp_utils.cc:107] Retry to connect to 10.215.195.140:43302 while the server is not yet listening.
I0619 18:03:18.146584 81939 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43302
W0619 18:03:19.931123 81939 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:03:19.936288 81939 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 18:03:20,613] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:03:21,005] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:03:21,357] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:03:21,358] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:03:21,359] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:03:21,359] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,359] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:03:21,369] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:03:21,370] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:03:21,443] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:03:21,444] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,529] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 78, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 816, in from_pretrained
    return cls._from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 370, in _from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1332, in from_pretrained
    model = cls(config, *init_args, **model_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 887, in __init__
    self.llama = LlamaModel(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in __init__
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in <listcomp>
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 452, in __init__
    self.self_attn = LlamaAttention(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 351, in __init__
    fuse_matmul_bias=mpu.mp_layers.is_fused_matmul_bias_supported(),
AttributeError: module 'paddle.distributed.fleet.meta_parallel' has no attribute 'mp_layers'
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:05:47,560] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:05:48,095] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 18:05:48.096558 83413 tcp_utils.cc:107] Retry to connect to 10.215.195.140:44671 while the server is not yet listening.
I0619 18:05:51.096726 83413 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44671
W0619 18:05:52.558118 83413 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:05:52.563220 83413 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 18:05:53,056] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:05:53,328] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:05:53,602] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:05:53,602] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:05:53,603] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:05:53,614] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:05:53,615] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:05:53,680] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:05:53,681] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,740] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:06:26,787] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:06:26,787] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.00it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:06:35,035] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:06:35,054] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:06:35,055] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 18:06:35,056] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-05-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:06:35,057] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:06:35,058] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - [0m
[32m[2023-06-19 18:06:35,059] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:06:35,064] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:06:35,075] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:06:35,101] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:06:35,102] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:06:35,102] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:06:35,102] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:06:35,104] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:06:35,106] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 137, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 978, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 779, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 483, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 441, in forward
    attn_output = self.o_proj(attn_output)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 493, in forward
    bias = MPScale.apply(self.bias, self.world_size)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 335, in forward
    out = paddle.scale(x, 1.0 / mp_degree)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/tensor/math.py", line 224, in scale
    return _C_ops.scale(x, scale, float(bias), bias_after_scale)
ValueError: (InvalidArgument) scale(): argument 'x' (position 0) must be Tensor, but got None (at ../paddle/fluid/pybind/eager_utils.cc:1024)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:09:30,174] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:09:30,704] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 18:09:30.705710 85104 tcp_utils.cc:107] Retry to connect to 10.215.195.140:43792 while the server is not yet listening.
I0619 18:09:33.705827 85104 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43792
W0619 18:09:35.320086 85104 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:09:35.325420 85104 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 18:09:35,818] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:09:36,085] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:09:36,365] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:09:36,366] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:09:36,366] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:09:36,367] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,368] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:09:36,445] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:09:36,447] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,505] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:10:06,046] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:10:06,046] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 293.31it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:10:14,331] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:10:14,351] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:10:14,352] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 18:10:14,353] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-09-30_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:10:14,354] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:10:14,355] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - [0m
[32m[2023-06-19 18:10:14,356] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:10:14,361] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:10:14,372] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:10:14,398] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:10:14,398] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:10:14,398] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:10:14,398] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:10:14,400] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:10:14,561] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:10:15,728] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:10:16,380] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:10:17,362] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 18:10:17,679] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 18:15:37,527] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6985.90
Effective Tokens per second: 2416.70
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:44:00,443] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:44:00,965] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 18:44:00.966954 91010 tcp_utils.cc:107] Retry to connect to 10.215.195.140:44229 while the server is not yet listening.
I0619 18:44:03.967108 91010 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44229
W0619 18:44:05.587716 91010 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:44:05.593236 91010 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 18:44:06,078] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:44:06,352] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:44:06,621] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:44:06,622] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:44:06,622] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:44:06,632] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:44:06,633] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:44:06,706] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:44:06,707] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,794] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:44:36,982] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:44:36,983] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 459.30it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:44:45,925] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:44:45,943] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:44:45,943] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:44:45,943] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:44:45,944] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:44:45,945] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-44-00_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:45,946] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:44:45,947] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - [0m
[32m[2023-06-19 18:44:45,948] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:44:45,952] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:44:45,964] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:44:46,003] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:44:46,003] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:44:46,003] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:44:46,003] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:44:46,003] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:44:46,003] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 18:44:46,003] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 18:44:46,003] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:44:46,004] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 18:44:46,004] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:44:46,006] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:44:46,116] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:44:47,490] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:44:48,070] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:44:55,422] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:46:09,524] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:51:09,140] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 18:51:10,847] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4795.92
Effective Tokens per second: 2028.73
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:10:47,553] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:10:48,064] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:10:48.065920 100381 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47565
W0619 19:10:52.558427 100381 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:10:52.568097 100381 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 19:10:53,083] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:10:53,380] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:10:53,652] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:10:53,653] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:10:53,654] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:10:53,655] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,655] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:10:53,666] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:10:53,666] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:10:53,743] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:10:53,745] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,829] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:11:47,091] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:11:47,091] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.09it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:11:55,850] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:11:55,873] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:11:55,874] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-10-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:11:55,875] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:11:55,876] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:11:55,877] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 19:11:55,878] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:11:55,878] [    INFO][0m - [0m
[32m[2023-06-19 19:11:55,878] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:11:55,883] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:11:55,900] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:11:55,941] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:11:55,941] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:11:55,942] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:11:55,942] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:11:55,944] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:11:56,242] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:14:24,916] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:14:25,438] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:14:25.439896 101956 tcp_utils.cc:107] Retry to connect to 10.215.195.140:47824 while the server is not yet listening.
I0619 19:14:28.440057 101956 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47824
W0619 19:14:30.000905 101956 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:14:30.006438 101956 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 19:14:30,529] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:14:30,799] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:14:31,089] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:14:31,089] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:14:31,090] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:14:31,101] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:14:31,101] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:14:31,160] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:14:31,161] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,221] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:15:30,790] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:15:30,790] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 493.80it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:15:39,270] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:15:39,320] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:15:39,320] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:15:39,321] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:15:39,322] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-14-25_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:15:39,323] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:39,324] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - [0m
[32m[2023-06-19 19:15:39,325] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:15:39,331] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:15:39,349] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:15:39,390] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:15:39,390] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:15:39,391] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Total optimization steps = 2500[0m
[32m[2023-06-19 19:15:39,391] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:15:39,394] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:15:39,720] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:15:44,163] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:55,679] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:56,609] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:08,033] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:19,574] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:24,218] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:10,217] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:11,155] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:05,825] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:26,993] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:32,844] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:09,041] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:21,893] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:15,240] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:15,773] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:21:15.773888 105527 tcp_utils.cc:107] Retry to connect to 10.215.195.140:54422 while the server is not yet listening.
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:26,998] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:27,533] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:21:27.534193 105968 tcp_utils.cc:130] Successfully connected to 10.215.195.140:48758
W0619 19:21:29.180711 105968 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:29.186033 105968 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 19:21:29,743] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:29,743] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:29,744] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:21:30,012] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:21:30,281] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:21:30,281] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:21:30,282] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:21:30,534] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
[32m[2023-06-19 19:21:30,536] [    INFO][0m - We are using <class 'paddlenlp.transformers.bloom.tokenizer.BloomTokenizer'> to load 'bigscience/bloomz-7b1-mt'.[0m
[32m[2023-06-19 19:21:30,536] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/vocab.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,749] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/vocab.json[0m
[32m[2023-06-19 19:21:30,750] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/merges.txt and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,980] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/merges.txt[0m
[32m[2023-06-19 19:21:30,981] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/tokenizer.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,423] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer.json[0m
[32m[2023-06-19 19:21:31,424] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[33m[2023-06-19 19:21:31,491] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json> not exist[0m
[32m[2023-06-19 19:21:31,491] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/special_tokens_map.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,702] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/special_tokens_map.json[0m
[32m[2023-06-19 19:21:31,703] [    INFO][0m - Already cached /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
Traceback (most recent call last):
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 32, in try_import
    mod = importlib.import_module(module_name)
  File "/root/miniconda3/envs/paddle/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'regex'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 74, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/tokenizer.py", line 367, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/tokenizer_utils_base.py", line 1593, in from_pretrained
    tokenizer = cls(*init_args, **init_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/bloom/tokenizer.py", line 203, in __init__
    re = try_import("regex")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 41, in try_import
    raise ImportError(err_msg)
ImportError: Failed importing regex. This likely means that some paddle modules require additional dependencies that have to be manually installed (usually with `pip install regex`). 
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:54,057] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:54,581] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:21:54.582331 106519 tcp_utils.cc:107] Retry to connect to 10.215.195.140:62509 while the server is not yet listening.
I0619 19:21:57.582501 106519 tcp_utils.cc:130] Successfully connected to 10.215.195.140:62509
W0619 19:21:59.226127 106519 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:59.231535 106519 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 19:21:59,728] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:22:00,004] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:22:00,285] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:22:00,285] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:22:00,286] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:22:00,298] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:22:00,298] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:22:00,381] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:22:00,382] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,490] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:22:35,888] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:22:35,888] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 528.52it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:22:45,583] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:22:45,607] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:22:45,608] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-21-54_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:22:45,609] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - optimizer_name_suffix         : shard03[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - per_device_train_batch_size   : 2[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:22:45,610] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - sharding                      : [<ShardingOption.FULL_SHARD: 'stage3'>][0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - sharding_parallel_rank        : 3[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - train_batch_size              : 2[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - weight_name_suffix            : [0m
[32m[2023-06-19 19:22:45,611] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:22:45,612] [    INFO][0m - [0m
[32m[2023-06-19 19:22:45,612] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:22:45,619] [ WARNING] group_sharded.py:128 - the input of scaler is None, please ensure the logic of your scaler outside is same as GroupShardedScaler.
WARNING:root:While using ClipGradByGlobalNorm in GroupShardedStage3, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:23:21,735] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Instantaneous batch size per device = 2[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:23:21,735] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:23:21,738] [    INFO][0m -   Number of trainable parameters = 13015864320 (per device)[0m
[33m[2023-06-19 19:23:25,334] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 19:23:26,471] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 19:23:27,566] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 19:23:28,742] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:23:50,496] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:25:16,903] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:27,047] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:48,822] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:53,820] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 19:26:54,958] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:56,088] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:58,420] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:28:08,971] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:28:09,489] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0619 19:28:09.490108 109708 tcp_utils.cc:107] Retry to connect to 10.215.195.140:57503 while the server is not yet listening.
I0619 19:28:12.490260 109708 tcp_utils.cc:130] Successfully connected to 10.215.195.140:57503
W0619 19:28:14.037938 109708 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:28:14.043263 109708 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2023-06-19 19:28:14,542] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:28:15,099] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:28:15,100] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:28:15,100] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:28:15,101] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,101] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:28:15,110] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:28:15,111] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:28:15,172] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:28:15,172] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,240] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:29:13,376] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:29:13,376] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 507.48it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:29:21,656] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - current_device                : gpu:3[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - gradient_accumulation_steps   : 2[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - local_process_index           : 3[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - local_rank                    : 3[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-28-09_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - process_index                 : 3[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:29:22,261] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2023-06-19 19:29:22,261] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:29:22,261] [    INFO][0m - [0m
[32m[2023-06-19 19:29:22,261] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:29:22,266] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:29:22,283] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:29:22,338] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:29:22,338] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:29:22,338] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Gradient Accumulation steps = 2[0m
[32m[2023-06-19 19:29:22,339] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:29:22,339] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:29:22,341] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:29:22,348] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:29:46,777] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:29:54,826] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:34:34,652] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:40:59,345] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[32m[2023-06-19 19:41:02,697] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 2088.39
Effective Tokens per second: 1114.46
