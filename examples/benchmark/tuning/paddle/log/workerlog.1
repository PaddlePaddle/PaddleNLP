/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:48:09,981] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:48:10,524] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 16:48:10.525696 50150 tcp_utils.cc:107] Retry to connect to 10.215.195.140:60812 while the server is not yet listening.
I0619 16:48:13.525818 50150 tcp_utils.cc:130] Successfully connected to 10.215.195.140:60812
W0619 16:48:15.238710 50150 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:48:15.245009 50150 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 16:48:15,744] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:48:16,022] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:48:16,330] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:48:16,331] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:48:16,333] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,333] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:48:16,343] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:48:16,343] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:48:16,472] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:48:16,474] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,533] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:48:47,240] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:48:47,241] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Traceback (most recent call last):
  File "benchmark.py", line 136, in <module>
    main()
  File "benchmark.py", line 114, in main
    dataset = load_dataset("Chinese-Vicuna/guanaco_belle_merge_v1.0")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'Chinese-Vicuna/guanaco_belle_merge_v1.0' on the Hub (ConnectionError)
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:50:39,171] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:50:39,709] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 16:50:39.710136 51457 tcp_utils.cc:130] Successfully connected to 10.215.195.140:46550
W0619 16:50:41.245378 51457 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:50:41.254575 51457 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 16:50:41,750] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:50:42,302] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:50:42,302] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:50:42,303] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:50:42,304] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,305] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:50:42,315] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:50:42,315] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:50:42,387] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:50:42,388] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,448] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:51:11,768] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:51:11,769] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset json (/root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 143.47it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7cf1329c0e366a03.arrow
[32m[2023-06-19 16:51:20,713] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 16:51:22,439] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 16:51:22,440] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 16:51:22,441] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 16:51:22,442] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_16-50-39_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 16:51:22,443] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:22,444] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 16:51:22,445] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 16:51:22,446] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - [0m
[32m[2023-06-19 16:51:22,447] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 16:51:22,455] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 16:51:22,470] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 16:51:22,516] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 16:51:22,517] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 16:51:22,517] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 16:51:22,517] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 16:51:22,519] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 16:51:22,521] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 16:51:24,787] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 16:51:25,841] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 16:51:26,605] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 16:51:27,238] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 16:51:27,907] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 16:51:28,522] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:28,868] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:30,852] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5
[33m[2023-06-19 16:51:31,369] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 2048.0[0m
Traceback (most recent call last):
  File "benchmark.py", line 139, in <module>
    main()
  File "benchmark.py", line 131, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 971, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 772, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 488, in forward
    hidden_states = self.mlp(hidden_states)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 324, in forward
    return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/functional/activation.py", line 991, in silu
    return _C_ops.silu(x)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   silu_ad_func(paddle::Tensor const&)
1   silu_ad_func(paddle::Tensor const&)
2   paddle::experimental::silu(paddle::Tensor const&)
3   void phi::SiluKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
4   void phi::ActivationGPUImpl<phi::dtype::float16, phi::GPUContext, phi::funcs::CudaSiluFunctor<phi::dtype::float16> >(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::funcs::CudaSiluFunctor<phi::dtype::float16> const&)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
14  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
15  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 37.079102MB memory on GPU 1, 79.321289GB memory has been allocated and available memory is only 26.125000MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:29:03,092] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:29:03,619] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 17:29:03.620020 65729 tcp_utils.cc:107] Retry to connect to 10.215.195.140:36267 while the server is not yet listening.
I0619 17:29:06.620164 65729 tcp_utils.cc:130] Successfully connected to 10.215.195.140:36267
W0619 17:29:08.399571 65729 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:29:08.405325 65729 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 17:29:08,947] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:29:09,285] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:29:09,634] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:29:09,634] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:29:09,645] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:29:09,646] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:29:09,737] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:29:09,738] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,793] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:29:39,898] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:29:39,899] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 527.52it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:29:47,228] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:29:47,246] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:29:47,246] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:29:47,246] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:29:47,247] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:29:47,248] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-29-03_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:47,249] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:29:47,250] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - [0m
[32m[2023-06-19 17:29:47,251] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:29:47,255] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:29:47,267] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:29:47,293] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:29:47,293] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:29:47,293] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:29:47,293] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:29:47,293] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:29:47,293] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 17:29:47,294] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 17:29:47,294] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:29:47,294] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 17:29:47,294] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:29:47,296] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:29:48,115] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:29:49,278] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:29:49,762] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:29:54,792] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:32:36,875] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:34:19,348] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:36:08,110] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 17:36:09,822] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4879.49
Effective Tokens per second: 2050.49
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:39:10,642] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:39:11,173] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 17:39:11.175047 70194 tcp_utils.cc:130] Successfully connected to 10.215.195.140:39512
W0619 17:39:15.635017 70194 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:39:15.643599 70194 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 17:39:16,147] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:39:16,718] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:39:16,719] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:39:16,720] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:39:16,721] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,721] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:39:16,731] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:39:16,732] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:39:16,803] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:39:16,804] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,889] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:39:44,882] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:39:44,882] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 419.68it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:39:52,360] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:39:52,377] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:39:52,377] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:39:52,377] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:39:52,378] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-39-11_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:39:52,379] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:52,380] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:39:52,381] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - [0m
[32m[2023-06-19 17:39:52,382] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:39:52,386] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:39:52,398] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:39:52,423] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:39:52,423] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:39:52,423] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:39:52,423] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:39:52,425] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:39:55,197] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:39:56,374] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:39:57,043] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:39:58,036] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:39:58,352] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:45:56,896] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:45:57,422] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 17:45:57.423056 74029 tcp_utils.cc:107] Retry to connect to 10.215.195.140:42038 while the server is not yet listening.
I0619 17:46:00.423213 74029 tcp_utils.cc:130] Successfully connected to 10.215.195.140:42038
W0619 17:46:03.710705 74029 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:46:03.716713 74029 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 17:46:04,304] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:46:04,633] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:46:04,940] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:46:04,941] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:46:04,942] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:46:04,943] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:04,943] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:46:04,954] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:46:04,954] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:46:05,020] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:46:05,022] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:05,078] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:46:35,679] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:46:35,680] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 459.20it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 17:46:44,086] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:46:44,105] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:46:44,106] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-45-57_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:46:44,107] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:46:44,108] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - [0m
[32m[2023-06-19 17:46:44,109] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:46:44,114] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:46:44,125] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:46:44,153] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:46:44,153] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:46:44,153] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:46:44,153] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:46:44,155] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:46:44,157] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:46:45,361] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:46:46,035] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:46:47,030] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:46:47,347] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 17:52:06,677] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6995.56
Effective Tokens per second: 2420.05
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:03:14,658] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:03:15,191] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 18:03:15.192019 81935 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43302
W0619 18:03:19.931186 81935 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:03:19.936980 81935 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 18:03:20,613] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:03:21,005] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:03:21,357] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:03:21,358] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:03:21,359] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:03:21,360] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,360] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:03:21,371] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:03:21,371] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:03:21,507] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:03:21,509] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,575] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 78, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 816, in from_pretrained
    return cls._from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 370, in _from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1332, in from_pretrained
    model = cls(config, *init_args, **model_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 887, in __init__
    self.llama = LlamaModel(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in __init__
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in <listcomp>
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 452, in __init__
    self.self_attn = LlamaAttention(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 351, in __init__
    fuse_matmul_bias=mpu.mp_layers.is_fused_matmul_bias_supported(),
AttributeError: module 'paddle.distributed.fleet.meta_parallel' has no attribute 'mp_layers'
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:05:47,630] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:05:48,166] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 18:05:48.167698 83409 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44671
W0619 18:05:52.558141 83409 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:05:52.563305 83409 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 18:05:53,056] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:05:53,328] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:05:53,602] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:05:53,602] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:05:53,603] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:05:53,614] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:05:53,615] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:05:53,674] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:05:53,675] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,734] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:06:26,378] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:06:26,379] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 463.77it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:06:34,453] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:06:34,471] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:06:34,471] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:06:34,471] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:06:34,472] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 18:06:34,473] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-05-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:06:34,474] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:06:34,475] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - [0m
[32m[2023-06-19 18:06:34,476] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:06:34,480] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:06:34,492] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:06:34,518] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:06:34,518] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:06:34,518] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:06:34,518] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:06:34,520] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:06:35,106] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 137, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 978, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 779, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 483, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 441, in forward
    attn_output = self.o_proj(attn_output)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 493, in forward
    bias = MPScale.apply(self.bias, self.world_size)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 335, in forward
    out = paddle.scale(x, 1.0 / mp_degree)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/tensor/math.py", line 224, in scale
    return _C_ops.scale(x, scale, float(bias), bias_after_scale)
ValueError: (InvalidArgument) scale(): argument 'x' (position 0) must be Tensor, but got None (at ../paddle/fluid/pybind/eager_utils.cc:1024)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:09:30,171] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:09:30,696] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 18:09:30.697765 85100 tcp_utils.cc:107] Retry to connect to 10.215.195.140:43792 while the server is not yet listening.
I0619 18:09:33.697939 85100 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43792
W0619 18:09:35.320058 85100 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:09:35.325528 85100 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 18:09:35,818] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:09:36,085] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:09:36,365] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:09:36,366] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:09:36,366] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:09:36,367] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,368] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:09:36,444] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:09:36,445] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,553] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:10:06,110] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:10:06,110] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 425.52it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:10:14,489] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:10:14,508] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:10:14,509] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:10:14,510] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-09-30_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:14,511] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:10:14,512] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - [0m
[32m[2023-06-19 18:10:14,513] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:10:14,518] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:10:14,530] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:10:14,556] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:10:14,556] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:10:14,556] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:10:14,557] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:10:14,559] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:10:14,561] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:10:15,735] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:10:16,380] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:10:17,362] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 18:10:17,679] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 18:15:37,527] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6985.89
Effective Tokens per second: 2416.70
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:44:00,388] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:44:00,935] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 18:44:00.936410 91006 tcp_utils.cc:107] Retry to connect to 10.215.195.140:44229 while the server is not yet listening.
I0619 18:44:03.936561 91006 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44229
W0619 18:44:05.587693 91006 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:44:05.592964 91006 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 18:44:06,078] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:44:06,352] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:44:06,621] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:44:06,621] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:44:06,622] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:44:06,632] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:44:06,632] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:44:06,723] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:44:06,724] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,814] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:44:36,726] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:44:36,726] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 371.93it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:44:45,327] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:44:45,356] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:44:45,357] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-44-00_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:44:45,358] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:44:45,359] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 18:44:45,360] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:44:45,361] [    INFO][0m - [0m
[32m[2023-06-19 18:44:45,361] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:44:45,365] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:44:45,377] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:44:45,406] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:44:45,406] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:44:45,406] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 18:44:45,406] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:44:45,408] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:44:46,116] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:44:47,496] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:44:48,071] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:44:55,422] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:46:09,525] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:51:09,141] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 18:51:10,847] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4795.92
Effective Tokens per second: 2028.73
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:10:47,502] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:10:48,065] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:10:48.067139 100377 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47565
W0619 19:10:52.558449 100377 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:10:52.563967 100377 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 19:10:53,083] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:10:53,380] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:10:53,652] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:10:53,653] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:10:53,654] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:10:53,655] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,655] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:10:53,665] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:10:53,665] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:10:53,793] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:10:53,795] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,852] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:11:47,813] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:11:47,813] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 418.80it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:11:56,143] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:11:56,166] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:11:56,167] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-10-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:11:56,168] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:11:56,169] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:11:56,170] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - [0m
[32m[2023-06-19 19:11:56,171] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:11:56,176] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:11:56,193] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:11:56,232] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:11:56,232] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:11:56,232] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:11:56,232] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:11:56,235] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:11:56,242] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 137, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 971, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 772, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 476, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 426, in forward
    attn_output, attn_weights = scaled_dot_product_attention(
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 178, in scaled_dot_product_attention
    attn_weights = F.softmax(attn_weights, axis=-1, dtype="float32").astype(query_states.dtype)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/functional/activation.py", line 1118, in softmax
    outs_cast = x if dtype is None else _C_ops.cast(x, dtype)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   cast_ad_func(paddle::Tensor const&, phi::DataType)
1   paddle::experimental::cast(paddle::Tensor const&, phi::DataType)
2   void phi::CastKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)
3   phi::CastKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DataType, phi::DenseTensor*)::{lambda()#1}::operator()() const
4   void phi::CastCUDAKernelImpl<float, float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
14  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
15  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 50.303955MB memory on GPU 1, 79.313477GB memory has been allocated and available memory is only 34.125000MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:14:24,905] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:14:25,427] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:14:25.428422 101952 tcp_utils.cc:107] Retry to connect to 10.215.195.140:47824 while the server is not yet listening.
I0619 19:14:28.428602 101952 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47824
W0619 19:14:30.000806 101952 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:14:30.006336 101952 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 19:14:30,529] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:14:30,799] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:14:31,089] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:14:31,089] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:14:31,090] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:14:31,101] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:14:31,101] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:14:31,268] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:14:31,269] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,327] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:15:30,039] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:15:30,039] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 413.52it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:15:38,345] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:15:38,369] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:15:38,369] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:15:38,369] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:15:38,370] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:15:38,371] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-14-25_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:15:38,372] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:15:38,373] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - [0m
[32m[2023-06-19 19:15:38,374] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:15:38,380] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:15:38,397] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:15:38,448] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:15:38,448] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:15:38,449] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Total optimization steps = 2500[0m
[32m[2023-06-19 19:15:38,449] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:15:38,451] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:15:39,721] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:15:44,162] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:55,678] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:56,609] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:08,032] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:19,574] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:24,218] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:10,217] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:11,153] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:05,826] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:26,993] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:32,844] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:09,041] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:21,893] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:15,276] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:15,805] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:21:15.806612 105523 tcp_utils.cc:107] Retry to connect to 10.215.195.140:54422 while the server is not yet listening.
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:26,970] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:27,523] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:21:27.524979 105964 tcp_utils.cc:130] Successfully connected to 10.215.195.140:48758
W0619 19:21:29.180617 105964 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:29.186738 105964 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 19:21:29,743] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:29,743] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:29,744] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:21:30,012] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:21:30,281] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:21:30,281] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:21:30,282] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:21:30,434] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
[32m[2023-06-19 19:21:30,435] [    INFO][0m - We are using <class 'paddlenlp.transformers.bloom.tokenizer.BloomTokenizer'> to load 'bigscience/bloomz-7b1-mt'.[0m
[32m[2023-06-19 19:21:30,435] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/vocab.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,765] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/vocab.json[0m
[32m[2023-06-19 19:21:30,766] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/merges.txt and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,981] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/merges.txt[0m
[32m[2023-06-19 19:21:30,982] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/tokenizer.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,381] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer.json[0m
[32m[2023-06-19 19:21:31,383] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[33m[2023-06-19 19:21:31,460] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json> not exist[0m
[32m[2023-06-19 19:21:31,460] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/special_tokens_map.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,683] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/special_tokens_map.json[0m
[32m[2023-06-19 19:21:31,684] [    INFO][0m - Already cached /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
Traceback (most recent call last):
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 32, in try_import
    mod = importlib.import_module(module_name)
  File "/root/miniconda3/envs/paddle/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'regex'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 74, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/tokenizer.py", line 367, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/tokenizer_utils_base.py", line 1593, in from_pretrained
    tokenizer = cls(*init_args, **init_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/bloom/tokenizer.py", line 203, in __init__
    re = try_import("regex")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 41, in try_import
    raise ImportError(err_msg)
ImportError: Failed importing regex. This likely means that some paddle modules require additional dependencies that have to be manually installed (usually with `pip install regex`). 
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:54,144] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:54,699] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:21:54.700277 106515 tcp_utils.cc:107] Retry to connect to 10.215.195.140:62509 while the server is not yet listening.
I0619 19:21:57.700441 106515 tcp_utils.cc:130] Successfully connected to 10.215.195.140:62509
W0619 19:21:59.228313 106515 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:59.233413 106515 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 19:21:59,728] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:22:00,004] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:22:00,285] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [1],  sharding_group: [0, 1, 2, 3], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:22:00,285] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:22:00,286] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:22:00,297] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:22:00,298] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:22:00,372] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:22:00,374] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,461] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:22:36,913] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:22:36,914] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 508.46it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:22:46,116] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:22:46,145] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:22:46,145] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:22:46,145] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:22:46,146] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:22:46,147] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-21-54_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - optimizer_name_suffix         : shard01[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:22:46,148] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - per_device_train_batch_size   : 2[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - sharding                      : [<ShardingOption.FULL_SHARD: 'stage3'>][0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2023-06-19 19:22:46,149] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - train_batch_size              : 2[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - weight_name_suffix            : [0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:22:46,150] [    INFO][0m - [0m
[32m[2023-06-19 19:22:46,151] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:22:46,160] [ WARNING] group_sharded.py:128 - the input of scaler is None, please ensure the logic of your scaler outside is same as GroupShardedScaler.
WARNING:root:While using ClipGradByGlobalNorm in GroupShardedStage3, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:23:21,990] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Instantaneous batch size per device = 2[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:23:21,990] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:23:21,993] [    INFO][0m -   Number of trainable parameters = 13015864320 (per device)[0m
[33m[2023-06-19 19:23:25,335] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 19:23:26,471] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 19:23:27,566] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 19:23:28,743] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:23:50,497] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:25:16,904] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:27,047] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:48,823] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:53,820] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 19:26:54,958] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:56,088] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:58,421] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:28:09,017] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:28:09,559] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0619 19:28:09.560496 109704 tcp_utils.cc:107] Retry to connect to 10.215.195.140:57503 while the server is not yet listening.
I0619 19:28:12.560653 109704 tcp_utils.cc:130] Successfully connected to 10.215.195.140:57503
W0619 19:28:14.038030 109704 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:28:14.045953 109704 gpu_resources.cc:149] device: 1, cuDNN Version: 8.6.
[2023-06-19 19:28:14,542] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:28:15,099] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 1, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [1], pp_group: [1], dp_group: [1], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:28:15,100] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:28:15,101] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:28:15,102] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,102] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:28:15,112] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:28:15,112] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:28:15,178] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:28:15,180] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,241] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:29:12,949] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:29:12,950] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 517.88it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:29:21,495] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - current_device                : gpu:1[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:29:22,255] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - gradient_accumulation_steps   : 2[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:29:22,256] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - local_process_index           : 1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - local_rank                    : 1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-28-09_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:29:22,257] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - process_index                 : 1[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:22,258] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:29:22,259] [    INFO][0m - [0m
[32m[2023-06-19 19:29:22,260] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:29:22,265] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:29:22,283] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:29:22,338] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:29:22,338] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:29:22,338] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Gradient Accumulation steps = 2[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:29:22,338] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:29:22,341] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:29:22,348] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:29:46,777] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:29:54,826] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:34:34,652] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:40:59,344] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[32m[2023-06-19 19:41:02,697] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 2088.39
Effective Tokens per second: 1114.46
