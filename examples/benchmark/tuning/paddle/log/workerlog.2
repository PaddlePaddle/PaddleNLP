/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:48:09,996] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:48:10,535] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 16:48:10.536393 50152 tcp_utils.cc:107] Retry to connect to 10.215.195.140:60812 while the server is not yet listening.
I0619 16:48:13.536538 50152 tcp_utils.cc:130] Successfully connected to 10.215.195.140:60812
W0619 16:48:15.238691 50152 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:48:15.244701 50152 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 16:48:15,744] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:48:16,022] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:48:16,023] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:48:16,330] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:48:16,331] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,332] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:48:16,343] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:48:16,344] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:48:16,422] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:48:16,423] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:48:16,511] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:48:46,747] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:48:46,748] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Traceback (most recent call last):
  File "benchmark.py", line 136, in <module>
    main()
  File "benchmark.py", line 114, in main
    dataset = load_dataset("Chinese-Vicuna/guanaco_belle_merge_v1.0")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1773, in load_dataset
    builder_instance = load_dataset_builder(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1502, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1219, in dataset_module_factory
    raise e1 from None
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/datasets/load.py", line 1175, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'Chinese-Vicuna/guanaco_belle_merge_v1.0' on the Hub (ConnectionError)
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 16:50:39,129] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 16:50:39,649] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 16:50:39.650616 51459 tcp_utils.cc:130] Successfully connected to 10.215.195.140:46550
W0619 16:50:41.245326 51459 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 16:50:41.250523 51459 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 16:50:41,750] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 16:50:42,024] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 16:50:42,301] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 16:50:42,302] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 16:50:42,303] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 16:50:42,303] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,303] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 16:50:42,313] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 16:50:42,313] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 16:50:42,376] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 16:50:42,377] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 16:50:42,438] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 16:51:10,543] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 16:51:10,543] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset json (/root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 169.07it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/Chinese-Vicuna___json/Chinese-Vicuna--guanaco_belle_merge_v1.0-c7b7cea160a93e0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7cf1329c0e366a03.arrow
[32m[2023-06-19 16:51:19,420] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 16:51:19,449] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 16:51:19,450] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_16-50-39_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 16:51:19,451] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 16:51:19,452] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 16:51:19,453] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - [0m
[32m[2023-06-19 16:51:19,454] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 16:51:19,459] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 16:51:19,471] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 16:51:19,501] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 16:51:19,501] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 16:51:19,501] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 16:51:19,502] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 16:51:19,504] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 16:51:22,521] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 16:51:24,786] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 16:51:25,841] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 16:51:26,605] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 16:51:27,237] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 16:51:27,908] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 16:51:28,522] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:28,868] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 16:51:30,852] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5
[33m[2023-06-19 16:51:31,368] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 2048.0[0m
Traceback (most recent call last):
  File "benchmark.py", line 139, in <module>
    main()
  File "benchmark.py", line 131, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 971, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 772, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 488, in forward
    hidden_states = self.mlp(hidden_states)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 324, in forward
    return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 504, in forward
    output_parallel = self.linear(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/functional/common.py", line 1842, in linear
    return _C_ops.linear(x, weight, bias)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, phi::dtype::float16>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::Allocator::Allocate(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
14  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
15  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 55.187500MB memory on GPU 2, 79.323242GB memory has been allocated and available memory is only 24.125000MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:29:03,151] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:29:03,676] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 17:29:03.677599 65731 tcp_utils.cc:130] Successfully connected to 10.215.195.140:36267
W0619 17:29:08.399518 65731 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:29:08.408960 65731 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 17:29:08,947] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:29:09,285] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:29:09,286] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:29:09,634] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:29:09,634] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:29:09,635] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:29:09,636] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,636] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:29:09,646] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:29:09,646] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:29:09,721] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:29:09,722] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:29:09,780] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:29:39,677] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:29:39,677] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 459.45it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:29:47,069] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:29:47,087] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:29:47,088] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-29-03_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:29:47,089] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:29:47,090] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:29:47,091] [    INFO][0m - [0m
[32m[2023-06-19 17:29:47,092] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:29:47,096] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:29:47,107] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:29:47,133] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:29:47,133] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:29:47,133] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 17:29:47,133] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:29:47,135] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:29:48,115] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:29:49,273] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:29:49,762] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:29:54,792] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:32:36,875] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:34:19,348] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:36:08,111] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 17:36:09,822] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4879.48
Effective Tokens per second: 2050.49
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:39:10,657] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:39:11,172] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 17:39:11.173524 70196 tcp_utils.cc:130] Successfully connected to 10.215.195.140:39512
W0619 17:39:15.634965 70196 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:39:15.639818 70196 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 17:39:16,147] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:39:16,439] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:39:16,718] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:39:16,719] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:39:16,719] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:39:16,720] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,720] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:39:16,730] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:39:16,730] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:39:16,800] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:39:16,801] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:39:16,876] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:39:47,222] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:39:47,222] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 371.77it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-ff95b1909b84b133.arrow
[32m[2023-06-19 17:39:54,859] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:39:54,877] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:39:54,877] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:39:54,877] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:39:54,878] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:39:54,879] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-39-11_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:54,880] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:39:54,881] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - [0m
[32m[2023-06-19 17:39:54,882] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:39:54,886] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:39:54,898] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:39:54,924] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:39:54,924] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:39:54,924] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:39:54,924] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:39:54,924] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:39:54,924] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:39:54,925] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:39:54,925] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:39:54,925] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:39:54,925] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:39:54,927] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:39:55,197] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:39:56,389] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:39:57,044] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:39:58,036] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:39:58,352] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
Traceback (most recent call last):
  File "benchmark.py", line 144, in <module>
    main()
  File "benchmark.py", line 136, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 995, in forward
    loss = self.criterion(logits, labels)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 842, in forward
    masked_lm_loss = self.loss_func(prediction_scores.astype("float32"), masked_lm_labels.unsqueeze(2))
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 563, in forward
    loss = mp_ops._c_softmax_with_cross_entropy(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_ops.py", line 405, in _c_softmax_with_cross_entropy
    softmax, loss = _legacy_C_ops.c_softmax_with_cross_entropy(
SystemError: (Fatal) Operator c_softmax_with_cross_entropy raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   c_softmax_with_cross_entropy_dygraph_function(paddle::Tensor const&, paddle::Tensor const&, paddle::framework::AttributeMap const&)
1   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameTensorMap const&, paddle::imperative::NameTensorMap const&, paddle::framework::AttributeMap&, phi::Place const&, paddle::framework::AttributeMap*, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
2   void paddle::imperative::Tracer::TraceOpImpl<egr::EagerVariable>(std::string const&, paddle::imperative::details::NameVarMapTrait<egr::EagerVariable>::Type const&, paddle::imperative::details::NameVarMapTrait<egr::EagerVariable>::Type const&, paddle::framework::AttributeMap&, phi::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&, paddle::framework::AttributeMap*, bool)
3   paddle::imperative::PreparedOp::Run(paddle::imperative::NameTensorMap const&, paddle::imperative::NameTensorMap const&, paddle::framework::AttributeMap const&, paddle::framework::AttributeMap const&)
4   paddle::framework::StructKernelImpl<paddle::operators::CSoftmaxWithCrossEntropyOpCUDAKernel<float, phi::GPUContext>, void>::Compute(phi::KernelContext*)
5   paddle::operators::CSoftmaxWithCrossEntropyProcessGroupFunctor<phi::GPUContext, float>::operator()(paddle::framework::ExecutionContext const&)
6   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
7   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
8   paddle::memory::AllocShared(phi::Place const&, unsigned long)
9   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
10  paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
11  paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::Allocator::Allocate(unsigned long)
14  paddle::memory::allocation::Allocator::Allocate(unsigned long)
15  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
16  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
17  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 235.351562MB memory on GPU 2, 79.303711GB memory has been allocated and available memory is only 44.125000MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)
. (at ../paddle/fluid/imperative/tracer.cc:322)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 17:45:56,871] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 17:45:57,388] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 17:45:57.389300 74031 tcp_utils.cc:107] Retry to connect to 10.215.195.140:42038 while the server is not yet listening.
I0619 17:46:00.389462 74031 tcp_utils.cc:130] Successfully connected to 10.215.195.140:42038
W0619 17:46:03.710645 74031 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 17:46:03.716182 74031 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 17:46:04,304] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 17:46:04,633] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 17:46:04,634] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 17:46:04,940] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 17:46:04,941] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 17:46:04,941] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 17:46:04,942] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:04,942] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 17:46:04,952] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 17:46:04,952] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 17:46:05,012] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 17:46:05,013] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 17:46:05,068] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 17:46:35,329] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 17:46:35,329] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 536.49it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 17:46:43,692] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 17:46:43,710] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 17:46:43,711] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_17-45-57_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 17:46:43,712] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 17:46:43,713] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 17:46:43,714] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - [0m
[32m[2023-06-19 17:46:43,715] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 17:46:43,719] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 17:46:43,730] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 17:46:43,756] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 17:46:43,756] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 17:46:43,756] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 17:46:43,756] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 17:46:43,756] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 17:46:43,757] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 17:46:43,757] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 17:46:43,757] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 17:46:43,757] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 17:46:43,757] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 17:46:43,759] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 17:46:44,158] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 17:46:45,372] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 17:46:46,035] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 17:46:47,030] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 17:46:47,346] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 17:52:06,677] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6995.56
Effective Tokens per second: 2420.05
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:03:14,628] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:03:15,154] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 18:03:15.155544 81937 tcp_utils.cc:107] Retry to connect to 10.215.195.140:43302 while the server is not yet listening.
I0619 18:03:18.155670 81937 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43302
W0619 18:03:19.931156 81937 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:03:19.936915 81937 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 18:03:20,613] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:03:21,005] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:03:21,006] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:03:21,357] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:03:21,358] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:03:21,359] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:03:21,360] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,360] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:03:21,370] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:03:21,370] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:03:21,456] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:03:21,458] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:03:21,544] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 78, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 816, in from_pretrained
    return cls._from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/modeling.py", line 370, in _from_pretrained
    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1332, in from_pretrained
    model = cls(config, *init_args, **model_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 887, in __init__
    self.llama = LlamaModel(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in __init__
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 651, in <listcomp>
    self.layers = nn.LayerList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 452, in __init__
    self.self_attn = LlamaAttention(config)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 351, in __init__
    fuse_matmul_bias=mpu.mp_layers.is_fused_matmul_bias_supported(),
AttributeError: module 'paddle.distributed.fleet.meta_parallel' has no attribute 'mp_layers'
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:05:47,635] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:05:48,162] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 18:05:48.163755 83411 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44671
W0619 18:05:52.558092 83411 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:05:52.563436 83411 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 18:05:53,056] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:05:53,328] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:05:53,329] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:05:53,602] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:05:53,602] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:05:53,603] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,604] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:05:53,614] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:05:53,615] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:05:53,678] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:05:53,680] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:05:53,738] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:06:23,609] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:06:23,610] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 406.70it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:06:32,038] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:06:32,057] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:32,058] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-05-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:06:32,059] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:06:32,060] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:06:32,061] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:06:32,062] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:06:32,062] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 18:06:32,062] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:06:32,062] [    INFO][0m - [0m
[32m[2023-06-19 18:06:32,062] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:06:32,066] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:06:32,078] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:06:32,104] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:06:32,104] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:06:32,104] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:06:32,104] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:06:32,106] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:06:35,106] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 137, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 978, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 779, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 483, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 441, in forward
    attn_output = self.o_proj(attn_output)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 493, in forward
    bias = MPScale.apply(self.bias, self.world_size)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 335, in forward
    out = paddle.scale(x, 1.0 / mp_degree)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/tensor/math.py", line 224, in scale
    return _C_ops.scale(x, scale, float(bias), bias_after_scale)
ValueError: (InvalidArgument) scale(): argument 'x' (position 0) must be Tensor, but got None (at ../paddle/fluid/pybind/eager_utils.cc:1024)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:09:30,181] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:09:30,710] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 18:09:30.711783 85102 tcp_utils.cc:107] Retry to connect to 10.215.195.140:43792 while the server is not yet listening.
I0619 18:09:33.711944 85102 tcp_utils.cc:130] Successfully connected to 10.215.195.140:43792
W0619 18:09:35.320109 85102 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:09:35.325413 85102 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 18:09:35,818] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:09:36,085] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:09:36,086] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:09:36,365] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:09:36,366] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:09:36,367] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:09:36,367] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,368] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:09:36,378] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:09:36,450] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:09:36,452] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:09:36,514] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:10:04,576] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:10:04,576] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 426.73it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:10:13,002] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:10:13,023] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:10:13,024] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-09-30_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:10:13,025] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - per_device_train_batch_size   : 16[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:10:13,026] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - train_batch_size              : 16[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:10:13,027] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:10:13,028] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 18:10:13,028] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:10:13,028] [    INFO][0m - [0m
[32m[2023-06-19 18:10:13,028] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:10:13,032] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:10:13,044] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:10:13,072] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:10:13,072] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:10:13,073] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Instantaneous batch size per device = 16[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 16[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Total optimization steps = 625[0m
[32m[2023-06-19 18:10:13,073] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:10:13,075] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:10:14,562] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:10:15,735] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:10:16,380] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:10:17,363] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 18:10:17,679] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[32m[2023-06-19 18:15:37,527] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 6985.94
Effective Tokens per second: 2416.72
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 18:44:00,396] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 18:44:00,911] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 18:44:00.912106 91008 tcp_utils.cc:107] Retry to connect to 10.215.195.140:44229 while the server is not yet listening.
I0619 18:44:03.912264 91008 tcp_utils.cc:130] Successfully connected to 10.215.195.140:44229
W0619 18:44:05.587668 91008 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 18:44:05.592976 91008 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 18:44:06,078] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 18:44:06,351] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 18:44:06,352] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 18:44:06,621] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 18:44:06,621] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 18:44:06,622] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,623] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2023-06-19 18:44:06,632] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2023-06-19 18:44:06,632] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2023-06-19 18:44:06,749] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2023-06-19 18:44:06,750] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2023-06-19 18:44:06,812] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/model_state.pdparams[0m
[32m[2023-06-19 18:44:37,813] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 18:44:37,813] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 384.48it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9609fdc6d1460e88.arrow
422
[32m[2023-06-19 18:44:46,038] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 18:44:46,065] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 18:44:46,066] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_18-44-00_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 18:44:46,067] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 18:44:46,068] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 18:44:46,069] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 18:44:46,070] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 18:44:46,070] [    INFO][0m - [0m
[32m[2023-06-19 18:44:46,070] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 18:44:46,074] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 18:44:46,086] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 18:44:46,111] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 18:44:46,111] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 18:44:46,111] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 18:44:46,112] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 18:44:46,114] [    INFO][0m -   Number of trainable parameters = 1684803584 (per device)[0m
[32m[2023-06-19 18:44:46,116] [    INFO][0m -   Number of trainable parameters = 6739214336 (all devices, roughly)[0m
[33m[2023-06-19 18:44:47,490] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 18:44:48,070] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:44:55,422] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:46:09,524] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[33m[2023-06-19 18:51:09,140] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
[32m[2023-06-19 18:51:10,847] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 4795.92
Effective Tokens per second: 2028.73
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:10:47,535] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:10:48,057] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:10:48.058931 100379 tcp_utils.cc:107] Retry to connect to 10.215.195.140:47565 while the server is not yet listening.
I0619 19:10:51.059106 100379 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47565
W0619 19:10:52.558475 100379 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:10:52.563735 100379 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 19:10:53,083] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:10:53,380] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:10:53,381] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:10:53,652] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:10:53,653] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:10:53,653] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:10:53,654] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,654] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:10:53,663] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:10:53,663] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:10:53,741] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:10:53,743] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:10:53,823] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:11:46,367] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:11:46,368] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 519.48it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:11:54,733] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:11:54,756] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:11:54,757] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-10-48_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:11:54,758] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - per_device_train_batch_size   : 8[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:11:54,759] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - train_batch_size              : 8[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:11:54,760] [    INFO][0m - [0m
[32m[2023-06-19 19:11:54,761] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:11:54,766] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:11:54,783] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:11:54,822] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:11:54,822] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:11:54,822] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:11:54,822] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:11:54,825] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:11:56,242] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 137, in main
    train_metrics = trainer.train()
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 718, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/examples/benchmark/peft/paddle/utils.py", line 24, in training_step
    return super().training_step(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1529, in training_step
    loss = self.compute_loss(model, inputs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/trainer/trainer.py", line 1488, in compute_loss
    outputs = model(**inputs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/meta_parallel/meta_parallel_base.py", line 37, in forward
    output = self._layers(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 971, in forward
    outputs = self.llama(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 772, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 476, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/llama/modeling.py", line 406, in forward
    value_states = self.v_proj(hidden_states).reshape(shape=[bsz, q_len, self.num_heads, self.head_dim])
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1253, in __call__
    return self.forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_layers.py", line 313, in forward
    input_parallel = mp_ops._c_identity(
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_ops.py", line 64, in _c_identity
    return c_identity_eager.apply(tensor)
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/distributed/fleet/layers/mpu/mp_ops.py", line 48, in forward
    return _legacy_C_ops.c_identity(
SystemError: (Fatal) Operator c_identity raises an paddle::memory::allocation::BadAlloc exception.
The exception content is
:

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::pylayer_method_apply(_object*, _object*, _object*)
1   c_identity_dygraph_function(paddle::Tensor const&, paddle::framework::AttributeMap const&)
2   c_identity_dygraph_function(paddle::Tensor const&, paddle::framework::AttributeMap const&)
3   paddle::imperative::Tracer::TraceOp(std::string const&, paddle::imperative::NameTensorMap const&, paddle::imperative::NameTensorMap const&, paddle::framework::AttributeMap&, phi::Place const&, paddle::framework::AttributeMap*, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&)
4   void paddle::imperative::Tracer::TraceOpImpl<egr::EagerVariable>(std::string const&, paddle::imperative::details::NameVarMapTrait<egr::EagerVariable>::Type const&, paddle::imperative::details::NameVarMapTrait<egr::EagerVariable>::Type const&, paddle::framework::AttributeMap&, phi::Place const&, bool, std::map<std::string, std::string, std::less<std::string >, std::allocator<std::pair<std::string const, std::string > > > const&, paddle::framework::AttributeMap*, bool)
5   paddle::imperative::PreparedOp::Run(paddle::imperative::NameTensorMap const&, paddle::imperative::NameTensorMap const&, paddle::framework::AttributeMap const&, paddle::framework::AttributeMap const&)
6   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
7   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
8   paddle::memory::AllocShared(phi::Place const&, unsigned long)
9   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
10  paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
11  paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
12  paddle::memory::allocation::Allocator::Allocate(unsigned long)
13  paddle::memory::allocation::Allocator::Allocate(unsigned long)
14  paddle::memory::allocation::Allocator::Allocate(unsigned long)
15  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
16  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
17  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 31.718750MB memory on GPU 2, 79.319336GB memory has been allocated and available memory is only 28.125000MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)
. (at ../paddle/fluid/imperative/tracer.cc:322)

/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:14:24,907] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:14:25,438] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:14:25.439893 101954 tcp_utils.cc:107] Retry to connect to 10.215.195.140:47824 while the server is not yet listening.
I0619 19:14:28.440057 101954 tcp_utils.cc:130] Successfully connected to 10.215.195.140:47824
W0619 19:14:30.000870 101954 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:14:30.006250 101954 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 19:14:30,529] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:14:30,798] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:14:30,799] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:14:31,089] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:14:31,089] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:14:31,090] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,091] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:14:31,101] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:14:31,102] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:14:31,167] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:14:31,168] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:14:31,228] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:15:26,152] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:15:26,153] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 397.38it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:15:34,415] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:15:34,449] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:15:34,450] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:15:34,451] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-14-25_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:15:34,452] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:15:34,453] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:15:34,454] [    INFO][0m - [0m
[32m[2023-06-19 19:15:34,455] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:15:34,462] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:15:34,480] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:15:34,525] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:15:34,525] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:15:34,525] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:15:34,525] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:15:34,525] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:15:34,525] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:15:34,526] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 4[0m
[32m[2023-06-19 19:15:34,526] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:15:34,526] [    INFO][0m -   Total optimization steps = 2500[0m
[32m[2023-06-19 19:15:34,526] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:15:34,528] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:15:39,721] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:15:44,163] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:55,679] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:15:56,609] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:08,033] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:19,574] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:16:24,219] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:10,218] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:18:11,153] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:05,825] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:26,994] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:19:32,844] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:09,041] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:20:21,893] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:15,243] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:15,792] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:21:15.793035 105525 tcp_utils.cc:107] Retry to connect to 10.215.195.140:54422 while the server is not yet listening.
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:26,979] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:27,515] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:21:27.516911 105966 tcp_utils.cc:130] Successfully connected to 10.215.195.140:48758
W0619 19:21:29.180691 105966 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:29.186161 105966 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 19:21:29,742] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:29,743] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:29,744] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:21:30,012] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:21:30,281] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:21:30,281] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:21:30,282] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:21:30,484] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
[32m[2023-06-19 19:21:30,486] [    INFO][0m - We are using <class 'paddlenlp.transformers.bloom.tokenizer.BloomTokenizer'> to load 'bigscience/bloomz-7b1-mt'.[0m
[32m[2023-06-19 19:21:30,486] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/vocab.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,752] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/vocab.json[0m
[32m[2023-06-19 19:21:30,753] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/merges.txt and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:30,988] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/merges.txt[0m
[32m[2023-06-19 19:21:30,989] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/tokenizer.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,411] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer.json[0m
[32m[2023-06-19 19:21:31,412] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[33m[2023-06-19 19:21:31,498] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/added_tokens.json> not exist[0m
[32m[2023-06-19 19:21:31,499] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/bigscience/bloomz-7b1-mt/special_tokens_map.json and saved to /root/.paddlenlp/models/bigscience/bloomz-7b1-mt[0m
[32m[2023-06-19 19:21:31,705] [    INFO][0m - Found /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/special_tokens_map.json[0m
[32m[2023-06-19 19:21:31,706] [    INFO][0m - Already cached /root/.paddlenlp/models/bigscience/bloomz-7b1-mt/tokenizer_config.json[0m
Traceback (most recent call last):
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 32, in try_import
    mod = importlib.import_module(module_name)
  File "/root/miniconda3/envs/paddle/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 973, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'regex'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "benchmark.py", line 145, in <module>
    main()
  File "benchmark.py", line 74, in main
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/auto/tokenizer.py", line 367, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/tokenizer_utils_base.py", line 1593, in from_pretrained
    tokenizer = cls(*init_args, **init_kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/utils.py", line 226, in __impl__
    init_func(self, *args, **kwargs)
  File "/root/paddlejob/workspace/hesijun/PaddleNLP/paddlenlp/transformers/bloom/tokenizer.py", line 203, in __init__
    re = try_import("regex")
  File "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/utils/lazy_import.py", line 41, in try_import
    raise ImportError(err_msg)
ImportError: Failed importing regex. This likely means that some paddle modules require additional dependencies that have to be manually installed (usually with `pip install regex`). 
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:21:54,152] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:21:54,718] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:21:54.719187 106517 tcp_utils.cc:107] Retry to connect to 10.215.195.140:62509 while the server is not yet listening.
I0619 19:21:57.719358 106517 tcp_utils.cc:130] Successfully connected to 10.215.195.140:62509
W0619 19:21:59.226099 106517 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:21:59.231427 106517 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 19:21:59,728] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 model comm group(s) create successfully!
[2023-06-19 19:21:59,729] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:22:00,004] [    INFO] topology.py:269 - Total 1 sharding comm group(s) create successfully!
[2023-06-19 19:22:00,285] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:22:00,285] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:22:00,286] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,287] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:22:00,297] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:22:00,297] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:22:00,376] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:22:00,377] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:22:00,540] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:22:37,607] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:22:37,608] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 366.89it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:22:46,628] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:22:46,652] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - dataset_rank                  : 2[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:22:46,653] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - gradient_accumulation_steps   : 1[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-21-54_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:22:46,654] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - optimizer_name_suffix         : shard02[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - per_device_train_batch_size   : 2[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:22:46,655] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - sharding                      : [<ShardingOption.FULL_SHARD: 'stage3'>][0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - sharding_parallel_rank        : 2[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - should_save_model_state       : False[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - tensor_parallel_degree        : -1[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - train_batch_size              : 2[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - weight_name_suffix            : [0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:22:46,656] [    INFO][0m - [0m
[32m[2023-06-19 19:22:46,657] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:22:46,664] [ WARNING] group_sharded.py:128 - the input of scaler is None, please ensure the logic of your scaler outside is same as GroupShardedScaler.
WARNING:root:While using ClipGradByGlobalNorm in GroupShardedStage3, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:23:22,099] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:23:22,099] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:23:22,099] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:23:22,100] [    INFO][0m -   Instantaneous batch size per device = 2[0m
[32m[2023-06-19 19:23:22,100] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:23:22,100] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-06-19 19:23:22,100] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:23:22,100] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:23:22,102] [    INFO][0m -   Number of trainable parameters = 13015864320 (per device)[0m
[33m[2023-06-19 19:23:25,335] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2023-06-19 19:23:26,472] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[33m[2023-06-19 19:23:27,566] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 16384.0[0m
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
[33m[2023-06-19 19:23:28,743] [ WARNING][0m - optimizer not run, scale_before: 16384.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:23:50,496] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:25:16,904] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:27,047] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:48,823] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
[33m[2023-06-19 19:26:53,820] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 8192.0[0m
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
[33m[2023-06-19 19:26:54,958] [ WARNING][0m - optimizer not run, scale_before: 8192.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:56,088] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
[33m[2023-06-19 19:26:58,421] [ WARNING][0m - optimizer not run, scale_before: 4096.0, scale_after: 4096.0[0m
/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2023-06-19 19:28:09,003] [ WARNING][0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues[0m
[2023-06-19 19:28:09,534] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0619 19:28:09.535401 109706 tcp_utils.cc:107] Retry to connect to 10.215.195.140:57503 while the server is not yet listening.
I0619 19:28:12.535560 109706 tcp_utils.cc:130] Successfully connected to 10.215.195.140:57503
W0619 19:28:14.038056 109706 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7
W0619 19:28:14.043457 109706 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2023-06-19 19:28:14,542] [    INFO] topology.py:269 - Total 4 data comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 1 model comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 pipe comm group(s) create successfully!
[2023-06-19 19:28:14,819] [    INFO] topology.py:269 - Total 4 sharding comm group(s) create successfully!
[2023-06-19 19:28:15,099] [    INFO] topology.py:220 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2023-06-19 19:28:15,100] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2023-06-19 19:28:15,101] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-06-19 19:28:15,101] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,102] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-13b/sentencepiece.bpe.model[0m
[32m[2023-06-19 19:28:15,112] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-13b/tokenizer_config.json[0m
[32m[2023-06-19 19:28:15,112] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-13b/special_tokens_map.json[0m
[32m[2023-06-19 19:28:15,180] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/config.json[0m
[32m[2023-06-19 19:28:15,181] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-13b'.[0m
[32m[2023-06-19 19:28:15,246] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-13b/model_state.pdparams[0m
[32m[2023-06-19 19:29:09,369] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2023-06-19 19:29:09,369] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-13b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
Found cached dataset parquet (/root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 541.27it/s]
Loading cached processed dataset at /root/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-91d3f5a1facd5ac1.arrow
422
[32m[2023-06-19 19:29:18,099] [    INFO][0m - Using half precision[0m
[32m[2023-06-19 19:29:18,122] [    INFO][0m - ============================================================[0m
[32m[2023-06-19 19:29:18,122] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-06-19 19:29:18,122] [    INFO][0m - paddle commit id              : 752670e21b66ff1c121c423c630b5d95f6e086be[0m
[32m[2023-06-19 19:29:18,122] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - amp_master_grad               : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - bf16                          : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - current_device                : gpu:2[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - device                        : gpu[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - disable_tqdm                  : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - do_eval                       : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - do_export                     : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - do_predict                    : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - do_train                      : False[0m
[32m[2023-06-19 19:29:18,123] [    INFO][0m - eval_accumulation_steps       : None[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - eval_steps                    : None[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - evaluation_strategy           : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - fp16                          : True[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - gradient_accumulation_steps   : 2[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - greater_is_better             : None[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - label_names                   : None[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - learning_rate                 : 5e-05[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - load_best_model_at_end        : False[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - local_process_index           : 2[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - local_rank                    : 2[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - log_level                     : -1[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - log_level_replica             : -1[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - log_on_each_node              : True[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - logging_dir                   : outputs/runs/Jun19_19-28-09_yq02-inf-sci-k8s-a800-hbxgn6-0099.yq02.baidu.com[0m
[32m[2023-06-19 19:29:18,124] [    INFO][0m - logging_first_step            : False[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - logging_steps                 : 50[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - lr_scheduler_type             : SchedulerType.LINEAR[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - max_steps                     : -1[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - metric_for_best_model         : None[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - no_cuda                       : False[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - num_train_epochs              : 1.0[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - output_dir                    : outputs[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - past_index                    : -1[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - pipeline_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:18,125] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - process_index                 : 2[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - recompute                     : False[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - run_name                      : outputs[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - save_on_each_node             : False[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - save_steps                    : 500[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - save_strategy                 : IntervalStrategy.NO[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - save_total_limit              : None[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - seed                          : 42[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - sharding                      : [][0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - sharding_degree               : -1[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - sharding_parallel_degree      : -1[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - should_log                    : False[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - should_save                   : False[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - should_save_model_state       : True[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2023-06-19 19:29:18,126] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - train_batch_size              : 4[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - warmup_ratio                  : 0.0[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - weight_decay                  : 0.0[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - world_size                    : 4[0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - [0m
[32m[2023-06-19 19:29:18,127] [    INFO][0m - The following columns in the training set  don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: text. If text are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.[0m
[2023-06-19 19:29:18,132] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2023-06-19 19:29:18,149] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2023-06-19 19:29:18,188] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2023-06-19 19:29:18,188] [ WARNING] hybrid_parallel_optimizer.py:261 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2023-06-19 19:29:18,188] [    INFO][0m - ***** Running training *****[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Num examples = 10000[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 8[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Gradient Accumulation steps = 2[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Total optimization steps = 1250[0m
[32m[2023-06-19 19:29:18,188] [    INFO][0m -   Total num train samples = 10000[0m
[32m[2023-06-19 19:29:18,191] [    INFO][0m -   Number of trainable parameters = 3254277120 (per device)[0m
[32m[2023-06-19 19:29:22,348] [    INFO][0m -   Number of trainable parameters = 13017108480 (all devices, roughly)[0m
[33m[2023-06-19 19:29:46,776] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:29:54,826] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:34:34,652] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[33m[2023-06-19 19:40:59,345] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 32768.0[0m
[32m[2023-06-19 19:41:02,697] [    INFO][0m - 
Training completed. 
[0m
Tokens per second: 2088.39
Effective Tokens per second: 1114.46
