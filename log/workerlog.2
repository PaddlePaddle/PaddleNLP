/ssd1/zhangbin41/miniconda3/envs/paddle_env/bin/python: can't open file 'test.py': [Errno 2] No such file or directory
/ssd1/zhangbin41/miniconda3/envs/paddle_env/bin/python: can't open file 'modelzoo/ernie-health/test.py': [Errno 2] No such file or directory
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 14:41:40,557] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
<function init_argv at 0x7f55e9dd4550>
[32m[2023-01-04 14:41:40,565] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 14:41:40,565] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:41:40,565] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 14:41:40,565] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:41:40,565] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - [0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 14:41:40,566] [    INFO][0m - [0m
I0104 14:41:40.567219 10648 tcp_utils.cc:130] Successfully connected to 10.255.129.12:36777
Traceback (most recent call last):
  File "model_zoo/ernie-health/test.py", line 27, in <module>
    test_pretrain()
  File "model_zoo/ernie-health/test.py", line 24, in test_pretrain
    main()
  File "/ssd1/zhangbin41/PaddleNLP/model_zoo/ernie-health/run_pretrain_trainer.py", line 100, in main
    f"Process rank: {training_args.local_rank}, device: {training_args.device}, world_size: {training_args.world_size}, "
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/trainer/training_args.py", line 680, in world_size
    paddle.distributed.init_parallel_env()
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 302, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 332, in barrier
    task = group.process_group.barrier(device_id)
RuntimeError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::distributed::ProcessGroupNCCL::Barrier(paddle::distributed::BarrierOptions const&)
1   phi::DenseTensor::DenseTensor(phi::Allocator*, phi::DenseTensorMeta const&)
2   paddle::experimental::DefaultAllocator::Allocate(unsigned long)
3   paddle::memory::Alloc(phi::Place const&, unsigned long)
4   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
5   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 256.000000B memory on GPU 3, 127.337851TB memory has been allocated and available memory is only 0.000000B.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=true`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:95)

/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 14:42:16,799] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
<function init_argv at 0x7fdfc1c77550>
[32m[2023-01-04 14:42:16,807] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 14:42:16,807] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:42:16,807] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 14:42:16,807] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:42:16,807] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 14:42:16,807] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - [0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 14:42:16,808] [    INFO][0m - [0m
I0104 14:42:16.809118 17113 tcp_utils.cc:130] Successfully connected to 10.255.129.12:39813
W0104 14:42:19.213506 17113 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 14:42:19.218555 17113 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 14:42:19,586] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 14:42:19,587] [    INFO][0m - Already cached /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 14:42:19,621] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 14:42:19,621] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 14:42:19,624] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[32m[2023-01-04 14:42:19,626] [    INFO][0m - Configuration saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/config.json[0m
[33m[2023-01-04 14:42:20,080] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:42:20,156] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:42:20,743] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:42:20,825] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:42:21,105] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[32m[2023-01-04 14:42:32,867] [    INFO][0m - Found /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams[0m
Traceback (most recent call last):
  File "model_zoo/ernie-health/test.py", line 27, in <module>
    test_pretrain()
  File "model_zoo/ernie-health/test.py", line 24, in test_pretrain
    main()
  File "/ssd1/zhangbin41/PaddleNLP/model_zoo/ernie-health/run_pretrain_trainer.py", line 126, in main
    model = model_class.from_pretrained(model_args.model_name_or_path)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 440, in from_pretrained
    return cls.from_pretrained_v2(pretrained_model_name_or_path, from_hf_hub=from_hf_hub, *args, **kwargs)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1264, in from_pretrained_v2
    model_weight_file = cls._resolve_model_file_path(
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 971, in _resolve_model_file_path
    weight_file_path = get_path_from_url_with_filelock(pretrained_model_name_or_path, cache_dir)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 192, in get_path_from_url_with_filelock
    result = get_path_from_url(url=url, root_dir=root_dir, md5sum=md5sum, check_exist=check_exist)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 159, in get_path_from_url
    if tarfile.is_tarfile(fullpath) or zipfile.is_zipfile(fullpath):
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 2466, in is_tarfile
    t = open(name)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1599, in open
    return func(name, "r", fileobj, **kwargs)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1696, in bz2open
    fileobj = bz2.BZ2File(fileobj or name, mode,
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/bz2.py", line 96, in __init__
    self._fp = _builtin_open(filename, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams'
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 14:44:21,969] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
<function init_argv at 0x7f16d021c550>
[32m[2023-01-04 14:44:21,977] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 14:44:21,977] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:44:21,977] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - [0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 14:44:21,978] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 14:44:21,979] [    INFO][0m - [0m
I0104 14:44:21.979442 37290 tcp_utils.cc:130] Successfully connected to 10.255.129.12:40353
W0104 14:44:24.916123 37290 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 14:44:24.923972 37290 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 14:44:25,331] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 14:44:25,332] [    INFO][0m - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie-health-chinese/vocab.txt and saved to /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese[0m
[32m[2023-01-04 14:44:25,625] [    INFO][0m - Found /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 14:44:25,663] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 14:44:25,664] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 14:44:25,667] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[33m[2023-01-04 14:44:26,099] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:44:26,171] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:44:26,712] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:44:26,784] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:44:27,037] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[32m[2023-01-04 14:44:38,153] [    INFO][0m - Found /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams[0m
Traceback (most recent call last):
  File "model_zoo/ernie-health/test.py", line 27, in <module>
    test_pretrain()
  File "model_zoo/ernie-health/test.py", line 24, in test_pretrain
    main()
  File "/ssd1/zhangbin41/PaddleNLP/model_zoo/ernie-health/run_pretrain_trainer.py", line 126, in main
    model = model_class.from_pretrained(model_args.model_name_or_path)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 440, in from_pretrained
    return cls.from_pretrained_v2(pretrained_model_name_or_path, from_hf_hub=from_hf_hub, *args, **kwargs)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1264, in from_pretrained_v2
    model_weight_file = cls._resolve_model_file_path(
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 971, in _resolve_model_file_path
    weight_file_path = get_path_from_url_with_filelock(pretrained_model_name_or_path, cache_dir)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 192, in get_path_from_url_with_filelock
    result = get_path_from_url(url=url, root_dir=root_dir, md5sum=md5sum, check_exist=check_exist)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 159, in get_path_from_url
    if tarfile.is_tarfile(fullpath) or zipfile.is_zipfile(fullpath):
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 2466, in is_tarfile
    t = open(name)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1599, in open
    return func(name, "r", fileobj, **kwargs)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1696, in bz2open
    fileobj = bz2.BZ2File(fileobj or name, mode,
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/bz2.py", line 96, in __init__
    self._fp = _builtin_open(filename, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams'
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 14:53:37,924] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
<function init_argv at 0x7f0ca22d3550>
[32m[2023-01-04 14:53:37,932] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 14:53:37,932] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:53:37,932] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - [0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 14:53:37,933] [    INFO][0m - [0m
I0104 14:53:37.934512  6590 tcp_utils.cc:130] Successfully connected to 10.255.129.12:38908
W0104 14:53:43.236181  6590 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 14:53:43.240182  6590 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 14:53:43,602] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 14:53:43,604] [    INFO][0m - Already cached /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 14:53:43,637] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 14:53:43,638] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 14:53:43,640] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[33m[2023-01-04 14:53:44,170] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:53:44,251] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:53:44,779] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:53:44,857] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:53:45,173] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:53:46,064] [ WARNING][0m - Some weights of the model checkpoint at ernie-health-chinese were not used when initializing ErnieHealthForTotalPretraining: ['electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.encoder.layers.11.linear2.weight', 'electra.encoder.layers.4.norm1.weight', 'electra.encoder.layers.6.linear2.weight', 'electra.encoder.layers.10.linear2.bias', 'electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.encoder.layers.5.linear1.weight', 'electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.encoder.layers.11.norm1.weight', 'electra.encoder.layers.1.linear1.bias', 'electra.encoder.layers.11.norm1.bias', 'electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.encoder.layers.1.norm1.bias', 'electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.encoder.layers.2.norm2.bias', 'electra.encoder.layers.8.norm1.weight', 'electra.encoder.layers.7.norm2.weight', 'electra.encoder.layers.4.linear1.bias', 'electra.encoder.layers.8.norm1.bias', 'electra.encoder.layers.5.linear1.bias', 'electra.encoder.layers.8.linear1.bias', 'electra.encoder.layers.9.norm1.bias', 'electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.encoder.layers.1.norm1.weight', 'electra.encoder.layers.7.linear1.weight', 'electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.embeddings.layer_norm.weight', 'electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.encoder.layers.2.linear2.weight', 'electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.encoder.layers.3.linear1.bias', 'electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.encoder.layers.4.norm2.weight', 'electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.encoder.layers.5.norm1.bias', 'electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.encoder.layers.0.linear1.weight', 'electra.encoder.layers.3.norm2.weight', 'electra.encoder.layers.2.norm1.bias', 'electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.encoder.layers.0.linear2.weight', 'electra.encoder.layers.4.linear2.weight', 'electra.encoder.layers.11.norm2.weight', 'electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.encoder.layers.7.linear2.weight', 'electra.encoder.layers.9.linear1.bias', 'electra.encoder.layers.2.linear2.bias', 'electra.encoder.layers.11.norm2.bias', 'electra.encoder.layers.7.linear2.bias', 'electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.encoder.layers.10.norm1.bias', 'electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.encoder.layers.5.linear2.bias', 'electra.encoder.layers.11.linear1.weight', 'electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.encoder.layers.5.norm2.weight', 'electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.encoder.layers.9.norm2.weight', 'electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.encoder.layers.10.linear2.weight', 'electra.encoder.layers.4.norm1.bias', 'electra.encoder.layers.8.norm2.weight', 'electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.encoder.layers.1.linear2.bias', 'electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.encoder.layers.9.norm2.bias', 'electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.encoder.layers.3.norm1.bias', 'electra.encoder.layers.6.norm1.weight', 'electra.encoder.layers.6.norm1.bias', 'electra.encoder.layers.10.norm1.weight', 'electra.encoder.layers.6.linear2.bias', 'electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.embeddings.layer_norm.bias', 'electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.encoder.layers.4.norm2.bias', 'electra.encoder.layers.2.linear1.weight', 'electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.encoder.layers.10.linear1.weight', 'electra.encoder.layers.11.linear2.bias', 'electra.encoder.layers.8.linear2.weight', 'electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.encoder.layers.3.norm1.weight', 'electra.encoder.layers.5.linear2.weight', 'electra.encoder.layers.3.linear2.weight', 'electra.encoder.layers.6.linear1.bias', 'electra.encoder.layers.4.linear1.weight', 'electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.encoder.layers.8.linear2.bias', 'electra.encoder.layers.11.linear1.bias', 'electra.encoder.layers.7.linear1.bias', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.encoder.layers.0.linear2.bias', 'electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.encoder.layers.1.linear1.weight', 'electra.encoder.layers.2.norm1.weight', 'electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.encoder.layers.1.linear2.weight', 'electra.encoder.layers.7.norm1.bias', 'electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.encoder.layers.5.norm2.bias', 'electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.encoder.layers.0.norm1.bias', 'electra.encoder.layers.1.norm2.bias', 'electra.encoder.layers.6.linear1.weight', 'electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.encoder.layers.0.linear1.bias', 'electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.encoder.layers.10.norm2.bias', 'electra.encoder.layers.0.norm2.bias', 'electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.encoder.layers.2.norm2.weight', 'electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.encoder.layers.3.norm2.bias', 'electra.encoder.layers.0.norm2.weight', 'electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.encoder.layers.3.linear1.weight', 'electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.encoder.layers.6.norm2.weight', 'electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.encoder.layers.8.linear1.weight', 'electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.encoder.layers.8.norm2.bias', 'electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.encoder.layers.9.linear2.weight', 'electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.encoder.layers.3.linear2.bias', 'electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.encoder.layers.10.norm2.weight', 'electra.encoder.layers.9.linear2.bias', 'electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layers.0.norm1.weight', 'electra.encoder.layers.2.linear1.bias', 'electra.encoder.layers.5.norm1.weight', 'electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.encoder.layers.9.norm1.weight', 'electra.encoder.layers.4.linear2.bias', 'electra.encoder.layers.7.norm2.bias', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layers.9.linear1.weight', 'electra.encoder.layers.10.linear1.bias', 'electra.encoder.layers.7.norm1.weight', 'electra.encoder.layers.1.norm2.weight', 'electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.encoder.layers.6.norm2.bias']
- This IS expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-01-04 14:53:46,065] [ WARNING][0m - Some weights of ErnieHealthForTotalPretraining were not initialized from the model checkpoint at ernie-health-chinese and are newly initialized: ['electra.discriminator.electra.encoder.layers.5.norm1.weight', 'electra.discriminator.electra.encoder.layers.0.norm1.bias', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.0.norm2.weight', 'electra.discriminator.electra.encoder.layers.5.linear1.weight', 'electra.discriminator.electra.encoder.layers.11.linear2.weight', 'electra.generator.electra.encoder.layers.7.linear1.bias', 'electra.discriminator.electra.encoder.layers.10.norm1.bias', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.3.linear2.bias', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm1.bias', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.4.linear1.weight', 'electra.discriminator.electra.encoder.layers.8.norm2.bias', 'electra.discriminator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.0.norm1.weight', 'electra.generator.electra.encoder.layers.4.norm1.bias', 'electra.discriminator.electra.encoder.layers.10.norm2.weight', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.7.linear2.bias', 'electra.discriminator.electra.encoder.layers.2.norm2.weight', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.8.linear2.bias', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.norm1.bias', 'electra.generator.electra.encoder.layers.3.linear1.bias', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.10.norm1.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.2.norm2.weight', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.7.linear2.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.discriminator.discriminator_mts.weight', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.4.norm2.weight', 'electra.generator.electra.encoder.layers.11.linear1.bias', 'electra.discriminator.electra.encoder.layers.4.norm2.bias', 'electra.discriminator.electra.encoder.layers.9.linear1.weight', 'electra.discriminator.electra.encoder.layers.0.linear2.weight', 'electra.generator.electra.encoder.layers.0.norm2.bias', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.1.linear1.weight', 'electra.discriminator.electra.encoder.layers.9.norm1.bias', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.7.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.norm2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.9.linear1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.1.linear2.weight', 'electra.discriminator.electra.encoder.layers.1.linear2.bias', 'electra.discriminator.electra.encoder.layers.8.norm2.weight', 'electra.discriminator.electra.encoder.layers.6.norm1.weight', 'electra.discriminator.electra.encoder.layers.6.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.11.norm2.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.6.norm1.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.7.linear2.weight', 'electra.discriminator.electra.encoder.layers.8.norm1.bias', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.7.norm1.bias', 'electra.generator.electra.embeddings.position_embeddings.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.4.norm2.bias', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.norm2.bias', 'electra.discriminator.discriminator_rtd.dense.bias', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.3.linear1.weight', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.1.norm1.weight', 'electra.generator.electra.encoder.layers.8.norm1.bias', 'electra.generator.electra.encoder.layers.11.norm2.weight', 'electra.generator.electra.encoder.layers.3.linear1.weight', 'electra.discriminator.electra.embeddings.position_embeddings.weight', 'electra.generator.electra.encoder.layers.10.norm1.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.2.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.norm2.bias', 'electra.generator.electra.encoder.layers.9.linear2.weight', 'electra.discriminator.electra.encoder.layers.8.linear2.bias', 'electra.generator.electra.encoder.layers.11.norm1.weight', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.norm2.weight', 'electra.generator.electra.encoder.layers.10.norm2.weight', 'electra.generator.electra.encoder.layers.0.linear2.bias', 'electra.generator.electra.encoder.layers.1.norm2.weight', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.0.linear2.bias', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.1.linear2.weight', 'electra.discriminator.electra.encoder.layers.3.linear2.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.8.norm1.weight', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.discriminator.electra.embeddings.token_type_embeddings.weight', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.generator.generator_predictions.layer_norm.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.6.linear1.bias', 'electra.discriminator.electra.encoder.layers.9.norm2.weight', 'electra.generator.electra.encoder.layers.1.norm2.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.10.linear2.weight', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.2.norm2.bias', 'electra.discriminator.discriminator_mts.bias', 'electra.generator.electra.encoder.layers.9.linear1.bias', 'electra.generator.electra.encoder.layers.10.linear2.weight', 'electra.discriminator.electra.encoder.layers.3.linear1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.6.linear2.weight', 'electra.discriminator.electra.encoder.layers.6.linear2.weight', 'electra.discriminator.electra.encoder.layers.11.norm2.weight', 'electra.generator.electra.encoder.layers.5.norm2.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.2.norm1.weight', 'electra.discriminator.electra.encoder.layers.3.norm1.bias', 'electra.discriminator.electra.encoder.layers.11.linear2.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.linear2.bias', 'electra.generator.electra.encoder.layers.0.linear2.weight', 'electra.generator.electra.encoder.layers.2.linear1.bias', 'electra.generator.electra.encoder.layers.5.linear1.weight', 'electra.discriminator.electra.encoder.layers.6.norm1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.1.norm1.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.3.norm2.weight', 'electra.discriminator.discriminator_csp.out_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.3.norm1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.6.linear1.bias', 'electra.generator.electra.encoder.layers.6.norm2.bias', 'electra.discriminator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.0.norm2.weight', 'electra.generator.electra.encoder.layers.4.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.5.norm2.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.4.norm1.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.2.norm1.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.0.linear1.bias', 'electra.discriminator.electra.encoder.layers.7.linear1.weight', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.discriminator.electra.embeddings.layer_norm.weight', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.7.norm1.weight', 'electra.discriminator.electra.encoder.layers.6.norm2.weight', 'electra.discriminator.electra.encoder.layers.4.linear1.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.8.linear1.bias', 'electra.generator.electra.embeddings.layer_norm.weight', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.8.linear1.bias', 'electra.generator.electra.encoder.layers.3.norm1.weight', 'electra.discriminator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.5.norm2.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.generator.electra.embeddings.layer_norm.bias', 'electra.discriminator.electra.encoder.layers.8.linear2.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.0.norm1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.5.linear1.bias', 'electra.discriminator.electra.encoder.layers.11.linear1.weight', 'electra.discriminator.electra.encoder.layers.5.linear1.bias', 'electra.generator.electra.encoder.layers.11.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.8.linear2.weight', 'electra.discriminator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.discriminator.electra.embeddings.layer_norm.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.3.norm2.bias', 'electra.generator.electra.embeddings.token_type_embeddings.weight', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.4.linear2.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.norm1.weight', 'electra.generator.electra.encoder.layers.5.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.norm2.weight', 'electra.generator.electra.encoder.layers.1.linear1.weight', 'electra.generator.electra.encoder.layers.2.linear2.weight', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.11.norm1.weight', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.5.linear2.bias', 'electra.generator.electra.encoder.layers.5.norm1.bias', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm1.bias', 'electra.discriminator.electra.encoder.layers.9.linear2.weight', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.6.norm1.weight', 'electra.generator.electra.encoder.layers.2.linear2.bias', 'electra.discriminator.electra.encoder.layers.2.norm2.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.discriminator.discriminator_csp.dense.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.10.linear1.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.discriminator.discriminator_rtd.dense.weight', 'electra.generator.electra.encoder.layers.3.linear2.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.7.norm1.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.7.linear1.bias', 'electra.discriminator.discriminator_rtd.dense_prediction.weight', 'electra.discriminator.electra.encoder.layers.2.linear2.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.0.linear1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.11.linear1.bias', 'electra.generator.generator_lm_head_bias', 'electra.discriminator.electra.encoder.layers.5.linear2.bias', 'electra.discriminator.electra.encoder.layers.4.linear1.bias', 'electra.discriminator.electra.encoder.layers.4.linear2.bias', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.8.norm1.weight', 'electra.generator.generator_predictions.dense.weight', 'electra.generator.electra.encoder.layers.9.norm1.weight', 'electra.generator.electra.encoder.layers.4.linear1.bias', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.embeddings.word_embeddings.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.9.norm2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.11.linear2.weight', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.7.norm1.weight', 'electra.generator.electra.encoder.layers.8.linear1.weight', 'electra.generator.electra.encoder.layers.4.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.discriminator.discriminator_csp.out_proj.weight', 'electra.discriminator.bias_mts.weight', 'electra.generator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.encoder.layers.10.norm1.weight', 'electra.generator.electra.encoder.layers.6.norm2.weight', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.generator.generator_predictions.dense.bias', 'electra.generator.electra.encoder.layers.8.norm2.weight', 'electra.generator.electra.encoder.layers.7.linear2.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.0.norm1.weight', 'electra.discriminator.discriminator_csp.dense.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.10.norm2.bias', 'electra.generator.electra.encoder.layers.2.linear1.weight', 'electra.generator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.1.norm2.weight', 'electra.generator.electra.encoder.layers.11.linear1.weight', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.7.norm2.bias', 'electra.generator.electra.encoder.layers.2.norm1.bias', 'electra.generator.electra.encoder.layers.2.norm1.weight', 'electra.discriminator.electra.encoder.layers.2.linear1.bias', 'electra.discriminator.electra.encoder.layers.0.linear1.weight', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm2.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.generator.generator_predictions.layer_norm.bias', 'electra.discriminator.electra.encoder.layers.3.norm2.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.10.norm2.bias', 'electra.discriminator.electra.encoder.layers.4.norm2.weight', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.0.linear1.weight', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.9.norm1.bias', 'electra.generator.electra.encoder.layers.11.norm1.bias', 'electra.discriminator.electra.encoder.layers.10.linear1.bias', 'electra.generator.electra.encoder.layers.3.norm2.weight', 'electra.generator.electra.encoder.layers.8.norm2.bias', 'electra.generator.electra.encoder.layers.11.norm2.bias', 'electra.discriminator.electra.encoder.layers.2.linear1.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.10.linear2.bias', 'electra.discriminator.electra.embeddings.word_embeddings.weight', 'electra.generator.electra.encoder.layers.7.norm2.weight', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.9.linear1.weight', 'electra.generator.electra.encoder.layers.1.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.3.linear2.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.8.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.discriminator.discriminator_rtd.dense_prediction.bias', 'electra.generator.electra.encoder.layers.1.norm1.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-01-04 14:53:46,065] [    INFO][0m - start load data : 2023-01-04 14:53:46[0m
[32m[2023-01-04 14:53:46,309] [    INFO][0m - load data done, total : 0.24387121200561523 s[0m
[32m[2023-01-04 14:53:46,400] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2023-01-04 14:53:46,401] [    INFO][0m - Using half precision[0m
[32m[2023-01-04 14:53:46,403] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:53:46,403] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-01-04 14:53:46,403] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - bf16                          :False[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - bf16_full_eval                :False[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - current_device                :gpu:7[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - dataloader_num_workers        :2[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - device                        :gpu[0m
[32m[2023-01-04 14:53:46,404] [    INFO][0m - disable_tqdm                  :False[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - do_eval                       :False[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - do_export                     :False[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - do_predict                    :False[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - do_train                      :True[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - eval_batch_size               :8[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - eval_iters                    :10[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - eval_steps                    :None[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - evaluation_strategy           :IntervalStrategy.NO[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - fp16                          :True[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - fp16_full_eval                :False[0m
[32m[2023-01-04 14:53:46,405] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - greater_is_better             :None[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - label_names                   :None[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - learning_rate                 :0.001[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - load_best_model_at_end        :False[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - local_process_index           :2[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - local_rank                    :2[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - log_level                     :-1[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - log_level_replica             :-1[0m
[32m[2023-01-04 14:53:46,406] [    INFO][0m - log_on_each_node              :True[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - logging_dir                   :output/eheath-pretraining/runs/Jan04_14-53-37_yq01-qianmo-com-255-129-12.yq01[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - logging_first_step            :False[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - logging_steps                 :20[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - max_steps                     :100[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - metric_for_best_model         :None[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - no_cuda                       :False[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - num_train_epochs              :3.0[0m
[32m[2023-01-04 14:53:46,407] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - output_dir                    :output/eheath-pretraining[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - past_index                    :-1[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - process_index                 :2[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - recompute                     :True[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2023-01-04 14:53:46,408] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - run_name                      :output/eheath-pretraining[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - save_on_each_node             :False[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - save_steps                    :25[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - save_total_limit              :10[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - scale_loss                    :32768[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - seed                          :42[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - sharding                      :[][0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - sharding_degree               :-1[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - should_log                    :False[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - should_save                   :False[0m
[32m[2023-01-04 14:53:46,409] [    INFO][0m - skip_memory_metrics           :True[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - test_iters                    :100[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - train_batch_size              :8[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - warmup_ratio                  :0.01[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - warmup_steps                  :0[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - weight_decay                  :0.01[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - world_size                    :3[0m
[32m[2023-01-04 14:53:46,410] [    INFO][0m - [0m
[32m[2023-01-04 14:53:46,461] [    INFO][0m - ***** Running training *****[0m
[32m[2023-01-04 14:53:46,461] [    INFO][0m -   Num examples = 200061[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Total optimization steps = 100[0m
[32m[2023-01-04 14:53:46,462] [    INFO][0m -   Total num train samples = 2400[0m
[32m[2023-01-04 14:53:46,491] [    INFO][0m -   Number of trainable parameters = 190772769[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5
Found inf or nan, current scale is: 2048.0, decrease to: 2048.0*0.5
[32m[2023-01-04 14:55:25,753] [    INFO][0m - 
Training completed. 
[0m
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 14:59:19,438] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
[32m[2023-01-04 14:59:19,446] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 14:59:19,446] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:59:19,446] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 14:59:19,446] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:59:19,446] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - [0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 14:59:19,447] [    INFO][0m - [0m
I0104 14:59:19.448168 32051 tcp_utils.cc:130] Successfully connected to 10.255.129.12:43418
W0104 14:59:22.406885 32051 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 14:59:22.414822 32051 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 14:59:22,839] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 14:59:22,841] [    INFO][0m - Checkpoint detected, resuming training at output/eheath-pretraining/checkpoint-100. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2023-01-04 14:59:22,841] [    INFO][0m - Already cached /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 14:59:22,876] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 14:59:22,877] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 14:59:22,879] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[33m[2023-01-04 14:59:23,285] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:59:23,360] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:59:23,847] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 14:59:23,940] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:59:24,242] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 14:59:25,185] [ WARNING][0m - Some weights of the model checkpoint at ernie-health-chinese were not used when initializing ErnieHealthForTotalPretraining: ['electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.encoder.layers.4.norm2.weight', 'electra.encoder.layers.3.norm2.bias', 'electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.encoder.layers.9.linear1.weight', 'electra.encoder.layers.0.norm2.bias', 'electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.embeddings.layer_norm.bias', 'electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.encoder.layers.3.norm1.bias', 'electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.encoder.layers.5.norm2.bias', 'electra.encoder.layers.5.linear2.bias', 'electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.encoder.layers.8.linear2.bias', 'electra.encoder.layers.6.norm2.weight', 'electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.encoder.layers.5.norm1.bias', 'electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.encoder.layers.10.linear2.weight', 'electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.encoder.layers.0.linear2.weight', 'electra.encoder.layers.8.linear1.weight', 'electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.encoder.layers.7.norm1.bias', 'electra.encoder.layers.3.linear2.weight', 'electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.encoder.layers.0.linear1.bias', 'electra.embeddings.layer_norm.weight', 'electra.encoder.layers.2.norm1.weight', 'electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.encoder.layers.5.norm1.weight', 'electra.encoder.layers.8.norm2.weight', 'electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.encoder.layers.9.norm2.weight', 'electra.encoder.layers.9.norm1.weight', 'electra.encoder.layers.2.norm2.weight', 'electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.encoder.layers.7.linear2.bias', 'electra.encoder.layers.10.norm1.bias', 'electra.encoder.layers.2.norm1.bias', 'electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.encoder.layers.2.linear2.bias', 'electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.encoder.layers.9.norm1.bias', 'electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.encoder.layers.4.norm1.weight', 'electra.encoder.layers.0.norm2.weight', 'electra.encoder.layers.1.norm1.bias', 'electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.encoder.layers.9.linear1.bias', 'electra.encoder.layers.11.linear1.bias', 'electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.encoder.layers.6.norm1.weight', 'electra.encoder.layers.7.linear1.bias', 'electra.encoder.layers.2.linear1.bias', 'electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.encoder.layers.7.norm2.bias', 'electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.encoder.layers.0.linear1.weight', 'electra.embeddings.position_embeddings.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.encoder.layers.10.norm2.bias', 'electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.encoder.layers.6.linear2.bias', 'electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.encoder.layers.1.linear1.bias', 'electra.encoder.layers.11.norm1.weight', 'electra.encoder.layers.3.linear2.bias', 'electra.encoder.layers.5.linear2.weight', 'electra.encoder.layers.4.norm2.bias', 'electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.encoder.layers.9.linear2.bias', 'electra.encoder.layers.4.norm1.bias', 'electra.encoder.layers.7.norm1.weight', 'electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.encoder.layers.10.norm2.weight', 'electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.encoder.layers.6.linear1.weight', 'electra.encoder.layers.8.norm2.bias', 'electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.encoder.layers.3.norm1.weight', 'electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.encoder.layers.2.norm2.bias', 'electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.encoder.layers.6.norm2.bias', 'electra.encoder.layers.8.linear2.weight', 'electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.encoder.layers.0.norm1.bias', 'electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.encoder.layers.11.norm2.bias', 'electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.encoder.layers.1.norm1.weight', 'electra.encoder.layers.10.linear1.bias', 'electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.encoder.layers.11.norm2.weight', 'electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.encoder.layers.11.linear2.bias', 'electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.encoder.layers.4.linear2.bias', 'electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.encoder.layers.10.norm1.weight', 'electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.encoder.layers.1.norm2.weight', 'electra.encoder.layers.5.linear1.bias', 'electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.encoder.layers.1.linear2.bias', 'electra.encoder.layers.2.linear1.weight', 'electra.encoder.layers.5.linear1.weight', 'electra.encoder.layers.11.norm1.bias', 'electra.encoder.layers.8.norm1.weight', 'electra.encoder.layers.4.linear2.weight', 'electra.encoder.layers.6.norm1.bias', 'electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.encoder.layers.1.norm2.bias', 'electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.encoder.layers.7.norm2.weight', 'electra.encoder.layers.5.norm2.weight', 'electra.encoder.layers.1.linear2.weight', 'electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.encoder.layers.6.linear1.bias', 'electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.encoder.layers.3.norm2.weight', 'electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.encoder.layers.1.linear1.weight', 'electra.encoder.layers.9.norm2.bias', 'electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.encoder.layers.10.linear2.bias', 'electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.encoder.layers.7.linear1.weight', 'electra.encoder.layers.10.linear1.weight', 'electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.encoder.layers.8.norm1.bias', 'electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.encoder.layers.3.linear1.weight', 'electra.encoder.layers.11.linear2.weight', 'electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.encoder.layers.6.linear2.weight', 'electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.encoder.layers.3.linear1.bias', 'electra.encoder.layers.0.linear2.bias', 'electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.encoder.layers.8.linear1.bias', 'electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.encoder.layers.11.linear1.weight', 'electra.encoder.layers.7.linear2.weight', 'electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.encoder.layers.4.linear1.bias', 'electra.encoder.layers.0.norm1.weight', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layers.2.linear2.weight', 'electra.encoder.layers.4.linear1.weight', 'electra.encoder.layers.9.linear2.weight', 'electra.encoder.layers.0.self_attn.k_proj.bias']
- This IS expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-01-04 14:59:25,185] [ WARNING][0m - Some weights of ErnieHealthForTotalPretraining were not initialized from the model checkpoint at ernie-health-chinese and are newly initialized: ['electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.10.linear2.weight', 'electra.discriminator.electra.encoder.layers.1.linear1.weight', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.2.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.7.norm2.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.generator.electra.embeddings.token_type_embeddings.weight', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.5.linear1.weight', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.norm2.bias', 'electra.generator.electra.encoder.layers.11.norm1.bias', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.9.norm2.bias', 'electra.generator.electra.encoder.layers.4.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.linear1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.2.linear1.bias', 'electra.discriminator.electra.encoder.layers.5.norm1.weight', 'electra.discriminator.electra.encoder.layers.4.linear2.weight', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.generator.generator_predictions.dense.weight', 'electra.generator.electra.encoder.layers.1.linear2.weight', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.11.norm1.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.2.linear2.weight', 'electra.generator.electra.encoder.layers.5.linear1.weight', 'electra.discriminator.electra.encoder.layers.0.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.5.linear1.bias', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.discriminator.discriminator_rtd.dense_prediction.bias', 'electra.generator.electra.encoder.layers.9.linear1.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.0.norm2.weight', 'electra.discriminator.electra.encoder.layers.10.norm1.bias', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.generator.electra.embeddings.layer_norm.bias', 'electra.discriminator.electra.encoder.layers.5.norm2.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.3.linear2.weight', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm2.weight', 'electra.generator.electra.encoder.layers.1.norm2.weight', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.7.linear1.bias', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.7.linear1.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.9.norm1.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.discriminator.electra.embeddings.token_type_embeddings.weight', 'electra.discriminator.electra.encoder.layers.9.linear1.bias', 'electra.generator.electra.encoder.layers.9.norm1.bias', 'electra.discriminator.electra.encoder.layers.10.linear2.bias', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.2.norm1.bias', 'electra.generator.electra.encoder.layers.11.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm2.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.generator.generator_predictions.layer_norm.bias', 'electra.generator.electra.encoder.layers.9.linear1.weight', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.0.linear2.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.10.norm2.bias', 'electra.generator.electra.encoder.layers.5.norm1.bias', 'electra.generator.electra.encoder.layers.10.norm2.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.9.norm2.weight', 'electra.generator.electra.encoder.layers.4.norm2.bias', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.3.norm2.weight', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.7.linear2.weight', 'electra.discriminator.electra.encoder.layers.6.norm2.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.11.linear1.weight', 'electra.generator.electra.encoder.layers.2.norm2.weight', 'electra.discriminator.electra.encoder.layers.4.norm2.weight', 'electra.generator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.6.norm2.weight', 'electra.generator.electra.encoder.layers.6.norm1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.norm1.weight', 'electra.generator.electra.encoder.layers.1.linear2.bias', 'electra.generator.electra.encoder.layers.1.linear1.weight', 'electra.discriminator.electra.encoder.layers.8.linear1.bias', 'electra.generator.electra.encoder.layers.0.linear1.weight', 'electra.generator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.4.linear1.bias', 'electra.discriminator.electra.encoder.layers.8.norm1.weight', 'electra.generator.electra.encoder.layers.9.norm2.bias', 'electra.generator.electra.encoder.layers.2.linear2.bias', 'electra.discriminator.electra.encoder.layers.11.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.norm2.bias', 'electra.discriminator.electra.encoder.layers.8.linear1.weight', 'electra.discriminator.discriminator_csp.out_proj.bias', 'electra.discriminator.electra.encoder.layers.7.norm1.bias', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.4.norm1.bias', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.0.norm2.bias', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.8.linear2.weight', 'electra.generator.electra.encoder.layers.11.linear2.bias', 'electra.discriminator.electra.encoder.layers.8.linear2.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.7.norm2.weight', 'electra.discriminator.discriminator_mts.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.3.norm2.weight', 'electra.discriminator.discriminator_rtd.dense_prediction.weight', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.5.norm1.weight', 'electra.generator.electra.encoder.layers.7.norm1.bias', 'electra.generator.electra.encoder.layers.10.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.10.norm2.weight', 'electra.generator.electra.encoder.layers.3.norm1.weight', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm2.bias', 'electra.generator.electra.encoder.layers.8.linear1.bias', 'electra.generator.electra.encoder.layers.8.norm2.bias', 'electra.generator.generator_predictions.dense.bias', 'electra.discriminator.electra.encoder.layers.0.norm2.bias', 'electra.discriminator.electra.encoder.layers.9.linear2.weight', 'electra.generator.electra.encoder.layers.0.norm1.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.4.linear2.weight', 'electra.discriminator.electra.encoder.layers.7.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.linear2.weight', 'electra.generator.electra.encoder.layers.0.linear1.bias', 'electra.discriminator.electra.encoder.layers.2.norm2.weight', 'electra.generator.electra.encoder.layers.7.linear1.weight', 'electra.discriminator.electra.encoder.layers.3.norm1.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.discriminator.discriminator_mts.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.8.norm1.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.discriminator.discriminator_csp.dense.bias', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.1.norm2.bias', 'electra.generator.electra.encoder.layers.11.norm2.weight', 'electra.generator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.linear1.weight', 'electra.generator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.generator.generator_predictions.layer_norm.weight', 'electra.generator.electra.encoder.layers.9.linear2.weight', 'electra.generator.electra.encoder.layers.8.linear2.bias', 'electra.discriminator.electra.encoder.layers.2.linear2.bias', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.4.linear2.bias', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.4.linear2.bias', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.6.norm2.bias', 'electra.discriminator.electra.encoder.layers.1.norm1.bias', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.0.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.linear1.bias', 'electra.generator.electra.encoder.layers.2.norm1.weight', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.8.norm2.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.3.norm1.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.6.linear2.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.7.linear2.bias', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.3.norm2.bias', 'electra.generator.electra.encoder.layers.11.norm1.weight', 'electra.generator.electra.encoder.layers.5.norm2.bias', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.1.linear2.bias', 'electra.generator.electra.embeddings.position_embeddings.weight', 'electra.discriminator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.10.linear2.weight', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.8.linear2.bias', 'electra.discriminator.electra.encoder.layers.11.norm2.weight', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.discriminator.electra.embeddings.word_embeddings.weight', 'electra.generator.electra.encoder.layers.3.norm1.bias', 'electra.generator.electra.encoder.layers.8.norm2.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.6.norm1.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.2.linear1.bias', 'electra.discriminator.discriminator_rtd.dense.bias', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.6.norm1.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.generator.electra.embeddings.word_embeddings.weight', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.7.norm2.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.3.linear2.weight', 'electra.discriminator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.10.norm1.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.0.linear2.bias', 'electra.generator.electra.encoder.layers.11.linear1.bias', 'electra.generator.electra.encoder.layers.2.linear1.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.1.norm1.bias', 'electra.generator.electra.encoder.layers.10.norm2.bias', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.10.norm1.weight', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.4.norm2.bias', 'electra.generator.electra.encoder.layers.10.linear1.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.generator.generator_lm_head_bias', 'electra.discriminator.discriminator_rtd.dense.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.0.linear2.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.9.norm1.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.7.linear2.weight', 'electra.generator.electra.encoder.layers.0.norm2.weight', 'electra.discriminator.electra.encoder.layers.10.linear1.bias', 'electra.discriminator.electra.encoder.layers.0.norm1.weight', 'electra.generator.electra.encoder.layers.3.linear1.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.discriminator.electra.embeddings.position_embeddings.weight', 'electra.generator.electra.encoder.layers.5.norm2.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.9.linear1.weight', 'electra.discriminator.electra.embeddings.layer_norm.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.1.linear2.weight', 'electra.discriminator.electra.encoder.layers.4.linear1.bias', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.11.linear2.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.8.norm2.weight', 'electra.generator.electra.encoder.layers.3.linear1.bias', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.2.norm1.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.11.norm2.bias', 'electra.generator.electra.encoder.layers.5.linear1.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.9.norm2.weight', 'electra.generator.electra.encoder.layers.6.linear1.bias', 'electra.discriminator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.2.norm2.bias', 'electra.generator.electra.encoder.layers.4.norm1.bias', 'electra.generator.electra.encoder.layers.8.linear1.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.8.norm1.bias', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.10.norm1.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.discriminator.discriminator_csp.out_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.11.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.6.norm1.bias', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.7.norm1.weight', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.6.linear1.bias', 'electra.discriminator.electra.encoder.layers.2.linear1.weight', 'electra.discriminator.electra.encoder.layers.2.linear2.weight', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.3.linear2.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.6.norm2.weight', 'electra.discriminator.electra.encoder.layers.9.norm1.bias', 'electra.discriminator.electra.encoder.layers.11.linear2.weight', 'electra.discriminator.electra.encoder.layers.2.norm1.bias', 'electra.discriminator.bias_mts.weight', 'electra.generator.electra.encoder.layers.3.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.linear1.bias', 'electra.generator.electra.encoder.layers.11.linear1.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.7.linear1.weight', 'electra.generator.electra.encoder.layers.0.norm1.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.linear1.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.0.norm1.bias', 'electra.discriminator.discriminator_csp.dense.weight', 'electra.generator.electra.encoder.layers.4.norm2.weight', 'electra.discriminator.electra.encoder.layers.5.linear2.bias', 'electra.generator.electra.embeddings.layer_norm.weight', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.5.linear2.bias', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.1.norm1.weight', 'electra.discriminator.electra.embeddings.layer_norm.weight', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.8.norm1.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-01-04 14:59:25,186] [    INFO][0m - start load data : 2023-01-04 14:59:25[0m
[32m[2023-01-04 14:59:25,431] [    INFO][0m - load data done, total : 0.2456958293914795 s[0m
[32m[2023-01-04 14:59:25,544] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2023-01-04 14:59:25,544] [    INFO][0m - Using half precision[0m
[32m[2023-01-04 14:59:25,547] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 14:59:25,547] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-01-04 14:59:25,547] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - bf16                          :False[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - bf16_full_eval                :False[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - current_device                :gpu:7[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2023-01-04 14:59:25,548] [    INFO][0m - dataloader_num_workers        :2[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - device                        :gpu[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - disable_tqdm                  :False[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - do_eval                       :False[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - do_export                     :False[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - do_predict                    :False[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - do_train                      :True[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - eval_batch_size               :8[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - eval_iters                    :10[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - eval_steps                    :None[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - evaluation_strategy           :IntervalStrategy.NO[0m
[32m[2023-01-04 14:59:25,549] [    INFO][0m - fp16                          :True[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - fp16_full_eval                :False[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - greater_is_better             :None[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - label_names                   :None[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - learning_rate                 :0.001[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - load_best_model_at_end        :False[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - local_process_index           :2[0m
[32m[2023-01-04 14:59:25,550] [    INFO][0m - local_rank                    :2[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - log_level                     :-1[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - log_level_replica             :-1[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - log_on_each_node              :True[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - logging_dir                   :output/eheath-pretraining/runs/Jan04_14-59-19_yq01-qianmo-com-255-129-12.yq01[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - logging_first_step            :False[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - logging_steps                 :20[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - max_steps                     :100[0m
[32m[2023-01-04 14:59:25,551] [    INFO][0m - metric_for_best_model         :None[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - no_cuda                       :False[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - num_train_epochs              :3.0[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - output_dir                    :output/eheath-pretraining[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - past_index                    :-1[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2023-01-04 14:59:25,552] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - process_index                 :2[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - recompute                     :True[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - run_name                      :output/eheath-pretraining[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - save_on_each_node             :False[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - save_steps                    :25[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2023-01-04 14:59:25,553] [    INFO][0m - save_total_limit              :10[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - scale_loss                    :32768[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - seed                          :42[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - sharding                      :[][0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - sharding_degree               :-1[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - should_log                    :False[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - should_save                   :False[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - skip_memory_metrics           :True[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - test_iters                    :100[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - train_batch_size              :8[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - warmup_ratio                  :0.01[0m
[32m[2023-01-04 14:59:25,554] [    INFO][0m - warmup_steps                  :0[0m
[32m[2023-01-04 14:59:25,555] [    INFO][0m - weight_decay                  :0.01[0m
[32m[2023-01-04 14:59:25,555] [    INFO][0m - world_size                    :3[0m
[32m[2023-01-04 14:59:25,555] [    INFO][0m - [0m
[32m[2023-01-04 14:59:25,555] [    INFO][0m - Loading model from output/eheath-pretraining/checkpoint-100 .[0m
[32m[2023-01-04 14:59:28,298] [    INFO][0m - ***** Running training *****[0m
[32m[2023-01-04 14:59:28,298] [    INFO][0m -   Num examples = 200061[0m
[32m[2023-01-04 14:59:28,298] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-01-04 14:59:28,298] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-01-04 14:59:28,299] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-01-04 14:59:28,299] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-01-04 14:59:28,299] [    INFO][0m -   Total optimization steps = 100[0m
[32m[2023-01-04 14:59:28,299] [    INFO][0m -   Total num train samples = 2400[0m
[32m[2023-01-04 14:59:28,313] [    INFO][0m -   Number of trainable parameters = 190772769[0m
[32m[2023-01-04 14:59:28,314] [    INFO][0m -   Continuing training from checkpoint, will skip to saved global_step[0m
[32m[2023-01-04 14:59:28,314] [    INFO][0m -   Continuing training from epoch 0[0m
[32m[2023-01-04 14:59:28,314] [    INFO][0m -   Continuing training from global step 100[0m
[32m[2023-01-04 14:59:28,314] [    INFO][0m -   Will skip the first 0 epochs then the first 100 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.[0m
[32m[2023-01-04 14:59:29,224] [    INFO][0m - Didn't find an RNG file for process 2, if you are resuming a training that wasn't launched in a distributed fashion, reproducibility is not guaranteed.[0m
[32m[2023-01-04 14:59:33,690] [    INFO][0m - 
Training completed. 
[0m
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 15:00:13,892] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
[32m[2023-01-04 15:00:13,910] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 15:00:13,911] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 15:00:13,911] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 15:00:13,911] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 15:00:13,911] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 15:00:13,912] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 15:00:13,912] [    INFO][0m - [0m
[32m[2023-01-04 15:00:13,912] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 15:00:13,912] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 15:00:13,912] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 15:00:13,913] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 15:00:13,913] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 15:00:13,913] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 15:00:13,913] [    INFO][0m - [0m
I0104 15:00:13.914644   334 tcp_utils.cc:130] Successfully connected to 10.255.129.12:37981
W0104 15:00:16.574127   334 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 15:00:16.578701   334 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 15:00:17,281] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 15:00:17,283] [    INFO][0m - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie-health-chinese/vocab.txt and saved to /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese[0m
[32m[2023-01-04 15:00:17,569] [    INFO][0m - Found /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 15:00:17,608] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 15:00:17,608] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 15:00:17,611] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[33m[2023-01-04 15:00:18,016] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 15:00:18,089] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 15:00:18,561] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 15:00:18,637] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 15:00:18,886] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[32m[2023-01-04 15:00:30,955] [    INFO][0m - Found /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams[0m
Traceback (most recent call last):
  File "model_zoo/ernie-health/test.py", line 26, in <module>
    test_pretrain()
  File "model_zoo/ernie-health/test.py", line 23, in test_pretrain
    main()
  File "/ssd1/zhangbin41/PaddleNLP/model_zoo/ernie-health/run_pretrain_trainer.py", line 126, in main
    model = model_class.from_pretrained(model_args.model_name_or_path)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 440, in from_pretrained
    return cls.from_pretrained_v2(pretrained_model_name_or_path, from_hf_hub=from_hf_hub, *args, **kwargs)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1264, in from_pretrained_v2
    model_weight_file = cls._resolve_model_file_path(
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/transformers/model_utils.py", line 971, in _resolve_model_file_path
    weight_file_path = get_path_from_url_with_filelock(pretrained_model_name_or_path, cache_dir)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 192, in get_path_from_url_with_filelock
    result = get_path_from_url(url=url, root_dir=root_dir, md5sum=md5sum, check_exist=check_exist)
  File "/ssd1/zhangbin41/PaddleNLP/paddlenlp/utils/downloader.py", line 159, in get_path_from_url
    if tarfile.is_tarfile(fullpath) or zipfile.is_zipfile(fullpath):
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 2466, in is_tarfile
    t = open(name)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1599, in open
    return func(name, "r", fileobj, **kwargs)
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/tarfile.py", line 1696, in bz2open
    fileobj = bz2.BZ2File(fileobj or name, mode,
  File "/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/bz2.py", line 96, in __init__
    self._fp = _builtin_open(filename, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/ernie-health-chinese.pdparams'
/ssd1/zhangbin41/miniconda3/envs/paddle_env/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-01-04 15:02:03,476] [    INFO][0m - loading configuration file<./configs/test.yaml>[0m
[32m[2023-01-04 15:02:03,484] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2023-01-04 15:02:03,484] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 15:02:03,484] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - model_name_or_path            :ernie-health-chinese[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - model_type                    :ernie-health[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - [0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - input_dir                     :./data[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - masked_lm_prob                :0.15[0m
[32m[2023-01-04 15:02:03,485] [    INFO][0m - max_seq_length                :512[0m
[32m[2023-01-04 15:02:03,486] [    INFO][0m - [0m
I0104 15:02:03.486390 19204 tcp_utils.cc:107] Retry to connect to 10.255.129.12:50016 while the server is not yet listening.
I0104 15:02:06.486608 19204 tcp_utils.cc:130] Successfully connected to 10.255.129.12:50016
W0104 15:02:09.591673 19204 gpu_resources.cc:61] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W0104 15:02:09.600560 19204 gpu_resources.cc:91] device: 7, cuDNN Version: 7.6.
[33m[2023-01-04 15:02:10,022] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 3, distributed training: True, 16-bits training: True[0m
[32m[2023-01-04 15:02:10,024] [    INFO][0m - Already cached /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/vocab.txt[0m
[32m[2023-01-04 15:02:10,058] [    INFO][0m - tokenizer config file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/tokenizer_config.json[0m
[32m[2023-01-04 15:02:10,059] [    INFO][0m - Special tokens file saved in /ssd1/zhangbin41/.paddlenlp/models/ernie-health-chinese/special_tokens_map.json[0m
[32m[2023-01-04 15:02:10,061] [    INFO][0m - Model config ElectraConfig {
  "attention_probs_dropout_prob": 0.1,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_choices": 2,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "type_vocab_size": 2,
  "vocab_size": 22608
}
[0m
[33m[2023-01-04 15:02:10,532] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 15:02:10,605] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 15:02:11,135] [ WARNING][0m - Accessing `initializer_range` through `model.initializer_range` will be deprecated after v2.6.0. Instead, do `model.config.initializer_range`[0m
[33m[2023-01-04 15:02:11,206] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 15:02:11,438] [ WARNING][0m - Accessing `layer_norm_eps` through `model.layer_norm_eps` will be deprecated after v2.6.0. Instead, do `model.config.layer_norm_eps`[0m
[33m[2023-01-04 15:02:12,297] [ WARNING][0m - Some weights of the model checkpoint at ernie-health-chinese were not used when initializing ErnieHealthForTotalPretraining: ['electra.encoder.layers.9.norm1.weight', 'electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.encoder.layers.8.norm1.weight', 'electra.encoder.layers.10.norm2.weight', 'electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.encoder.layers.8.linear2.bias', 'electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.encoder.layers.8.linear1.bias', 'electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.encoder.layers.8.linear2.weight', 'electra.encoder.layers.6.linear1.bias', 'electra.encoder.layers.0.linear2.weight', 'electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.encoder.layers.10.linear2.weight', 'electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.encoder.layers.0.norm2.weight', 'electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.encoder.layers.3.linear1.weight', 'electra.encoder.layers.4.norm2.bias', 'electra.encoder.layers.7.linear1.weight', 'electra.encoder.layers.10.norm1.bias', 'electra.encoder.layers.0.norm2.bias', 'electra.encoder.layers.8.linear1.weight', 'electra.encoder.layers.7.linear2.bias', 'electra.encoder.layers.5.norm2.bias', 'electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.encoder.layers.9.linear1.bias', 'electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.encoder.layers.5.linear2.bias', 'electra.encoder.layers.9.linear2.weight', 'electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.encoder.layers.11.norm1.weight', 'electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.encoder.layers.1.linear2.weight', 'electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.encoder.layers.10.linear1.bias', 'electra.encoder.layers.4.linear1.bias', 'electra.encoder.layers.3.linear1.bias', 'electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.encoder.layers.2.linear2.bias', 'electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.encoder.layers.5.norm1.weight', 'electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.encoder.layers.9.linear1.weight', 'electra.encoder.layers.0.norm1.weight', 'electra.encoder.layers.3.norm1.weight', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.encoder.layers.5.linear1.bias', 'electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.encoder.layers.5.norm2.weight', 'electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.encoder.layers.10.linear1.weight', 'electra.encoder.layers.4.norm2.weight', 'electra.encoder.layers.2.norm2.bias', 'electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.encoder.layers.4.norm1.weight', 'electra.encoder.layers.7.linear2.weight', 'electra.encoder.layers.2.norm2.weight', 'electra.encoder.layers.3.norm2.bias', 'electra.encoder.layers.6.norm1.weight', 'electra.encoder.layers.3.linear2.weight', 'electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.encoder.layers.5.norm1.bias', 'electra.encoder.layers.10.norm1.weight', 'electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.encoder.layers.3.norm1.bias', 'electra.encoder.layers.6.norm2.weight', 'electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.encoder.layers.1.norm1.bias', 'electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.encoder.layers.1.linear2.bias', 'electra.encoder.layers.8.norm1.bias', 'electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.encoder.layers.5.linear1.weight', 'electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.encoder.layers.11.norm2.bias', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.encoder.layers.1.norm2.weight', 'electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.encoder.layers.7.linear1.bias', 'electra.encoder.layers.8.norm2.bias', 'electra.encoder.layers.6.linear2.bias', 'electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.encoder.layers.6.norm1.bias', 'electra.encoder.layers.0.linear2.bias', 'electra.encoder.layers.1.linear1.weight', 'electra.encoder.layers.2.linear2.weight', 'electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.encoder.layers.6.linear2.weight', 'electra.encoder.layers.7.norm2.bias', 'electra.encoder.layers.11.norm1.bias', 'electra.encoder.layers.4.linear2.bias', 'electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.encoder.layers.6.linear1.weight', 'electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.encoder.layers.2.norm1.weight', 'electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.encoder.layers.3.norm2.weight', 'electra.encoder.layers.0.norm1.bias', 'electra.encoder.layers.9.norm2.bias', 'electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.encoder.layers.11.linear1.bias', 'electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.encoder.layers.0.linear1.bias', 'electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.encoder.layers.10.linear2.bias', 'electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.encoder.layers.1.linear1.bias', 'electra.encoder.layers.6.norm2.bias', 'electra.encoder.layers.1.norm2.bias', 'electra.encoder.layers.9.norm2.weight', 'electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.encoder.layers.9.linear2.bias', 'electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.encoder.layers.2.norm1.bias', 'electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.embeddings.layer_norm.bias', 'electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.encoder.layers.4.norm1.bias', 'electra.encoder.layers.1.norm1.weight', 'electra.encoder.layers.5.linear2.weight', 'electra.encoder.layers.2.linear1.bias', 'electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.encoder.layers.7.norm1.bias', 'electra.encoder.layers.10.norm2.bias', 'electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.encoder.layers.11.linear2.bias', 'electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layers.8.norm2.weight', 'electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.encoder.layers.11.norm2.weight', 'electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.encoder.layers.4.linear1.weight', 'electra.encoder.layers.2.linear1.weight', 'electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.encoder.layers.7.norm2.weight', 'electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.encoder.layers.7.norm1.weight', 'electra.encoder.layers.3.linear2.bias', 'electra.encoder.layers.11.linear2.weight', 'electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.embeddings.layer_norm.weight', 'electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.encoder.layers.9.norm1.bias', 'electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.encoder.layers.11.linear1.weight', 'electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.encoder.layers.4.linear2.weight', 'electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.encoder.layers.0.linear1.weight']
- This IS expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieHealthForTotalPretraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-01-04 15:02:12,298] [ WARNING][0m - Some weights of ErnieHealthForTotalPretraining were not initialized from the model checkpoint at ernie-health-chinese and are newly initialized: ['electra.discriminator.electra.encoder.layers.1.linear2.weight', 'electra.generator.electra.encoder.layers.7.linear2.bias', 'electra.discriminator.electra.embeddings.token_type_embeddings.weight', 'electra.discriminator.electra.encoder.layers.7.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.encoder.layers.10.linear1.bias', 'electra.discriminator.electra.encoder.layers.3.norm1.bias', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.9.linear1.bias', 'electra.generator.electra.encoder.layers.6.norm1.bias', 'electra.generator.electra.encoder.layers.0.linear1.weight', 'electra.discriminator.electra.encoder.layers.2.linear1.bias', 'electra.discriminator.electra.encoder.layers.0.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.1.linear1.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.11.norm1.weight', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.2.norm2.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.2.norm1.weight', 'electra.discriminator.electra.encoder.layers.11.norm1.bias', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.7.norm2.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.8.norm1.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.11.linear2.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.0.linear2.bias', 'electra.discriminator.electra.encoder.layers.4.linear2.bias', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.4.linear1.weight', 'electra.discriminator.electra.encoder.layers.11.linear1.weight', 'electra.generator.electra.encoder.layers.10.norm1.weight', 'electra.discriminator.electra.encoder.layers.11.linear2.weight', 'electra.discriminator.electra.encoder.layers.5.norm2.bias', 'electra.generator.electra.encoder.layers.1.linear2.weight', 'electra.generator.electra.encoder.layers.8.linear2.bias', 'electra.discriminator.electra.encoder.layers.9.linear1.weight', 'electra.discriminator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.10.linear2.weight', 'electra.discriminator.electra.encoder.layers.5.linear2.bias', 'electra.discriminator.electra.encoder.layers.10.norm2.weight', 'electra.discriminator.electra.encoder.layers.2.norm2.weight', 'electra.generator.electra.encoder.layers.4.linear2.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.7.linear2.weight', 'electra.discriminator.electra.embeddings.layer_norm.weight', 'electra.discriminator.electra.encoder.layers.8.linear2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.10.norm2.bias', 'electra.discriminator.electra.embeddings.layer_norm.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.9.norm1.bias', 'electra.generator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.1.norm2.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.linear1.weight', 'electra.generator.electra.encoder.layers.0.norm2.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.0.linear2.weight', 'electra.generator.electra.encoder.layers.5.norm1.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.6.linear1.bias', 'electra.generator.electra.encoder.layers.5.norm1.weight', 'electra.discriminator.electra.encoder.layers.11.norm2.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.3.linear1.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.10.linear2.weight', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.6.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.8.norm2.weight', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.2.norm1.bias', 'electra.discriminator.electra.encoder.layers.3.linear2.weight', 'electra.generator.electra.encoder.layers.8.linear2.weight', 'electra.generator.electra.encoder.layers.8.norm1.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.3.norm1.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.discriminator.electra.embeddings.word_embeddings.weight', 'electra.discriminator.electra.encoder.layers.8.linear2.weight', 'electra.generator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.8.linear1.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.10.linear1.weight', 'electra.discriminator.electra.encoder.layers.0.linear1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.10.linear2.bias', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.5.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.discriminator.electra.embeddings.position_embeddings.weight', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.linear2.bias', 'electra.generator.electra.encoder.layers.10.norm1.bias', 'electra.generator.electra.embeddings.token_type_embeddings.weight', 'electra.discriminator.electra.encoder.layers.7.norm2.bias', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.generator.generator_predictions.layer_norm.bias', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm1.bias', 'electra.generator.electra.encoder.layers.7.linear1.weight', 'electra.discriminator.electra.encoder.layers.1.linear2.bias', 'electra.discriminator.electra.encoder.layers.0.norm2.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.6.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.7.linear1.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.9.linear2.weight', 'electra.generator.electra.encoder.layers.9.norm2.bias', 'electra.discriminator.electra.encoder.layers.0.linear2.weight', 'electra.generator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.3.linear2.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.generator.electra.embeddings.position_embeddings.weight', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.generator.generator_predictions.layer_norm.weight', 'electra.generator.electra.encoder.layers.8.norm2.bias', 'electra.generator.electra.encoder.layers.3.linear2.bias', 'electra.generator.electra.encoder.layers.1.norm1.weight', 'electra.generator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.2.norm2.bias', 'electra.generator.electra.encoder.layers.2.linear1.weight', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.6.norm1.weight', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.9.linear1.weight', 'electra.generator.electra.encoder.layers.5.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.1.norm1.bias', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.linear1.weight', 'electra.generator.electra.encoder.layers.9.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.0.norm1.weight', 'electra.discriminator.electra.encoder.layers.5.norm2.weight', 'electra.generator.electra.encoder.layers.5.linear1.weight', 'electra.generator.electra.encoder.layers.11.linear2.bias', 'electra.generator.electra.encoder.layers.0.norm1.bias', 'electra.discriminator.electra.encoder.layers.6.linear1.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.4.norm2.bias', 'electra.generator.electra.encoder.layers.4.norm2.bias', 'electra.discriminator.electra.encoder.layers.0.linear2.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.4.linear2.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.5.norm2.weight', 'electra.generator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.4.norm1.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.3.norm2.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.generator.generator_predictions.dense.weight', 'electra.generator.electra.encoder.layers.2.norm1.weight', 'electra.discriminator.electra.encoder.layers.1.linear1.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.9.norm2.weight', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.k_proj.weight', 'electra.generator.generator_predictions.dense.bias', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.7.linear1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.11.norm1.bias', 'electra.generator.electra.encoder.layers.8.norm1.weight', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.k_proj.bias', 'electra.generator.electra.embeddings.word_embeddings.weight', 'electra.generator.electra.encoder.layers.6.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.8.norm1.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.1.norm2.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.2.norm2.bias', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.0.norm1.bias', 'electra.generator.electra.encoder.layers.4.norm2.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.weight', 'electra.discriminator.discriminator_rtd.dense.bias', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.discriminator.bias_mts.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.11.linear2.weight', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.3.norm2.bias', 'electra.generator.electra.encoder.layers.7.norm2.bias', 'electra.discriminator.electra.encoder.layers.8.norm2.bias', 'electra.generator.electra.encoder.layers.8.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.8.norm2.weight', 'electra.discriminator.electra.encoder.layers.11.linear1.bias', 'electra.discriminator.electra.encoder.layers.3.norm2.weight', 'electra.generator.electra.encoder.layers.0.linear1.bias', 'electra.generator.electra.encoder.layers.3.norm2.bias', 'electra.discriminator.electra.encoder.layers.1.norm1.bias', 'electra.generator.electra.encoder.layers.1.norm2.weight', 'electra.generator.electra.encoder.layers.1.norm2.bias', 'electra.generator.electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.9.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.9.linear1.bias', 'electra.discriminator.electra.encoder.layers.4.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.2.linear2.weight', 'electra.discriminator.discriminator_rtd.dense_prediction.weight', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.4.linear2.bias', 'electra.generator.electra.encoder.layers.7.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.2.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.5.linear2.weight', 'electra.discriminator.electra.encoder.layers.0.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.10.norm1.bias', 'electra.generator.electra.encoder.layers.3.linear1.weight', 'electra.discriminator.electra.encoder.layers.5.linear1.bias', 'electra.generator.electra.encoder.layers.5.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.7.linear2.weight', 'electra.generator.electra.encoder.layers.7.norm1.bias', 'electra.generator.electra.encoder.layers.3.linear1.bias', 'electra.discriminator.electra.encoder.layers.6.norm1.weight', 'electra.discriminator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.weight', 'electra.generator.electra.embeddings.layer_norm.bias', 'electra.generator.electra.encoder.layers.1.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.5.self_attn.k_proj.weight', 'electra.discriminator.discriminator_rtd.dense.weight', 'electra.discriminator.electra.encoder.layers.7.norm1.weight', 'electra.generator.electra.encoder.layers.6.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.11.norm1.weight', 'electra.generator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.3.linear2.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.6.linear1.weight', 'electra.generator.electra.encoder.layers.8.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.7.norm1.weight', 'electra.generator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.9.norm1.weight', 'electra.generator.electra.encoder.layers.9.norm1.bias', 'electra.generator.electra.encoder.layers.2.norm1.bias', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.11.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.4.linear1.weight', 'electra.generator.electra.encoder.layers.10.norm2.weight', 'electra.discriminator.discriminator_mts.weight', 'electra.generator.electra.encoder.layers.3.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.0.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.6.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.2.linear2.bias', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.9.norm2.weight', 'electra.discriminator.electra.encoder.layers.8.self_attn.v_proj.bias', 'electra.discriminator.discriminator_csp.out_proj.bias', 'electra.generator.electra.encoder.layers.11.linear1.bias', 'electra.discriminator.electra.encoder.layers.10.norm1.weight', 'electra.discriminator.electra.encoder.layers.8.linear1.weight', 'electra.generator.electra.encoder.layers.5.linear1.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.11.linear1.weight', 'electra.generator.electra.encoder.layers.3.norm1.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.q_proj.weight', 'electra.generator.electra.embeddings.layer_norm.weight', 'electra.generator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.2.self_attn.v_proj.weight', 'electra.discriminator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.10.norm2.bias', 'electra.discriminator.electra.encoder.layers.11.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.bias', 'electra.generator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.7.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.norm2.bias', 'electra.discriminator.discriminator_rtd.dense_prediction.bias', 'electra.discriminator.electra.encoder.layers.1.norm1.weight', 'electra.discriminator.electra.encoder.layers.7.norm1.bias', 'electra.generator.generator_lm_head_bias', 'electra.discriminator.electra.encoder.layers.1.linear1.bias', 'electra.discriminator.electra.encoder.layers.6.linear2.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.10.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.8.linear1.weight', 'electra.generator.electra.encoder.layers.6.norm2.weight', 'electra.discriminator.electra.encoder.layers.8.linear1.bias', 'electra.discriminator.electra.encoder.layers.6.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.7.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.2.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.9.norm1.weight', 'electra.discriminator.electra.encoder.layers.4.norm1.weight', 'electra.generator.electra.encoder.layers.11.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.9.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.4.self_attn.q_proj.weight', 'electra.generator.electra.encoder.layers.6.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.weight', 'electra.discriminator.discriminator_csp.dense.weight', 'electra.discriminator.electra.encoder.layers.7.linear1.weight', 'electra.discriminator.electra.encoder.layers.2.linear2.weight', 'electra.discriminator.electra.encoder.layers.6.norm2.bias', 'electra.discriminator.discriminator_mts.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.bias', 'electra.discriminator.electra.encoder.layers.9.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.1.self_attn.v_proj.bias', 'electra.generator.electra.encoder.layers.10.linear2.bias', 'electra.generator.electra.encoder.layers.11.norm2.bias', 'electra.generator.electra.encoder.layers.2.linear2.bias', 'electra.discriminator.electra.encoder.layers.6.norm1.bias', 'electra.generator.electra.encoder.layers.2.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.4.linear1.bias', 'electra.generator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.4.linear1.bias', 'electra.generator.electra.encoder.layers.0.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.7.norm2.weight', 'electra.generator.electra.encoder.layers.6.norm2.bias', 'electra.discriminator.electra.encoder.layers.3.self_attn.v_proj.bias', 'electra.discriminator.electra.encoder.layers.8.self_attn.q_proj.weight', 'electra.discriminator.discriminator_csp.dense.bias', 'electra.generator.electra.encoder.layers.1.linear2.bias', 'electra.discriminator.electra.encoder.layers.3.linear1.bias', 'electra.discriminator.electra.encoder.layers.2.linear1.weight', 'electra.generator.electra.encoder.layers.1.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.6.self_attn.out_proj.weight', 'electra.generator.electra.encoder.layers.3.norm1.weight', 'electra.discriminator.electra.encoder.layers.3.self_attn.out_proj.bias', 'electra.generator.electra.encoder.layers.0.norm2.weight', 'electra.generator.electra.encoder.layers.2.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.5.self_attn.q_proj.bias', 'electra.discriminator.electra.encoder.layers.11.norm2.bias', 'electra.discriminator.electra.encoder.layers.0.self_attn.k_proj.weight', 'electra.generator.electra.encoder.layers.2.linear1.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.v_proj.weight', 'electra.generator.electra.encoder.layers.11.norm2.weight', 'electra.discriminator.electra.encoder.layers.11.self_attn.out_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.5.norm1.weight', 'electra.generator.electra.encoder.layers.8.self_attn.out_proj.bias', 'electra.discriminator.discriminator_csp.out_proj.weight', 'electra.discriminator.electra.encoder.layers.4.norm1.bias', 'electra.discriminator.electra.encoder.layers.10.self_attn.q_proj.weight', 'electra.discriminator.electra.encoder.layers.10.self_attn.out_proj.bias', 'electra.discriminator.electra.encoder.layers.10.linear1.bias', 'electra.discriminator.electra.encoder.layers.4.norm2.weight', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.bias', 'electra.generator.electra.encoder.layers.1.self_attn.k_proj.weight', 'electra.discriminator.electra.encoder.layers.6.norm2.weight', 'electra.discriminator.electra.encoder.layers.0.norm2.weight', 'electra.generator.electra.encoder.layers.11.self_attn.q_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-01-04 15:02:12,298] [    INFO][0m - start load data : 2023-01-04 15:02:12[0m
[32m[2023-01-04 15:02:12,546] [    INFO][0m - load data done, total : 0.2476949691772461 s[0m
[32m[2023-01-04 15:02:12,647] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2023-01-04 15:02:12,647] [    INFO][0m - Using half precision[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - ============================================================[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - paddle commit id              :941811b2f0824beba49bbca0f0c55232abe4785a[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - _no_sync_in_gradient_accumulation:True[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - adam_beta1                    :0.9[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - adam_beta2                    :0.999[0m
[32m[2023-01-04 15:02:12,650] [    INFO][0m - adam_epsilon                  :1e-08[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - bf16                          :False[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - bf16_full_eval                :False[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - current_device                :gpu:7[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - dataloader_drop_last          :False[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - dataloader_num_workers        :2[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - device                        :gpu[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - disable_tqdm                  :False[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - do_eval                       :False[0m
[32m[2023-01-04 15:02:12,651] [    INFO][0m - do_export                     :False[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - do_predict                    :False[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - do_train                      :True[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - eval_batch_size               :8[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - eval_iters                    :10[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - eval_steps                    :None[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - evaluation_strategy           :IntervalStrategy.NO[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - fp16                          :True[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - fp16_full_eval                :False[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - fp16_opt_level                :O1[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - gradient_accumulation_steps   :1[0m
[32m[2023-01-04 15:02:12,652] [    INFO][0m - greater_is_better             :None[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - ignore_data_skip              :False[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - label_names                   :None[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - learning_rate                 :0.001[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - load_best_model_at_end        :False[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - local_process_index           :2[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - local_rank                    :2[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - log_level                     :-1[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - log_level_replica             :-1[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - log_on_each_node              :True[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - logging_dir                   :output/eheath-pretraining/runs/Jan04_15-02-03_yq01-qianmo-com-255-129-12.yq01[0m
[32m[2023-01-04 15:02:12,653] [    INFO][0m - logging_first_step            :False[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - logging_steps                 :20[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - logging_strategy              :IntervalStrategy.STEPS[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - lr_scheduler_type             :SchedulerType.LINEAR[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - max_grad_norm                 :1.0[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - max_steps                     :100[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - metric_for_best_model         :None[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - minimum_eval_times            :None[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - no_cuda                       :False[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - num_train_epochs              :3.0[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - optim                         :OptimizerNames.ADAMW[0m
[32m[2023-01-04 15:02:12,654] [    INFO][0m - output_dir                    :output/eheath-pretraining[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - overwrite_output_dir          :False[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - past_index                    :-1[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - per_device_eval_batch_size    :8[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - per_device_train_batch_size   :8[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - prediction_loss_only          :False[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - process_index                 :2[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - recompute                     :True[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - remove_unused_columns         :True[0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - report_to                     :['visualdl'][0m
[32m[2023-01-04 15:02:12,655] [    INFO][0m - resume_from_checkpoint        :None[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - run_name                      :output/eheath-pretraining[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - save_on_each_node             :False[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - save_steps                    :25[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - save_strategy                 :IntervalStrategy.STEPS[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - save_total_limit              :10[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - scale_loss                    :32768[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - seed                          :42[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - sharding                      :[][0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - sharding_degree               :-1[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - should_log                    :False[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - should_save                   :False[0m
[32m[2023-01-04 15:02:12,656] [    INFO][0m - skip_memory_metrics           :True[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - test_iters                    :100[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - train_batch_size              :8[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - warmup_ratio                  :0.01[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - warmup_steps                  :0[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - weight_decay                  :0.01[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - world_size                    :3[0m
[32m[2023-01-04 15:02:12,657] [    INFO][0m - [0m
[32m[2023-01-04 15:02:12,708] [    INFO][0m - ***** Running training *****[0m
[32m[2023-01-04 15:02:12,708] [    INFO][0m -   Num examples = 200061[0m
[32m[2023-01-04 15:02:12,708] [    INFO][0m -   Num Epochs = 1[0m
[32m[2023-01-04 15:02:12,709] [    INFO][0m -   Instantaneous batch size per device = 8[0m
[32m[2023-01-04 15:02:12,709] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 24[0m
[32m[2023-01-04 15:02:12,709] [    INFO][0m -   Gradient Accumulation steps = 1[0m
[32m[2023-01-04 15:02:12,709] [    INFO][0m -   Total optimization steps = 100[0m
[32m[2023-01-04 15:02:12,709] [    INFO][0m -   Total num train samples = 2400[0m
[32m[2023-01-04 15:02:12,725] [    INFO][0m -   Number of trainable parameters = 190772769[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
Found inf or nan, current scale is: 16384.0, decrease to: 16384.0*0.5
Found inf or nan, current scale is: 8192.0, decrease to: 8192.0*0.5
Found inf or nan, current scale is: 4096.0, decrease to: 4096.0*0.5
Found inf or nan, current scale is: 2048.0, decrease to: 2048.0*0.5
Found inf or nan, current scale is: 1024.0, decrease to: 1024.0*0.5
[32m[2023-01-04 15:03:50,326] [    INFO][0m - 
Training completed. 
[0m
