# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../transformers.md:1
msgid "PaddleNLP Transformer API"
msgstr "PaddleNLP Transformer API"

#: ../../transformers.md:3
msgid "随着深度学习的发展，NLP领域涌现了一大批高质量的Transformer类预训练模型，多次刷新各种NLP任务SOTA。PaddleNLP为用户提供了常用的BERT、ERNIE、RoBERTa、XLNet经典结构预训练模型，让开发者能够方便快捷应用各类Transformer预训练模型及其下游任务。"
msgstr ""
"With the development of deep learning, a large number of high-quality Transformer-like pre-training models have emerged in the NLP field,"
"and various NLP task SOTA have been refreshed many times."
"PaddleNLP provides users with commonly used BERT, ERNIE, RoBERTa, XLNet classical structured pre-training models, allowing developers to easily and quickly apply various Transformer pre-training models and their downstream tasks."

#: ../../transformers.md:6
msgid "Transformer预训练模型汇总"
msgstr "Transformer pre-trained model summary"

#: ../../transformers.md:8
msgid "下表汇总了目前PaddleNLP支持的各类预训练模型。用户可以使用PaddleNLP提供的模型，完成问答、文本分类、序列标注、文本生成等任务。同时我们提供了34种预训练的参数权重供用户使用，其中包含了17种中文语言模型的预训练权重。"
msgstr ""
"The following table summarizes the various pre-training models currently supported by PaddleNLP"
"Users can use the models provided by PaddleNLP to complete tasks such as question answering, text classification, sequence labeling, and text generation."
"At the same time, we provide 34 pre-trained parameter weights for users, including 17 pre-trained weights for Chinese language models."

#: ../../transformers.md:77
msgid ""
"NOTE：其中中文的预训练模型有bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-"
"chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-"
"wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, "
"chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, "
"unified_transformer-12L-cn, unified_transformer-12L-cn-luge。"
msgstr ""

#: ../../transformers.md:79
msgid "预训练模型使用方法"
msgstr "How to use the per-trained models"

#: ../../transformers.md:81
msgid ""
"PaddleNLP Transformer API在提丰富预训练模型的同时，也降低了用户的使用门槛。只需十几行代码"
"，用户即可完成模型加载和下游任务Fine-tuning。"
msgstr ""
"While the PaddleNLP Transformer API provides rich pre-trained models, it also lowers the threshold for users to use."
"User only need a dozen lines of code to complete model loading and downstream task fine-tuning."


#: ../../transformers.md:109
msgid "上面的代码给出使用预训练模型的简要示例，更完整详细的示例代码，可以参考使用预训练模型Fine-tune完成中文文本分类任务。"
msgstr ""
"The above code gives a brief example of using a pretrained model."
"To see the more complete and detailed code sample, please refer to how to use the pre-trained model fine-tune to complete the Chinese text classification task."


#: ../../transformers.md:111
msgid "加载数据集：PaddleNLP内置了多种数据集，用户可以一键导入所需的数据集。"
msgstr "Loading datasets: PaddleNLP has built-in multiple datasets, and users can import the desired datasets with one click."

#: ../../transformers.md:112
msgid ""
"加载预训练模型：PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载。第一个参数是汇总表中对应的 "
"Pretrained "
"Weight，可加载对应的预训练权重。BertForSequenceClassification初始化__init__所需的其他参数，如num_classes等，也是通过from_pretrained()传入。Tokenizer使用同样的from_pretrained方法加载。"
msgstr ""
"Loading per-trained model: PaddleNLP's pretrained models can be easily loaded via the from_pretrained() method."
"The first parameter is the corresponding Per-trained Weight in the summary table,which is to load the corresponding per-trained weight."
"Other parameters are required by BertForSequenceClassification to initialize __init__, such as num_classes,etc."
"which are also passed in through from_pretrained()."
"Tokenizers are loaded using the same from_pretrained method."

#: ../../transformers.md:113
msgid "使用tokenier将dataset处理成模型的输入。此部分可以参考前述的详细示例代码。"
msgstr "Use tokenier to process the dataset as input to the model. This part can refer to the detailed sample code mentioned above."

#: ../../transformers.md:114
msgid "定义训练所需的优化器，loss函数等，就可以开始进行模型fine-tune任务。"
msgstr "Define the optimizer, loss function, etc. for training, and you can start the model fine-tune task"

#: ../../transformers.md:117
msgid "预训练模型适用任务汇总"
msgstr "Summary of tasks which are suitable for pre-training models"

#: ../../transformers.md:119
msgid "本小节按照模型适用的不同任务类型，对上表Transformer预训练模型汇总的Task进行分类汇总。主要包括文本分类、序列标注、问答任务、文本生成、机器翻译等。"
msgstr ""
"This section categorizes and summarizes the tasks summarized by the Transformer pre-training model in the above table according to the different task types applicable to the model."
"Including text classification, sequence labeling, question and answer tasks, text generation, machine translation, etc."

#: ../../transformers.md:164
msgid ""
"用户可以切换表格中的不同模型，来处理相同类型的任务。如对于预训练模型使用方法中的文本分类任务，用户可以将BertForSequenceClassification换成ErnieForSequenceClassification,"
" 来寻找更适合的预训练模型。"
msgstr "Users can switch between different models in the table to handle the same type of task."
"For the text classification tasks using per-trained models, users can replace BertForSequenceClassification with ErnieForSequenceClassification to find a more suitable pre-trained model."

#: ../../transformers.md:167
msgid "Reference"
msgstr "Reference"

#: ../../transformers.md:168
msgid ""
"部分中文预训练模型来自：ymcui/Chinese-BERT-wwm, ymcui/Chinese-XLNet, "
"huggingface/xlnet_chinese_large, Knover/luge-dialogue"
msgstr ""
"Some of the Chinese per-trained models are from: ymcui/Chinese-BERT-wwm, ymcui/Chinese-XLNet,"
"huggingface/xlnet_chinese_large, Knover/luge-dialogue"

#: ../../transformers.md:169
msgid ""
"Sun, Yu, et al. \"Ernie: Enhanced representation through knowledge "
"integration.\" arXiv preprint arXiv:1904.09223 (2019)."
msgstr ""
"Sun, Yu, et al. \"Ernie: Enhanced representation through knowledge "
"integration.\" arXiv preprint arXiv:1904.09223 (2019)."

#: ../../transformers.md:170
msgid ""
"Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional "
"transformers for language understanding.\" arXiv preprint "
"arXiv:1810.04805 (2018)."
msgstr ""
"Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional "
"transformers for language understanding.\" arXiv preprint "
"arXiv:1810.04805 (2018)."

#: ../../transformers.md:171
msgid ""
"Cui, Yiming, et al. \"Pre-training with whole word masking for chinese "
"bert.\" arXiv preprint arXiv:1906.08101 (2019)."
msgstr ""
"Cui, Yiming, et al. \"Pre-training with whole word masking for chinese "
"bert.\" arXiv preprint arXiv:1906.08101 (2019)."

#: ../../transformers.md:172
msgid ""
"Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint "
"arXiv:1706.03762 (2017)."
msgstr ""
"Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint "
"arXiv:1706.03762 (2017)."

#: ../../transformers.md:173
msgid ""
"Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for "
"language understanding.\" arXiv preprint arXiv:1906.08237 (2019)."
msgstr ""
"Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for "
"language understanding.\" arXiv preprint arXiv:1906.08237 (2019)."

#: ../../transformers.md:174
msgid ""
"Clark, Kevin, et al. \"Electra: Pre-training text encoders as "
"discriminators rather than generators.\" arXiv preprint arXiv:2003.10555 "
"(2020)."
msgstr ""
"Clark, Kevin, et al. \"Electra: Pre-training text encoders as "
"discriminators rather than generators.\" arXiv preprint arXiv:2003.10555 "
"(2020)."

#: ../../transformers.md:175
msgid ""
"Radford, Alec, et al. \"Language models are unsupervised multitask "
"learners.\" OpenAI blog 1.8 (2019): 9."
msgstr ""
"Radford, Alec, et al. \"Language models are unsupervised multitask "
"learners.\" OpenAI blog 1.8 (2019): 9."
