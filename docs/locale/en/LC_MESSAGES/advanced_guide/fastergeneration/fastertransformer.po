# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:3
msgid "Transformer高性能加速"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:7
msgid "使用环境说明"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:9
msgid "本项目依赖于 PaddlePaddle 2.1.0 及以上版本或适当的 develop 版本"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:10
msgid "CMake >= 3.10"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:11
msgid "CUDA 10.1 或 10.2（需要 PaddlePaddle 框架一致）"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:12
msgid "gcc 版本需要与编译 PaddlePaddle 版本一致，比如使用 gcc8.2"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:13
msgid "推荐使用 Python3"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:14
msgid ""
"`FasterTransformer "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#setup>`_ 使用必要的环境"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:15
msgid "环境依赖"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:17
msgid "attrdict"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:18
msgid "pyyaml"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:26
msgid "快速开始"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:28
msgid ""
"我们实现了基于 FasterTransformer 的自定义 op 的接入，用于加速文本生成模型在 GPU "
"上的预测性能。接下来，我们将分别介绍基于 Python 动态图和预测库使用 FasterTransformer 自定义 op 的方式，包括 op "
"的编译与使用。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:31
msgid "Python 动态图使用自定义 op"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:34
msgid "JIT 自动编译"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:36
msgid ""
"目前当基于动态图使用 FasterTransformer 预测加速自定义 op 时，PaddleNLP 提供了 Just In Time "
"的自动编译，在一些 API 上，用户无需关注编译流程，可以直接执行对应的 API，程序会自动编译需要的第三方库。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:38
msgid ""
"以 Transformer 为例，可以直接调用 `TransformerGenerator()` 这个 API，程序会自动编译。使用示例可以参考 "
"`Transformer 预测加速使用示例-sample "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" 预测加速使用示例-机器翻译 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:41
#: ../advanced_guide/fastergeneration/fastertransformer.rst:154
msgid "编译自定义OP"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:43
msgid "除了自动编译外，如果需要自行编译，我们已经提供对应的 CMakeLists.txt，可以参考使用如下的方式完成编译。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:46
#: ../advanced_guide/fastergeneration/fastertransformer.rst:159
msgid "PaddleNLP 准备"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:48
msgid ""
"首先，如果需要从源码自行编译，可以直接使用 Python 的 package 下的 paddlenlp，或是可从 github 克隆一个 "
"PaddleNLP，并重新编译:"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:50
#: ../advanced_guide/fastergeneration/fastertransformer.rst:163
msgid "以下以从 github 上 clone 一个新版 PaddleNLP 为例:"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:56
msgid "其次，配置环境变量，让我们可以使用当前 clone 的 paddlenlp，并进入到自定义 OP 的路径，准备后续的编译操作："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:64
#: ../advanced_guide/fastergeneration/fastertransformer.rst:176
msgid "编译"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:66
#: ../advanced_guide/fastergeneration/fastertransformer.rst:178
msgid "编译之前，请确保安装的 PaddlePaddle 的版本高于 2.1.0 或是基于最新的 develop 分支的代码编译，并且正常可用。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:68
#: ../advanced_guide/fastergeneration/fastertransformer.rst:180
msgid "编译自定义 OP 可以参照一下步骤："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:78
#: ../advanced_guide/fastergeneration/fastertransformer.rst:190
msgid "可以使用的编译选项包括："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:80
msgid ""
"`-DPY_CMD`: 指定当前装有 PaddlePaddle 版本的 python 环境，比如 "
"`-DPY_CMD=python3.7`。若未指定 `-DPY_CMD` 将会默认使用系统命令 `python` 对应的 Python。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:81
#: ../advanced_guide/fastergeneration/fastertransformer.rst:207
msgid ""
"`-DSM`: 是指的所用 GPU 的 compute capability，建议不使用该选项设置，未设置时将自动检测。如要设置，需根据 "
"[compute capability](https://developer.nvidia.com/zh-cn/cuda-"
"gpus#compute) 进行设置，如 V100 时设置 `-DSM=70` 或 T4 时设置 `-DSM=75`。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:82
#: ../advanced_guide/fastergeneration/fastertransformer.rst:208
msgid ""
"`-DWITH_GPT`: 是否编译带有 GPT 相关的 lib。若使用 GPT-2 高性能推理，需要加上 `-DWITH_GPT=ON`。默认为"
" OFF。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:83
#: ../advanced_guide/fastergeneration/fastertransformer.rst:209
msgid ""
"`-DWITH_UNIFIED`: 是否编译带有 Unified Transformer 或是 UNIMOText 相关的 "
"lib。若使用，需要加上 `-DWITH_UNIFIED=ON`。默认为 ON。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:84
#: ../advanced_guide/fastergeneration/fastertransformer.rst:210
msgid "`-DWITH_BART`: 是否编译带有 BART 支持的相关 lib。若使用，需要加上 `-DWITH_BART=ON`。默认为 ON。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:85
#: ../advanced_guide/fastergeneration/fastertransformer.rst:211
msgid "`-DWITH_DECODER`: 是否编译带有 decoder 优化的 lib。默认为 ON。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:87
msgid ""
"最终，编译会在 `./build/lib/` 路径下，产出 `libdecoding_op.so`，即需要的 FasterTransformer "
"decoding 执行的库。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:90
msgid "使用 Transformer decoding 高性能推理"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:92
msgid ""
"编写 python 脚本的时候，调用 `FasterTransformer API "
"<https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.ops.faster_transformer.transformer.faster_transformer.html#paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer>`_"
" 即可实现 Transformer 模型的高性能预测。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:94
msgid "举例如下："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:120
msgid ""
"若当前环境下没有需要的自定义 op 的动态库，将会使用 JIT 自动编译需要的动态库。如果需要自行编译自定义 op "
"所需的动态库，可以如前文所述进行编译。编译好后，使用 "
"`FasterTransformer(decoding_lib=\"/path/to/lib\", ...)` 可以完成导入。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:122
msgid ""
"更详细的例子可以参考 `Transformer 预测加速使用示例-sample "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" 预测加速使用示例-机器翻译 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_，我们提供了更详细用例。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:125
msgid "Transformer decoding 示例代码"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:127
msgid "使用 PaddlePaddle 仅执行 decoding 测试（float32）："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:137
msgid ""
"使用 PaddlePaddle 仅执行 decoding 测试（float16）： 执行 float16 的 "
"decoding，需要在执行的时候，加上 `--use_fp16_decoding` 选项。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:148
msgid ""
"其中，`decoding_gemm` 不同参数的意义可以参考 `FasterTransformer 文档 "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-"
"decoderdecoding-demos>`_。这里提前执行 `decoding_gemm`，可以在当前路径下生成一个 config "
"文件，里面会包含针对当前 decoding 部分提供的配置下，性能最佳的矩阵乘的算法，并在执行的时候读入这个数据。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:151
msgid "C++ 预测库使用自定义 op"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:156
msgid ""
"在 C++ 预测库使用自定义 OP 需要将实现的 C++、CUDA 代码**以及 C++ 预测的 "
"demo**编译成一个可执行文件。因预测库支持方式与 Python 不同，这个过程将不会产生自定义 op "
"的动态库，将直接得到可执行文件。我们已经提供对应的 CMakeLists.txt ，可以参考使用如下的方式完成编译。并获取执行 demo。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:161
msgid ""
"首先，因为需要基于当前环境重新编译，当前的 paddlenlp 的 python 包里面并不包含 FasterTransformer 相关 "
"lib，需要从源码自行编译，可以直接使用 Python 的 package 下的 paddlenlp，或是可从 github 克隆一个 "
"PaddleNLP，并重新编译:"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:169
msgid "其次，让我们可以使用当前 clone 的 paddlenlp，并进入到自定义 OP 的路径，准备后续的编译操作："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:192
msgid ""
"`-DPADDLE_LIB`: 需要指明使用的 PaddlePaddle 预测库的路径 "
"`/path/to/paddle_inference_install_dir/`，需要使用的 PaddlePaddle 的 lib "
"可以选择自行编译或者直接从官网下载 `paddle_inference_linux_lib "
"<https://paddleinference.paddlepaddle.org.cn/user_guides/download_lib.html#linux>`_。需要注意的是，在该路径下，预测库的组织结构满足："
" .. code-block::"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:206
msgid ""
"`-DDEMO`: 说明预测库使用 demo 的位置。比如指定 -DDEMO=./demo/transformer_e2e.cc 或是 "
"-DDEMO=./demo/gpt.cc。最好使用绝对路径，若使用相对路径，需要是相对于 "
"`PaddleNLP/paddlenlp/ops/faster_transformer/src/` 的相对路径。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:212
msgid "`-DWITH_MKL`: 若当前是使用的 mkl 的 Paddle lib，那么需要打开 MKL 以引入 MKL 相关的依赖。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:213
msgid "`-DON_INFER`: 是否编译 paddle inference 预测库。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:214
msgid "**当使用预测库的自定义 op 的时候，请务必开启 `-DON_INFER=ON` 选项，否则，不会得到预测库的可执行文件。**"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:217
msgid "执行 Transformer decoding on PaddlePaddle"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:219
msgid ""
"编译完成后，在 `build/bin/` 路径下将会看到 `transformer_e2e` "
"的一个可执行文件。通过设置对应的设置参数完成执行的过程。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:226
msgid "举例说明："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:235
msgid "其中："
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:237
msgid ""
"`decoding_gemm` 不同参数的意义可以参考 `FasterTransformer 文档 "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-"
"decoderdecoding-demos>`_。这里提前执行 `decoding_gemm`，可以在当前路径下生成一个 config "
"文件，里面会包含针对当前 decoding 部分提供的配置下，性能最佳的矩阵乘的算法，并在执行的时候读入这个数据。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:238
msgid "`DATA_HOME` 则是 `paddlenlp.utils.env.DATA_HOME` 返回的路径。"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:240
msgid ""
"预测所需要的模型文件，可以通过 `faster_transformer/README.md "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/machine_translation/transformer/faster_transformer/README.md>`_"
" 文档中所记述的方式导出。"
msgstr ""

