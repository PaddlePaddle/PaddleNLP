# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:3
msgid "Transformer高性能加速"
msgstr "Transformer with High Performance Acceleration"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:7
msgid "使用环境说明"
msgstr "Instruction for Environment of Use"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:9
msgid "本项目依赖于 PaddlePaddle 2.1.0 及以上版本或适当的 develop 版本"
msgstr "This project depends on PaddlePaddle 2.1.0 and above or develop version"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:10
msgid "CMake >= 3.10"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:11
msgid "CUDA 10.1 或 10.2（需要 PaddlePaddle 框架一致）"
msgstr "CUDA 10.1 or 10.2(need the match the version used in PaddlePaddle)"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:12
msgid "gcc 版本需要与编译 PaddlePaddle 版本一致，比如使用 gcc8.2"
msgstr "The version of your gcc needs to match the verions used for compiling PaddlePaddle, e.g. gcc8.2"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:13
msgid "推荐使用 Python3"
msgstr "Python 3 is recommanded"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:14
msgid ""
"`FasterTransformer "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#setup>`_ 使用必要的环境"
msgstr ""
"`FasterTransformer <https://github.com/NVIDIA/FasterTransformer/tree/v3.1#setup>`_ Use the correct environment"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:15
msgid "环境依赖"
msgstr "Environment Dependencies"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:17
msgid "attrdict"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:18
msgid "pyyaml"
msgstr ""

#: ../advanced_guide/fastergeneration/fastertransformer.rst:26
msgid "快速开始"
msgstr "Quick Start"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:28
msgid ""
"我们实现了基于 FasterTransformer 的自定义 op 的接入，用于加速文本生成模型在 GPU "
"上的预测性能。接下来，我们将分别介绍基于 Python 动态图和预测库使用 FasterTransformer 自定义 op 的方式，包括 op "
"的编译与使用。"
msgstr ""
"We implemented the access of custom op based on the FasterTransformer to accelerate the prediction performance of text generation model on GPU. "
"Next, we will introduce how to use FasterTransformer to customize op based on Python dynamic graph and prediction library, inclduing the compilation and use of op."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:31
msgid "Python 动态图使用自定义 op"
msgstr "Python Dynamic Graph using Custom Op"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:34
msgid "JIT 自动编译"
msgstr "JIT Auto Compilation"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:36
msgid ""
"目前当基于动态图使用 FasterTransformer 预测加速自定义 op 时，PaddleNLP 提供了 Just In Time "
"的自动编译，在一些 API 上，用户无需关注编译流程，可以直接执行对应的 API，程序会自动编译需要的第三方库。"
msgstr ""
"Presently, when using FasterTransformer to predict and accelerate custom ops based on dunamic graphs, PaddleNLP provides "
"automatic compilation of Just In Time. For some APIs, users do not need to worry about the compilation process, "
"they can call the APIs directly and the program will automatically compile the required third-party library."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:38
msgid ""
"以 Transformer 为例，可以直接调用 `TransformerGenerator()` 这个 API，程序会自动编译。使用示例可以参考 "
"`Transformer 预测加速使用示例-sample "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" 预测加速使用示例-机器翻译 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_。"
msgstr ""
"Taking Transformer as an example, we can call `TransformerGenerator()` directly, so the program will be automatically compiled."
"For usage examples, please refer to `Transformer Acceleration for Prediction-sample <https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" Acceleration for Prediction Demo - Machine Translation <https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_. "

#: ../advanced_guide/fastergeneration/fastertransformer.rst:41
#: ../advanced_guide/fastergeneration/fastertransformer.rst:154
msgid "编译自定义OP"
msgstr "Compile Custom OP"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:43
msgid "除了自动编译外，如果需要自行编译，我们已经提供对应的 CMakeLists.txt，可以参考使用如下的方式完成编译。"
msgstr "Besides the auto-compilation, if you need to manual compilation, we provide CMakeLists.txt, you can refer to the example below."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:46
#: ../advanced_guide/fastergeneration/fastertransformer.rst:159
msgid "PaddleNLP 准备"
msgstr "PaddleNLP Preparation"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:48
msgid ""
"首先，如果需要从源码自行编译，可以直接使用 Python 的 package 下的 paddlenlp，或是可从 github 克隆一个 "
"PaddleNLP，并重新编译:"
msgstr ""
"First of all, if you would like to compile from the source code, please use the paddlenlp folder under Python/package folder, or you can clone the PaddleNLP repository from Github."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:50
#: ../advanced_guide/fastergeneration/fastertransformer.rst:163
msgid "以下以从 github 上 clone 一个新版 PaddleNLP 为例:"
msgstr "Let's clone a PaddleNLP from Github and take it as the example:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:56
msgid "其次，配置环境变量，让我们可以使用当前 clone 的 paddlenlp，并进入到自定义 OP 的路径，准备后续的编译操作："
msgstr "Secondly, configure the environment variables, so that we can use the cloned paddlenlp, "
"and cd into the path where you store the custom OP, then prepare for the following compilation process:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:64
#: ../advanced_guide/fastergeneration/fastertransformer.rst:176
msgid "编译"
msgstr "Comiplation"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:66
#: ../advanced_guide/fastergeneration/fastertransformer.rst:178
msgid "编译之前，请确保安装的 PaddlePaddle 的版本高于 2.1.0 或是基于最新的 develop 分支的代码编译，并且正常可用。"
msgstr "Please make sure the version of PaddlePaddle is higher than 2.1.0 or based on the latest develop branch before starting the compilation, also make sure they are working."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:68
#: ../advanced_guide/fastergeneration/fastertransformer.rst:180
msgid "编译自定义 OP 可以参照一下步骤："
msgstr "Following the next step to customize OP:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:78
#: ../advanced_guide/fastergeneration/fastertransformer.rst:190
msgid "可以使用的编译选项包括："
msgstr "The available options for compilation:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:80
msgid ""
"`-DPY_CMD`: 指定当前装有 PaddlePaddle 版本的 python 环境，比如 "
"`-DPY_CMD=python3.7`。若未指定 `-DPY_CMD` 将会默认使用系统命令 `python` 对应的 Python。"
msgstr ""
"`-DPY_CMD`: points to the python environment which has installed PaddlePaddle, for instance "
"`-DPY_CMD=python3.7`. If the `-DPY_CMD` is not configured, the system default `python` will be used."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:81
#: ../advanced_guide/fastergeneration/fastertransformer.rst:207
msgid ""
"`-DSM`: 是指的所用 GPU 的 compute capability，建议不使用该选项设置，未设置时将自动检测。如要设置，需根据 "
"[compute capability](https://developer.nvidia.com/zh-cn/cuda-"
"gpus#compute) 进行设置，如 V100 时设置 `-DSM=70` 或 T4 时设置 `-DSM=75`。"
msgstr ""
"`-DSM`: It means the compute capability, not recommanded to use, when unset it will be detected automatically. "
"Follow the instruction [compute capability](https://developer.nvidia.com/zh-cn/cuda-gpus#compute) to configure it, "
"e.g. set `-DSM=70` for V100, or set `-DSM=75` for T4."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:82
#: ../advanced_guide/fastergeneration/fastertransformer.rst:208
msgid ""
"`-DWITH_GPT`: 是否编译带有 GPT 相关的 lib。若使用 GPT-2 高性能推理，需要加上 `-DWITH_GPT=ON`。默认为"
" OFF。"
msgstr ""
"`-DWITH_GPT: Whether to compile the lib relevant to GPT. You need to add `-DWITH_GPT=ON` for use GPT-2. The default value is OFF."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:83
#: ../advanced_guide/fastergeneration/fastertransformer.rst:209
msgid ""
"`-DWITH_UNIFIED`: 是否编译带有 Unified Transformer 或是 UNIMOText 相关的 "
"lib。若使用，需要加上 `-DWITH_UNIFIED=ON`。默认为 ON。"
msgstr ""
"`-DWITH_UNIFIED`: Whether to compile libs with Unified Transformer or UNIMOText. If used, you need to set `-DWITH_UNIFIED=ON`. The default value is ON."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:84
#: ../advanced_guide/fastergeneration/fastertransformer.rst:210
msgid "`-DWITH_BART`: 是否编译带有 BART 支持的相关 lib。若使用，需要加上 `-DWITH_BART=ON`。默认为 ON。"
msgstr ""
"`-DWITH_BART`: Whether to compile libs with support of BART. If used, also set `-DWITH_BART=ON`. The default value is ON."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:85
#: ../advanced_guide/fastergeneration/fastertransformer.rst:211
msgid "`-DWITH_DECODER`: 是否编译带有 decoder 优化的 lib。默认为 ON。"
msgstr "`-DWITH_DECODER`: Whether to compile libs optimaized by decoder. The default value is ON."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:87
msgid ""
"最终，编译会在 `./build/lib/` 路径下，产出 `libdecoding_op.so`，即需要的 FasterTransformer "
"decoding 执行的库。"
msgstr ""
"The compilation will create `libdecoding_op.so` under `./build/lib/` , it is the the library for decoding of FasterTransformer."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:90
msgid "使用 Transformer decoding 高性能推理"
msgstr "Use the high performance Transformer Decoding"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:92
msgid ""
"编写 python 脚本的时候，调用 `FasterTransformer API "
"<https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.ops.faster_transformer.transformer.faster_transformer.html#paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer>`_"
" 即可实现 Transformer 模型的高性能预测。"
msgstr ""
"When writing the Python script, call `FasterTransformer API "
"<https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.ops.faster_transformer.transformer.faster_transformer.html#paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer>`_"
" to do the high performance prediction."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:94
msgid "举例如下："
msgstr "For example:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:120
msgid ""
"若当前环境下没有需要的自定义 op 的动态库，将会使用 JIT 自动编译需要的动态库。如果需要自行编译自定义 op "
"所需的动态库，可以如前文所述进行编译。编译好后，使用 "
"`FasterTransformer(decoding_lib=\"/path/to/lib\", ...)` 可以完成导入。"
msgstr ""
"If there is no required dynamic custom OPs, JIT will be used to compile the required dynamic libraries. If you need to "
"manually compile the library of your custom op, you can follow the aforementioned approache to do the compilation."
"After it compiled, use `FasterTransformer(decoding_lib=\"/path/to/lib\", ...)` to import it."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:122
msgid ""
"更详细的例子可以参考 `Transformer 预测加速使用示例-sample "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" 预测加速使用示例-机器翻译 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_，我们提供了更详细用例。"
msgstr ""
"For more detailed examples, please refer to `Transformer Demo of Accelerated Prediction-sample <https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/ops/faster_transformer/sample/decoding_sample.py>`_，`Transformer"
" Demo of Accelerated Prediction-Machine Translation <https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer/faster_transformer>`_, here we have provided more detailed exmples."


#: ../advanced_guide/fastergeneration/fastertransformer.rst:125
msgid "Transformer decoding 示例代码"
msgstr "Demo Code - Transfoerm Decoding"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:127
msgid "使用 PaddlePaddle 仅执行 decoding 测试（float32）："
msgstr "Use PaddlePaddle to execute decoding for test (folat32):"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:137
msgid ""
"使用 PaddlePaddle 仅执行 decoding 测试（float16）： 执行 float16 的 "
"decoding，需要在执行的时候，加上 `--use_fp16_decoding` 选项。"
msgstr ""
"Use PaddlePaddle to execute decoding for test (folat16): you need to add `--use_fp16_decoding` to use float16 when decoding."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:148
msgid ""
"其中，`decoding_gemm` 不同参数的意义可以参考 `FasterTransformer 文档 "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-"
"decoderdecoding-demos>`_。这里提前执行 `decoding_gemm`，可以在当前路径下生成一个 config "
"文件，里面会包含针对当前 decoding 部分提供的配置下，性能最佳的矩阵乘的算法，并在执行的时候读入这个数据。"
msgstr ""
"For the parameters of `decoding_gemm`, please refer to the `FasterTransformer Documentation <https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-decoderdecoding-demos>`_. "
"Here the `decoding_gemm` is executed first, it will generate a config file under the current path. Inside this config file, it contains the best algorithm of matrix multiplication under the current decoding configureation, "
"and this file will be used during execution."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:151
msgid "C++ 预测库使用自定义 op"
msgstr "Use Custom OPs in C++ Library"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:156
msgid ""
"在 C++ 预测库使用自定义 OP 需要将实现的 C++、CUDA 代码**以及 C++ 预测的 "
"demo**编译成一个可执行文件。因预测库支持方式与 Python 不同，这个过程将不会产生自定义 op "
"的动态库，将直接得到可执行文件。我们已经提供对应的 CMakeLists.txt ，可以参考使用如下的方式完成编译。并获取执行 demo。"
msgstr ""
"In order to use custom OPs with C++ library, you need to compile the code of C++, CUDA and **C++ Prediction Demo** to an executable file."
"Because the way how the library is supported is different from the approach in Python, it will not generate dynamic libraries of custom OP during the procedure, "
"you will get the executable directlt. We provide the CMakeLists.txt, you can refer to the following example to complete the compilation."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:161
msgid ""
"首先，因为需要基于当前环境重新编译，当前的 paddlenlp 的 python 包里面并不包含 FasterTransformer 相关 "
"lib，需要从源码自行编译，可以直接使用 Python 的 package 下的 paddlenlp，或是可从 github 克隆一个 "
"PaddleNLP，并重新编译:"
msgstr ""
"Firstly, because it needs to be recompiled based on the current environment, the current Python package of PaddleNLP does not contain any FasterTransfer related libs."
" you need to compile from the source code. You can use the source code of Python package paddlenlp, or clone from the PaddleNLP Github and compile from that."


#: ../advanced_guide/fastergeneration/fastertransformer.rst:169
msgid "其次，让我们可以使用当前 clone 的 paddlenlp，并进入到自定义 OP 的路径，准备后续的编译操作："
msgstr "Secondly, please cd into the path of custom OP in the cloned PaddleNLP repository:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:192
msgid ""
"`-DPADDLE_LIB`: 需要指明使用的 PaddlePaddle 预测库的路径 "
"`/path/to/paddle_inference_install_dir/`，需要使用的 PaddlePaddle 的 lib "
"可以选择自行编译或者直接从官网下载 `paddle_inference_linux_lib "
"<https://paddleinference.paddlepaddle.org.cn/user_guides/download_lib.html#linux>`_。需要注意的是，在该路径下，预测库的组织结构满足："
" .. code-block::"
msgstr ""
"`-DPADDLE_LIB`: Need to point the path of PaddlePaddle `/path/to/paddle_inference_install_dir/`, the PaddlePaddle lib can be obtained by "
"compiling it yourself or download it from the official website `paddle_inference_linux_lib <https://paddleinference.paddlepaddle.org.cn/user_guides/download_lib.html#linux>`_ ."
"It should be noted that under this path, the folder structure needs to be:  .. code-block::"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:206
msgid ""
"`-DDEMO`: 说明预测库使用 demo 的位置。比如指定 -DDEMO=./demo/transformer_e2e.cc 或是 "
"-DDEMO=./demo/gpt.cc。最好使用绝对路径，若使用相对路径，需要是相对于 "
"`PaddleNLP/paddlenlp/ops/faster_transformer/src/` 的相对路径。"
msgstr ""
"`-DDEMO`: it points to the path of demo. For example,  -DDEMO=./demo/transformer_e2e.cc or "
"-DDEMO=./demo/gpt.cc. Use of absolute path is recommended, if relative path is used, then it needs to be relative to the "
"path of `PaddleNLP/paddlenlp/ops/faster_transformer/src/`."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:212
msgid "`-DWITH_MKL`: 若当前是使用的 mkl 的 Paddle lib，那么需要打开 MKL 以引入 MKL 相关的依赖。"
msgstr ""
"`-DWITH_MKL`: if you use the Paddle lib of mkl, then you need to open MKL to import the relevant dependencies."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:213
msgid "`-DON_INFER`: 是否编译 paddle inference 预测库。"
msgstr "`-DON_INFER`: Whether to compile paddle inference library."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:214
msgid "**当使用预测库的自定义 op 的时候，请务必开启 `-DON_INFER=ON` 选项，否则，不会得到预测库的可执行文件。**"
msgstr "**When using the custom OP, please make sure to set `-DON_INFER=ON`, otherwise the executable file will not be generated.**"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:217
msgid "执行 Transformer decoding on PaddlePaddle"
msgstr "Execute Transformer decoding on PaddlePaddle"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:219
msgid ""
"编译完成后，在 `build/bin/` 路径下将会看到 `transformer_e2e` "
"的一个可执行文件。通过设置对应的设置参数完成执行的过程。"
msgstr ""
"An executable file `transformer_e2e` will be generated under `build/bin/` after compilation. Complete the execution process by setting the corresponding setting parameters."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:226
msgid "举例说明："
msgstr "For example:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:235
msgid "其中："
msgstr "Notice:"

#: ../advanced_guide/fastergeneration/fastertransformer.rst:237
msgid ""
"`decoding_gemm` 不同参数的意义可以参考 `FasterTransformer 文档 "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-"
"decoderdecoding-demos>`_。这里提前执行 `decoding_gemm`，可以在当前路径下生成一个 config "
"文件，里面会包含针对当前 decoding 部分提供的配置下，性能最佳的矩阵乘的算法，并在执行的时候读入这个数据。"
msgstr ""
"For the details of different parameters in `decoding_gemm`, please refer to `FasterTransformer Documentation "
"<https://github.com/NVIDIA/FasterTransformer/tree/v3.1#execute-the-decoderdecoding-demos>`_."
"Here the `decoding_gemm` is executed first, it will generate a config file under the current path. Inside this config file, it contains the best algorithm of matrix multiplication under the current decoding configureation, "
"and this file will be used during execution."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:238
msgid "`DATA_HOME` 则是 `paddlenlp.utils.env.DATA_HOME` 返回的路径。"
msgstr "`DATA_HOME` is the return path of `paddlenlp.utils.env.DATA_HOME`."

#: ../advanced_guide/fastergeneration/fastertransformer.rst:240
msgid ""
"预测所需要的模型文件，可以通过 `faster_transformer/README.md "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/machine_translation/transformer/faster_transformer/README.md>`_"
" 文档中所记述的方式导出。"
msgstr ""
"The model can be exported by the approach mentioned here: `faster_transformer/README.md "
"<https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/machine_translation/transformer/faster_transformer/README.md>`_. "

