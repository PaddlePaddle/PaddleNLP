# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../advanced_guide/model_compression/introduction.rst:3
#: ../advanced_guide/model_compression/introduction.rst:11
msgid "模型压缩简介"
msgstr "Introduction of Model Compression"

#: ../advanced_guide/model_compression/introduction.rst:6
msgid ""
"近些年，基于Transformer的语言模型在机器翻译、阅读理解、文本匹配、自然语言推理等自然语言处理任务上取得了实质\\ "
"进展。然而，海量的参数和计算资源的大量耗费，使BERT及其变体在部署中困难重重。模型压缩的发展，使得这些问题得到\\ 了缓解。"
msgstr ""
"In the recent years, the models based on Transformer have advanced significantly. However, the tromendous amount of parameters and computing resources required by the these models make the deployment "
"of BERT models and their variations hard to achieve. The development of model compression mitigates this issue."

#: ../advanced_guide/model_compression/introduction.rst:13
msgid ""
"模型压缩在保证一定精度的情况下，能够降低模型的存储，加速模型的推理时间。常见的模型压缩方法主要包括模型裁剪、量化和蒸馏。\\ "
"下面分别对这几种方法进行简要的介绍。"
msgstr ""
"The model compression can reduce model size and speed up inference while maintaining an acceptable accuracy. The common ways to compress models are pruning, quantizting and distilling."
"Let's have a look at these approaches."

#: ../advanced_guide/model_compression/introduction.rst:17
msgid "模型裁剪"
msgstr "Model Pruning"

#: ../advanced_guide/model_compression/introduction.rst:18
msgid "模型裁剪是通过对已经训练好的模型中不重要的网络连接进行裁剪，减少模型的冗余和计算量，从而减少网络存储、大幅度进行加速的模型压缩方法。"
msgstr "Model pruning is a model compression method that reduces the redundancy and computational complexity of the model by pruning the unimportant network connections in the trained model, "
"thereby reducing network storage and greatly accelerating the speed of inference."

#: ../advanced_guide/model_compression/introduction.rst:21
msgid "量化"
msgstr "Quantization"

#: ../advanced_guide/model_compression/introduction.rst:22
msgid ""
"一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示。实际上，有时不需要保留那么高的精度，可以通过量化的方法减少\\ "
"模型的存储空间，通常用INT8代替Float32存储。比如，SGD（Stochastic Gradient "
"Descent）所需要的精度仅为6~8bit，\\ "
"因此合理的量化网络也可保证精度的情况下减小模型的存储体积，并且能够大幅度加速，使得神经网络在CPU上的运行成为可能。\\ "
"通常，量化包含多种方法，例如：二值神经网络、三元权重网络以及XNOR网络。"
msgstr ""
"Normally, the parameters used in neural networks are the 32 bits float type. In fact, it is unnecessary to keep such accuracy, quantization can reduce the size of model, "
"Float32 is replaced with INT8 usually. For instance, SGD(Stochastic Gradient Descent) only needs the accuracy to be 6 to 8 bits. Therefore,  it becomes possible to run neural networks on CPU "
"by properly quantizating the networks to reduce size, maintain accuracy as well as speed up inference."

#: ../advanced_guide/model_compression/introduction.rst:29
msgid "蒸馏"
msgstr "Distillation"

#: ../advanced_guide/model_compression/introduction.rst:30
msgid ""
"蒸馏本质是student模型（参数量较少的模型）对teacher模型（参数量较多）的拟合，student模型从teacher中学到知识，比自己单独学习效果更好，。比较常见的方法通常是由Bert"
" base蒸馏到\\ Bi-LSTM或者是Transformer层数更少的BERT小模型。例如DistilBERT，它保留了BERT-base "
"97%的精度，\\ 减少了40%的参数，推理速度快了60%。"
msgstr ""
"The essence of distillation is the fitting of the student model(the model with less parameters) to the teacher model(the model with more parameters. The student model learns knowledge from the teacher, "
"which is better than learning it by itself. The more common method is usually to distill from Bert based models to Bi-LSTM or a small BERT model with fewer Transfomer layers. For example, "
"DistilBERT which retains 97% of ther BERT-base accuracy, reduces its parameters by 40% and inferences 60% faster."

#: ../advanced_guide/model_compression/introduction.rst:36
msgid "模型压缩示例"
msgstr "Model Compression Demo"

#: ../advanced_guide/model_compression/introduction.rst:38
msgid ""
"下面将会对基于飞桨实现的常见的模型压缩示例进行介绍，其中《由BERT到Bi-LSTM的知识蒸馏》可以作为蒸馏实验的\"Hello "
"World\"示例。\\ "
"而《使用DynaBERT中的策略对BERT进行压缩》中使用的DynaBERT则是同时对不同尺寸的子网络进行训练，通过该方法训练后可以在推理阶段直接对模型裁剪。"
msgstr ""
"The following shows common model compression technics used in PaddleNLP. The \"Knowledge Distillation From BERT to Bi-LSTM\" can be used as a \"Hello World\" example. "
"In another example \"Using The Strategy in DynaBERT to Compress BERT\", the DynaBERT trains sub-networks of different sizes simultaneously, after training in this way, "
"the model can be pruned directly at the inference stage."
