# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../advanced_guide/model_compression/introduction.rst:3
#: ../advanced_guide/model_compression/introduction.rst:11
msgid "模型压缩简介"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:6
msgid ""
"近些年，基于Transformer的语言模型在机器翻译、阅读理解、文本匹配、自然语言推理等自然语言处理任务上取得了实质\\ "
"进展。然而，海量的参数和计算资源的大量耗费，使BERT及其变体在部署中困难重重。模型压缩的发展，使得这些问题得到\\ 了缓解。"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:13
msgid ""
"模型压缩在保证一定精度的情况下，能够降低模型的存储，加速模型的推理时间。常见的模型压缩方法主要包括模型裁剪、量化和蒸馏。\\ "
"下面分别对这几种方法进行简要的介绍。"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:17
msgid "模型裁剪"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:18
msgid "模型裁剪是通过对已经训练好的模型中不重要的网络连接进行裁剪，减少模型的冗余和计算量，从而减少网络存储、大幅度进行加速的模型压缩方法。"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:21
msgid "量化"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:22
msgid ""
"一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示。实际上，有时不需要保留那么高的精度，可以通过量化的方法减少\\ "
"模型的存储空间，通常用INT8代替Float32存储。比如，SGD（Stochastic Gradient "
"Descent）所需要的精度仅为6~8bit，\\ "
"因此合理的量化网络也可保证精度的情况下减小模型的存储体积，并且能够大幅度加速，使得神经网络在CPU上的运行成为可能。\\ "
"通常，量化包含多种方法，例如：二值神经网络、三元权重网络以及XNOR网络。"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:29
msgid "蒸馏"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:30
msgid ""
"蒸馏本质是student模型（参数量较少的模型）对teacher模型（参数量较多）的拟合，student模型从teacher中学到知识，比自己单独学习效果更好，。比较常见的方法通常是由Bert"
" base蒸馏到\\ Bi-LSTM或者是Transformer层数更少的BERT小模型。例如DistilBERT，它保留了BERT-base "
"97%的精度，\\ 减少了40%的参数，推理速度快了60%。"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:36
msgid "模型压缩示例"
msgstr ""

#: ../advanced_guide/model_compression/introduction.rst:38
msgid ""
"下面将会对基于飞桨实现的常见的模型压缩示例进行介绍，其中《由BERT到Bi-LSTM的知识蒸馏》可以作为蒸馏实验的\"Hello "
"World\"示例。\\ "
"而《使用DynaBERT中的策略对BERT进行压缩》中使用的DynaBERT则是同时对不同尺寸的子网络进行训练，通过该方法训练后可以在推理阶段直接对模型裁剪。"
msgstr ""

