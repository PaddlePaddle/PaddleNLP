# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../advanced_guide/model_compression/distill_lstm.rst:2
msgid "由BERT到Bi-LSTM的知识蒸馏"
msgstr "The Knowledge Distillation From BERT To Bi-LSTM>"

#: ../../advanced_guide/model_compression/distill_lstm.rst:6
msgid "整体原理介绍"
msgstr "Introduction of The Big Picture"

#: ../../advanced_guide/model_compression/distill_lstm.rst:8
msgid ""
"本例是将特定任务下BERT模型的知识蒸馏到基于Bi-LSTM的小模型中，主要参考论文 `Distilling Task-Specific "
"Knowledge from BERT into Simple Neural Networks "
"<https://arxiv.org/abs/1903.12136>`_ \\ 实现。整体原理如下："
msgstr ""
"This example is to distill the BERT model under a specific task into a smaller Bi-LSTM based model."
"The main reference paper is the implementation of `Distilling Task-Specific Knowledge from BERT into Simple Neural Networks <https://arxiv.org/abs/1903.12136>`_ \\"
"The overall principle is as follows:"


#: ../../advanced_guide/model_compression/distill_lstm.rst:11
msgid "在本例中，较大的模型是BERT被称为教师模型，Bi-LSTM被称为学生模型。"
msgstr ""
"In this example, the bigger model is the BERT, called teacher model, whereas Bi-LSTM is called student model."

#: ../../advanced_guide/model_compression/distill_lstm.rst:13
msgid "小模型学习大模型的知识，需要小模型学习蒸馏相关的损失函数。在本实验中，损失函数是均方误差损失函数，传入函数的两个参数分别是学生模型的输出和教师模型的输出。"
msgstr ""
"In order to learn from the bigger model, the smaller model needs to learn the loss functions related to distillation. In this particular example, "
"the loss function is the Mean Squared Error(MSE). The inputs of the loss function are the outputs from student and teacher models."

#: ../../advanced_guide/model_compression/distill_lstm.rst:15
msgid ""
"在论文的模型蒸馏阶段，作者为了能让教师模型表达出更多的“暗知识”(dark "
"knowledge，通常指分类任务中低概率类别与高概率类别的关系)供学生模型学习，对训练数据进行了数据增强。通过数据增强，可以产生更多无标签的训练数据，在训练过程中，学生模型可借助教师模型的“暗知识”，在更大的数据集上进行训练，产生更好的蒸馏效果。本文的作者使用了三种数据增强方式，分别是："
msgstr ""
"In the process of model distillation inside the paper, in order to trigger the teacher model to express more \"dark knowledge\"(dark knowledge , usually referrring to the relationship between low-probability categories and high-probability categoeris in classification tasks) "
"for the student model to learn, the training data is therefore augmented. Through the augmentation, more unlablled training data is generated, so during the training process, the student model can "
"be trained on a larger dataset from the \"dark knowledge\" thus yield a better result of distillation. The author in this article adopts 3 augmentation methods, they are:"

#: ../../advanced_guide/model_compression/distill_lstm.rst:17
msgid "Masking，即以一定的概率将原数据中的word token替换成 ``[MASK]`` ；"
msgstr ""
"Masking, meaning replacing word tokens in the original dataset with ``[MASK]`` with a certain probablity;"

#: ../../advanced_guide/model_compression/distill_lstm.rst:19
msgid "POS—guided word replacement，即以一定的概率将原数据中的词用与其有相同POS tag的词替换；"
msgstr ""
"POS-guided word replacement, this is, replace the words i the original data with words with same POS tag with a certain probability;"

#: ../../advanced_guide/model_compression/distill_lstm.rst:21
msgid "n-gram sampling，即以一定的概率，从每条数据中采样n-gram，其中n的范围可通过人工设置。"
msgstr ""
"n-gram sampling, it is to sample n-gram from each piece of data with a certain probability, where the range of n can be manually defined."

#: ../../advanced_guide/model_compression/distill_lstm.rst:26
msgid "模型蒸馏步骤介绍"
msgstr ""
"Introductin of Model Distillation Steps"

#: ../../advanced_guide/model_compression/distill_lstm.rst:28
msgid ""
"本实验分为三个训练过程：在特定任务上对BERT进行微调、在特定任务上对基于Bi-LSTM的小模型进行训练（用于评价蒸馏效果"
"）、将BERT模型的知识蒸馏到基于Bi-LSTM的小模型上。"
msgstr ""
"There are 3 steps in this experiment: fine-tuning BERT for specific tasks, training a small Bi-LSTM based model on a specific task(for evaluating the distillation effect),"
"distill the knowledge of the BERT model to a small Bi-LSTM based model."

#: ../../advanced_guide/model_compression/distill_lstm.rst:31
msgid "1. 基于bert-base-uncased预训练模型在特定任务上进行微调"
msgstr ""
"1. Fine-tune on specific tasks based on the bert-base-uncased pretrained models."

#: ../../advanced_guide/model_compression/distill_lstm.rst:33
msgid ""
"训练BERT的fine-tuning模型，可以去 `PaddleNLP "
"<https:github.com/PaddlePaddle/PaddleNLP>`_ 中\\ 的 `glue "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/benchmark/glue>`_"
" 目录下对bert-base-uncased做微调。"
msgstr ""
"Train the BERT based fine-tuning model(bert-base-uncased), it can be found under the `glue <https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/benchmark/glue>`_ in `PaddleNLP <https:github.com/PaddlePaddle/PaddleNLP>`_ ."

#: ../../advanced_guide/model_compression/distill_lstm.rst:36
msgid ""
"以GLUE的SST-2任务为例，用bert-base-"
"uncased做微调之后，可以得到一个在SST-2任务上的教师模型，可以把在dev上取得最好Accuracy的模型保存下来，用于第三步的蒸馏。"
msgstr ""
"Take the task of SST-2 under GLUE, after fine-tuning with bert-base-uncased, we can get a teacher model from the SST-2, "
"then the model with the highest accuracy trained from the dev is saved for the distillation in the step 3."

#: ../../advanced_guide/model_compression/distill_lstm.rst:40
msgid "2. 训练基于Bi-LSTM的小模型"
msgstr "2. Training the small and Bi-LSTM based model"

#: ../../advanced_guide/model_compression/distill_lstm.rst:42
msgid ""
"在本示例中，小模型采取的是基于双向LSTM的分类模型，网络层分别是 ``Embedding`` 、``LSTM`` 、 带有 ``tanh`` "
"激活函数的 ``Linear`` 层，最后经过\\ 一个全连接的输出层得到logits。``LSTM`` 网络层定义如下："
msgstr ""
"In this example, the small model adopts a bi-directional LSTM-based classification model and the layers are: ``Embedding``, ``LSTM``, and ``Linear`` layer with ``tanh`` activation function,"
"Finally,\\ logits are obtrained through a fully connected output layer. The definition of ``LSTM`` layer as follows:"

#: ../../advanced_guide/model_compression/distill_lstm.rst:50
msgid "基于Bi-LSTM的小模型的 ``forward`` 函数定义如下："
msgstr ""
"The ``forward`` function in small Bi-LSTM based model:"

#: ../../advanced_guide/model_compression/distill_lstm.rst:66
msgid "3.数据增强介绍"
msgstr "3. Introduction of Data Augmentation"

#: ../../advanced_guide/model_compression/distill_lstm.rst:68
msgid ""
"接下来的蒸馏过程，蒸馏时使用的训练数据集并不只包含数据集中原有的数据，而是按照上文原理介绍中的A、C两种方法进行数据增强后的总数据。 "
"在多数情况下，``alpha`` 会被设置为0，表示无视硬标签，学生模型只利用数据增强后的无标签数据进行训练。根据教师模型提供的软标签 "
"``teacher_logits`` \\ ，对比学生模型的 ``logits`` "
"，计算均方误差损失。由于数据增强过程产生了更多的数据，学生模型可以从教师模型中学到更多的暗知识。"
msgstr ""
"In the following distillation process, the training dataset used in distillation does not only contain the original data, "
"but also the entire data obtained after the data augmentation according to the 2 methods A and C in the aforementioned principle."
"In most cases, ``alpha`` is set to 0, which means the hard labels are ignored, so the student model is only trained with the unlablled augmented data. "
"Then according to the soft lables ``teacher_logits`` provided from the teacher model, the student ``logits`` is calculated with the MSE. Due to the extra data generated during the process of data augmentation, "
"student model can learn more dark knowledge from the teacher model."

#: ../../advanced_guide/model_compression/distill_lstm.rst:72
msgid "数据增强的核心代码如下："
msgstr ""
"The core implementation of data augmentation as follow:"

#: ../../advanced_guide/model_compression/distill_lstm.rst:107
msgid "4.蒸馏模型"
msgstr "4. Distillation of Model"

#: ../../advanced_guide/model_compression/distill_lstm.rst:109
msgid ""
"这一步是将教师模型BERT的知识蒸馏到基于Bi-LSTM的学生模型中，在本例中，主要是让学生模型（Bi-"
"LSTM）去学习教师模型的输出logits。\\ 蒸馏时使用的训练数据集是由上一步数据增强后的数据，核心代码如下："
msgstr ""
"In this step, the knowledge from the teacher BERT model is distilled to the Bi-LSTM student model. In this example, "
"it is mainly for studetn model(Bi-LSTM) to learn the output logits from the teacher model. \\ "
"The data used during the distillation come from the last step, the main code of implement as follow:"