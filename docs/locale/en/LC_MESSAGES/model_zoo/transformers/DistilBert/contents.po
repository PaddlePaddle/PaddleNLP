# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-19 14:17+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.1\n"

#: ../model_zoo/transformers/DistilBert/contents.rst:5
msgid "DistilBERT模型汇总"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:9
msgid "下表汇总介绍了目前PaddleNLP支持的DistilBERT模型对应预训练权重。 关于模型的具体细节可以参考对应链接。"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:13
msgid "Pretrained Weight"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:13
msgid "Language"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:13
msgid "Details of the model"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:15
msgid "``distilbert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:15
#: ../model_zoo/transformers/DistilBert/contents.rst:20
#: ../model_zoo/transformers/DistilBert/contents.rst:25
#: ../model_zoo/transformers/DistilBert/contents.rst:30
msgid "English"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:15
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-uncased``."
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:20
msgid "``distilbert-base-cased``"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:20
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-cased``."
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:25
msgid "``renmada/distilbert-base-multilingual-cased``"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:25
msgid ""
"6-layer, 768-hidden, 12-heads, 200M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-multilingual-cased``."
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:30
msgid "``renmada/sshleifer-tiny-distilbert-base-uncase-finetuned-sst-2-english``"
msgstr ""

#: ../model_zoo/transformers/DistilBert/contents.rst:30
msgid "2-layer, 2-hidden, 2-heads, 50K parameters. The DistilBERT model."
msgstr ""

