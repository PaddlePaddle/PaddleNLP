# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-19 14:17+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.1\n"

#: ../model_zoo/transformers/all/transformers.rst:2
msgid "PaddleNLP Transformer API"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:4
msgid ""
"随着深度学习的发展，NLP领域涌现了一大批高质量的Transformer类预训练模型，多次刷新各种NLP任务SOTA（State of the "
"Art）。 PaddleNLP为用户提供了常用的 "
"``BERT``、``ERNIE``、``ALBERT``、``RoBERTa``、``XLNet`` 等经典结构预训练模型， "
"让开发者能够方便快捷应用各类Transformer预训练模型及其下游任务。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:10
msgid "Transformer预训练模型汇总"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:14
msgid ""
"下表汇总了介绍了目前PaddleNLP支持的各类预训练模型以及对应预训练权重。我们目前提供了 **32** 种网络结构， **136** "
"种预训练的参数权重供用户使用， 其中包含了 **59** 种中文语言模型的预训练权重。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:18
#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:18
msgid "Pretrained Weight"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:18
msgid "Language"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:18
msgid "Details of the model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:20
#: ../model_zoo/transformers/all/transformers.rst:666
msgid "ALBERT_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:20
msgid "``albert-base-v1``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:20
#: ../model_zoo/transformers/all/transformers.rst:24
#: ../model_zoo/transformers/all/transformers.rst:28
#: ../model_zoo/transformers/all/transformers.rst:32
#: ../model_zoo/transformers/all/transformers.rst:36
#: ../model_zoo/transformers/all/transformers.rst:40
#: ../model_zoo/transformers/all/transformers.rst:44
#: ../model_zoo/transformers/all/transformers.rst:48
#: ../model_zoo/transformers/all/transformers.rst:76
#: ../model_zoo/transformers/all/transformers.rst:80
#: ../model_zoo/transformers/all/transformers.rst:84
#: ../model_zoo/transformers/all/transformers.rst:88
#: ../model_zoo/transformers/all/transformers.rst:92
#: ../model_zoo/transformers/all/transformers.rst:96
#: ../model_zoo/transformers/all/transformers.rst:148
#: ../model_zoo/transformers/all/transformers.rst:196
#: ../model_zoo/transformers/all/transformers.rst:200
#: ../model_zoo/transformers/all/transformers.rst:204
#: ../model_zoo/transformers/all/transformers.rst:208
#: ../model_zoo/transformers/all/transformers.rst:212
#: ../model_zoo/transformers/all/transformers.rst:216
#: ../model_zoo/transformers/all/transformers.rst:220
#: ../model_zoo/transformers/all/transformers.rst:224
#: ../model_zoo/transformers/all/transformers.rst:228
#: ../model_zoo/transformers/all/transformers.rst:232
#: ../model_zoo/transformers/all/transformers.rst:236
#: ../model_zoo/transformers/all/transformers.rst:241
#: ../model_zoo/transformers/all/transformers.rst:246
#: ../model_zoo/transformers/all/transformers.rst:252
#: ../model_zoo/transformers/all/transformers.rst:256
#: ../model_zoo/transformers/all/transformers.rst:260
#: ../model_zoo/transformers/all/transformers.rst:264
#: ../model_zoo/transformers/all/transformers.rst:300
#: ../model_zoo/transformers/all/transformers.rst:304
#: ../model_zoo/transformers/all/transformers.rst:308
#: ../model_zoo/transformers/all/transformers.rst:316
#: ../model_zoo/transformers/all/transformers.rst:320
#: ../model_zoo/transformers/all/transformers.rst:324
#: ../model_zoo/transformers/all/transformers.rst:328
#: ../model_zoo/transformers/all/transformers.rst:351
#: ../model_zoo/transformers/all/transformers.rst:355
#: ../model_zoo/transformers/all/transformers.rst:359
#: ../model_zoo/transformers/all/transformers.rst:363
#: ../model_zoo/transformers/all/transformers.rst:367
#: ../model_zoo/transformers/all/transformers.rst:371
#: ../model_zoo/transformers/all/transformers.rst:375
#: ../model_zoo/transformers/all/transformers.rst:379
#: ../model_zoo/transformers/all/transformers.rst:387
#: ../model_zoo/transformers/all/transformers.rst:391
#: ../model_zoo/transformers/all/transformers.rst:395
#: ../model_zoo/transformers/all/transformers.rst:399
#: ../model_zoo/transformers/all/transformers.rst:403
#: ../model_zoo/transformers/all/transformers.rst:407
#: ../model_zoo/transformers/all/transformers.rst:411
#: ../model_zoo/transformers/all/transformers.rst:415
#: ../model_zoo/transformers/all/transformers.rst:420
#: ../model_zoo/transformers/all/transformers.rst:425
#: ../model_zoo/transformers/all/transformers.rst:430
#: ../model_zoo/transformers/all/transformers.rst:434
#: ../model_zoo/transformers/all/transformers.rst:454
#: ../model_zoo/transformers/all/transformers.rst:457
#: ../model_zoo/transformers/all/transformers.rst:476
#: ../model_zoo/transformers/all/transformers.rst:480
#: ../model_zoo/transformers/all/transformers.rst:484
#: ../model_zoo/transformers/all/transformers.rst:488
#: ../model_zoo/transformers/all/transformers.rst:536
#: ../model_zoo/transformers/all/transformers.rst:540
#: ../model_zoo/transformers/all/transformers.rst:549
#: ../model_zoo/transformers/all/transformers.rst:554
#: ../model_zoo/transformers/all/transformers.rst:559
#: ../model_zoo/transformers/all/transformers.rst:563
#: ../model_zoo/transformers/all/transformers.rst:567
#: ../model_zoo/transformers/all/transformers.rst:571
#: ../model_zoo/transformers/all/transformers.rst:575
#: ../model_zoo/transformers/all/transformers.rst:579
#: ../model_zoo/transformers/all/transformers.rst:583
#: ../model_zoo/transformers/all/transformers.rst:588
#: ../model_zoo/transformers/all/transformers.rst:593
#: ../model_zoo/transformers/all/transformers.rst:598
#: ../model_zoo/transformers/all/transformers.rst:637
#: ../model_zoo/transformers/all/transformers.rst:641
msgid "English"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:20
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters."
" ALBERT base model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:24
msgid "``albert-large-v1``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:24
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M "
"parameters. ALBERT large model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:28
msgid "``albert-xlarge-v1``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:28
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M "
"parameters. ALBERT xlarge model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:32
msgid "``albert-xxlarge-v1``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:32
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 64-heads, 223M "
"parameters. ALBERT xxlarge model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:36
msgid "``albert-base-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:36
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters."
" ALBERT base model (version2)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:40
msgid "``albert-large-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:40
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M "
"parameters. ALBERT large model (version2)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:44
msgid "``albert-xlarge-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:44
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M "
"parameters. ALBERT xlarge model (version2)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:48
msgid "``albert-xxlarge-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:48
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 64-heads, 223M "
"parameters. ALBERT xxlarge model (version2)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:52
msgid "``albert-chinese-tiny``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:52
#: ../model_zoo/transformers/all/transformers.rst:56
#: ../model_zoo/transformers/all/transformers.rst:60
#: ../model_zoo/transformers/all/transformers.rst:64
#: ../model_zoo/transformers/all/transformers.rst:68
#: ../model_zoo/transformers/all/transformers.rst:72
#: ../model_zoo/transformers/all/transformers.rst:112
#: ../model_zoo/transformers/all/transformers.rst:117
#: ../model_zoo/transformers/all/transformers.rst:123
#: ../model_zoo/transformers/all/transformers.rst:129
#: ../model_zoo/transformers/all/transformers.rst:133
#: ../model_zoo/transformers/all/transformers.rst:137
#: ../model_zoo/transformers/all/transformers.rst:154
#: ../model_zoo/transformers/all/transformers.rst:159
#: ../model_zoo/transformers/all/transformers.rst:164
#: ../model_zoo/transformers/all/transformers.rst:169
#: ../model_zoo/transformers/all/transformers.rst:173
#: ../model_zoo/transformers/all/transformers.rst:268
#: ../model_zoo/transformers/all/transformers.rst:272
#: ../model_zoo/transformers/all/transformers.rst:276
#: ../model_zoo/transformers/all/transformers.rst:280
#: ../model_zoo/transformers/all/transformers.rst:284
#: ../model_zoo/transformers/all/transformers.rst:288
#: ../model_zoo/transformers/all/transformers.rst:292
#: ../model_zoo/transformers/all/transformers.rst:296
#: ../model_zoo/transformers/all/transformers.rst:312
#: ../model_zoo/transformers/all/transformers.rst:333
#: ../model_zoo/transformers/all/transformers.rst:337
#: ../model_zoo/transformers/all/transformers.rst:342
#: ../model_zoo/transformers/all/transformers.rst:346
#: ../model_zoo/transformers/all/transformers.rst:383
#: ../model_zoo/transformers/all/transformers.rst:438
#: ../model_zoo/transformers/all/transformers.rst:442
#: ../model_zoo/transformers/all/transformers.rst:446
#: ../model_zoo/transformers/all/transformers.rst:450
#: ../model_zoo/transformers/all/transformers.rst:460
#: ../model_zoo/transformers/all/transformers.rst:465
#: ../model_zoo/transformers/all/transformers.rst:470
#: ../model_zoo/transformers/all/transformers.rst:473
#: ../model_zoo/transformers/all/transformers.rst:492
#: ../model_zoo/transformers/all/transformers.rst:496
#: ../model_zoo/transformers/all/transformers.rst:500
#: ../model_zoo/transformers/all/transformers.rst:504
#: ../model_zoo/transformers/all/transformers.rst:508
#: ../model_zoo/transformers/all/transformers.rst:512
#: ../model_zoo/transformers/all/transformers.rst:516
#: ../model_zoo/transformers/all/transformers.rst:520
#: ../model_zoo/transformers/all/transformers.rst:524
#: ../model_zoo/transformers/all/transformers.rst:528
#: ../model_zoo/transformers/all/transformers.rst:532
#: ../model_zoo/transformers/all/transformers.rst:544
#: ../model_zoo/transformers/all/transformers.rst:603
#: ../model_zoo/transformers/all/transformers.rst:608
#: ../model_zoo/transformers/all/transformers.rst:613
#: ../model_zoo/transformers/all/transformers.rst:617
#: ../model_zoo/transformers/all/transformers.rst:621
#: ../model_zoo/transformers/all/transformers.rst:625
#: ../model_zoo/transformers/all/transformers.rst:629
#: ../model_zoo/transformers/all/transformers.rst:633
#: ../model_zoo/transformers/all/transformers.rst:645
#: ../model_zoo/transformers/all/transformers.rst:649
#: ../model_zoo/transformers/all/transformers.rst:653
msgid "Chinese"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:52
msgid ""
"4 repeating layers, 128 embedding, 312-hidden, 12-heads, 4M parameters. "
"ALBERT tiny model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:56
msgid "``albert-chinese-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:56
msgid ""
"6 repeating layers, 128 embedding, 384-hidden, 12-heads, _M parameters. "
"ALBERT small model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:60
msgid "``albert-chinese-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:60
msgid ""
"12 repeating layers, 128 embedding, 768-hidden, 12-heads, 12M parameters."
" ALBERT base model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:64
msgid "``albert-chinese-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:64
msgid ""
"24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 18M "
"parameters. ALBERT large model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:68
msgid "``albert-chinese-xlarge``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:68
msgid ""
"24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 60M "
"parameters. ALBERT xlarge model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:72
msgid "``albert-chinese-xxlarge``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:72
msgid ""
"12 repeating layers, 128 embedding, 4096-hidden, 16-heads, 235M "
"parameters. ALBERT xxlarge model (Chinese)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:76
#: ../model_zoo/transformers/all/transformers.rst:668
msgid "BART_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:76
msgid "``bart-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:76
msgid "12-layer, 768-hidden, 12-heads, 217M parameters. BART base model (English)"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:80
msgid "``bart-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:80
msgid ""
"24-layer, 768-hidden, 16-heads, 509M parameters. BART large model "
"(English)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:84
#: ../model_zoo/transformers/all/transformers.rst:670
msgid "BERT_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:84
msgid "``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:84
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:88
msgid "``bert-large-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:88
#: ../model_zoo/transformers/all/transformers.rst:308
#: ../model_zoo/transformers/all/transformers.rst:324
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:92
msgid "``bert-base-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:92
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English"
" text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:96
msgid "``bert-large-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:96
msgid ""
"24-layer, 1024-hidden, 16-heads, 335M parameters. Trained on cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:100
msgid "``bert-base-multilingual-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:100
#: ../model_zoo/transformers/all/transformers.rst:106
#: ../model_zoo/transformers/all/transformers.rst:141
msgid "Multilingual"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:100
msgid ""
"12-layer, 768-hidden, 12-heads, 168M parameters. Trained on lower-cased "
"text in the top 102 languages with the largest Wikipedias."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:106
msgid "``bert-base-multilingual-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:106
msgid ""
"12-layer, 768-hidden, 12-heads, 179M parameters. Trained on cased text in"
" the top 104 languages with the largest Wikipedias."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:112
msgid "``bert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:112
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:117
msgid "``bert-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:117
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:123
msgid "``bert-wwm-ext-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:123
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on cased Chinese"
" Simplified and Traditional text using Whole-Word-Masking with extented "
"data."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:129
msgid "``junnyu/ckiplab-bert-base-chinese-ner``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:129
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on NER task."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:133
msgid "``junnyu/ckiplab-bert-base-chinese-pos``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:133
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on POS task."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:137
msgid "``junnyu/ckiplab-bert-base-chinese-ws``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:137
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Finetuned on WS task."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:141
msgid "``junnyu/nlptown-bert-base-multilingual-uncased-sentiment``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:141
msgid ""
"12-layer, 768-hidden, 12-heads, 167M parameters. Finetuned for sentiment "
"analysis on product reviews in six languages: English, Dutch, German, "
"French, Spanish and Italian."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:148
msgid "``junnyu/tbs17-MathBERT``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:148
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on pre-k to "
"graduate math language (English) using a masked language modeling (MLM) "
"objective."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:154
msgid "``macbert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:154
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained with novel MLM "
"as correction pre-training task."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:159
msgid "``macbert-large-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:159
msgid ""
"24-layer, 1024-hidden, 16-heads, 326M parameters. Trained with novel MLM "
"as correction pre-training task."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:164
msgid "``simbert-base-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:164
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on 22 million "
"pairs of similar sentences crawed from Baidu Know."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:169
msgid "``Langboat/mengzi-bert-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:169
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained on 300G Chinese "
"Corpus Datasets."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:173
msgid "``Langboat/mengzi-bert-base-fin``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:173
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained on 20G Finacial "
"Corpus, based on ``Langboat/mengzi-bert-base``."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:178
msgid "BERT-Japanese_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:178
msgid "``iverxin/bert-base-japanese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:178
#: ../model_zoo/transformers/all/transformers.rst:182
#: ../model_zoo/transformers/all/transformers.rst:187
#: ../model_zoo/transformers/all/transformers.rst:191
msgid "Japanese"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:178
msgid "12-layer, 768-hidden, 12-heads, 110M parameters. Trained on Japanese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:182
msgid "``iverxin/bert-base-japanese-whole-word-masking``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:182
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. Trained on Japanese text"
" using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:187
msgid "``iverxin/bert-base-japanese-char``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:187
msgid ""
"12-layer, 768-hidden, 12-heads, 89M parameters. Trained on Japanese char "
"text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:191
msgid "``iverxin/bert-base-japanese-char-whole-word-masking``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:191
msgid ""
"12-layer, 768-hidden, 12-heads, 89M parameters. Trained on Japanese char "
"text using Whole-Word-Masking."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:196
#: ../model_zoo/transformers/all/transformers.rst:672
msgid "BigBird_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:196
msgid "``bigbird-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:196
msgid ""
"12-layer, 768-hidden, 12-heads, 127M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:200
#: ../model_zoo/transformers/all/transformers.rst:674
msgid "Blenderbot_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:200
msgid "``blenderbot-3B``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:200
msgid "26-layer, 32-heads, 3B parameters. The Blenderbot base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:204
msgid "``blenderbot-400M-distill``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:204
msgid ""
"14-layer, 384-hidden, 32-heads, 400M parameters. The Blenderbot distil "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:208
msgid "``blenderbot-1B-distill``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:208
msgid "14-layer, 32-heads, 1478M parameters. The Blenderbot Distil 1B model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:212
#: ../model_zoo/transformers/all/transformers.rst:676
msgid "Blenderbot-Small_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:212
msgid "``blenderbot_small-90M``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:212
msgid "16-layer, 16-heads, 90M parameters. The Blenderbot small model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:216
#: ../model_zoo/transformers/all/transformers.rst:678
msgid "ConvBert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:216
msgid "``convbert-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:216
msgid "12-layer, 768-hidden, 12-heads, 106M parameters. The ConvBERT base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:220
msgid "``convbert-medium-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:220
msgid ""
"12-layer, 384-hidden, 8-heads, 17M parameters. The ConvBERT medium small "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:224
msgid "``convbert-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:224
msgid "12-layer, 128-hidden, 4-heads, 13M parameters. The ConvBERT small model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:228
#: ../model_zoo/transformers/all/transformers.rst:680
msgid "CTRL_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:228
msgid "``ctrl``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:228
msgid "48-layer, 1280-hidden, 16-heads, 1701M parameters. The CTRL base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:232
msgid "``sshleifer-tiny-ctrl``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:232
msgid "2-layer, 16-hidden, 2-heads, 5M parameters. The Tiny CTRL model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:236
#: ../model_zoo/transformers/all/transformers.rst:682
msgid "DistilBert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:236
msgid "``distilbert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:236
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:241
msgid "``distilbert-base-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:241
msgid ""
"6-layer, 768-hidden, 12-heads, 66M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:246
msgid "``distilbert-base-multilingual-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:246
msgid ""
"6-layer, 768-hidden, 12-heads, 200M parameters. The DistilBERT model "
"distilled from the BERT model ``bert-base-multilingual-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:252
msgid "``sshleifer-tiny-distilbert-base-uncase-finetuned-sst-2-english``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:252
msgid "2-layer, 2-hidden, 2-heads, 50K parameters. The DistilBERT model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:256
#: ../model_zoo/transformers/all/transformers.rst:684
msgid "ELECTRA_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:256
msgid "``electra-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:256
msgid ""
"12-layer, 768-hidden, 4-heads, 14M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:260
msgid "``electra-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:260
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:264
msgid "``electra-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:264
msgid ""
"24-layer, 1024-hidden, 16-heads, 334M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:268
msgid "``chinese-electra-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:268
msgid "12-layer, 768-hidden, 4-heads, 12M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:272
msgid "``chinese-electra-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:272
#: ../model_zoo/transformers/all/transformers.rst:496
msgid "12-layer, 768-hidden, 12-heads, 102M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:276
msgid "``ernie-health-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:276
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained on Chinese "
"medical corpus."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:280
msgid "``junnyu/hfl-chinese-electra-180g-base-discriminator``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:280
msgid ""
"Discriminator, 12-layer, 768-hidden, 12-heads, 102M parameters. Trained "
"on 180g Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:284
msgid "``junnyu/hfl-chinese-electra-180g-small-ex-discriminator``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:284
msgid ""
"Discriminator, 24-layer, 256-hidden, 4-heads, 24M parameters. Trained on "
"180g Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:288
msgid "``junnyu/hfl-chinese-legal-electra-small-generator``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:288
msgid ""
"Generator, 12-layer, 64-hidden, 1-heads, 3M parameters. Trained on "
"Chinese legal corpus."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:292
#: ../model_zoo/transformers/all/transformers.rst:686
msgid "ERNIE_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:292
msgid "``ernie-3.0-medium-zh``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:292
#: ../model_zoo/transformers/all/transformers.rst:312
#: ../model_zoo/transformers/all/transformers.rst:333
#: ../model_zoo/transformers/all/transformers.rst:438
#: ../model_zoo/transformers/all/transformers.rst:613
msgid "12-layer, 768-hidden, 12-heads, 108M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:296
msgid "``ernie-tiny``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:296
msgid "3-layer, 1024-hidden, 16-heads, _M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:300
msgid "``ernie-2.0-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:300
#: ../model_zoo/transformers/all/transformers.rst:316
msgid ""
"12-layer, 768-hidden, 12-heads, 103M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:304
msgid "``ernie-2.0-en-finetuned-squad``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:304
msgid ""
"12-layer, 768-hidden, 12-heads, 110M parameters. Trained on finetuned "
"squad text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:308
msgid "``ernie-2.0-large-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:312
#: ../model_zoo/transformers/all/transformers.rst:688
msgid "ERNIE-DOC_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:312
msgid "``ernie-doc-base-zh``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:316
msgid "``ernie-doc-base-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:320
#: ../model_zoo/transformers/all/transformers.rst:690
msgid "ERNIE-GEN_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:320
msgid "``ernie-gen-base-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:320
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on lower-cased "
"English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:324
msgid "``ernie-gen-large-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:328
msgid "``ernie-gen-large-en-430g``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:328
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on lower-cased "
"English text. with extended data (430 GB)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:333
#: ../model_zoo/transformers/all/transformers.rst:692
msgid "ERNIE-GRAM_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:333
msgid "``ernie-gram-zh``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:337
msgid "``ernie-gram-zh-finetuned-dureader-robust``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:337
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on Chinese text."
" Then finetuned on dreader-robust"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:342
#: ../model_zoo/transformers/all/transformers.rst:694
msgid "GPT_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:342
msgid "``gpt-cpm-large-cn``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:342
msgid "32-layer, 2560-hidden, 32-heads, 2.6B parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:346
msgid "``gpt-cpm-small-cn-distill``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:346
msgid ""
"12-layer, 768-hidden, 12-heads, 109M parameters. The model distilled from"
" the GPT model ``gpt-cpm-large-cn``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:351
msgid "``gpt2-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:351
msgid "12-layer, 768-hidden, 12-heads, 117M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:355
msgid "``gpt2-medium-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:355
msgid "24-layer, 1024-hidden, 16-heads, 345M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:359
msgid "``gpt2-large-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:359
#: ../model_zoo/transformers/all/transformers.rst:379
msgid "36-layer, 1280-hidden, 20-heads, 774M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:363
msgid "``gpt2-xl-en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:363
msgid ""
"48-layer, 1600-hidden, 25-heads, 1558M parameters. Trained on English "
"text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:367
msgid "``junnyu/distilgpt2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:367
msgid "6-layer, 768-hidden, 12-heads, 81M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:371
msgid "``junnyu/microsoft-DialoGPT-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:371
#: ../model_zoo/transformers/all/transformers.rst:476
msgid "12-layer, 768-hidden, 12-heads, 124M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:375
msgid "``junnyu/microsoft-DialoGPT-medium``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:375
msgid "24-layer, 1024-hidden, 16-heads, 354M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:379
msgid "``junnyu/microsoft-DialoGPT-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:383
msgid "``junnyu/uer-gpt2-chinese-poem``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:383
msgid ""
"12-layer, 768-hidden, 12-heads, 103M parameters. Trained on Chinese "
"poetry corpus."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:387
#: ../model_zoo/transformers/all/transformers.rst:696
msgid "LayoutLM_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:387
msgid "``layoutlm-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:387
msgid ""
"12-layer, 768-hidden, 12-heads, 339M parameters. LayoutLm base uncased "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:391
msgid "``layoutlm-large-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:391
msgid ""
"24-layer, 1024-hidden, 16-heads, 51M parameters. LayoutLm large Uncased "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:395
#: ../model_zoo/transformers/all/transformers.rst:698
msgid "LayoutLMV2_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:395
msgid "``layoutlmv2-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:395
msgid ""
"12-layer, 768-hidden, 12-heads, 200M parameters. LayoutLmv2 base uncased "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:399
msgid "``layoutlmv2-large-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:399
msgid ""
"24-layer, 1024-hidden, 16-heads, _M parameters. LayoutLmv2 large uncased "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:403
#: ../model_zoo/transformers/all/transformers.rst:700
msgid "LayoutXLM_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:403
msgid "``layoutxlm-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:403
msgid ""
"12-layer, 768-hidden, 12-heads, 369M parameters. Layoutxlm base uncased "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:407
msgid "MBart_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:407
msgid "``mbart-large-cc25``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:407
msgid ""
"12-layer, 1024-hidden, 12-heads, 1123M parameters. The ``mbart-large-"
"cc25`` model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:411
msgid "``mbart-large-en-ro``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:411
msgid ""
"12-layer, 768-hidden, 16-heads, 1123M parameters. The ``mbart-large rn-"
"ro`` model ."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:415
msgid "``mbart-large-50-one-to-many-mmt``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:415
msgid ""
"12-layer, 1024-hidden, 16-heads, 1123M parameters. ``mbart-large-50-one-"
"to-many-mmt`` model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:420
msgid "``mbart-large-50-many-to-one-mmt``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:420
msgid ""
"12-layer, 1024-hidden, 16-heads, 1123M parameters. ``mbart-large-50-many-"
"to-one-mmt`` model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:425
msgid "``mbart-large-50-many-to-many-mmt``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:425
msgid ""
"12-layer, 1024-hidden, 16-heads, 1123M parameters. ``mbart-large-50-many-"
"to-many-mmt`` model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:430
msgid "Mobilebert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:430
msgid "``mobilebert-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:430
msgid "24-layer, 512-hidden, 4-heads, 24M parameters. Mobilebert uncased Model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:434
#: ../model_zoo/transformers/all/transformers.rst:706
msgid "MPNet_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:434
msgid "``mpnet-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:434
msgid "12-layer, 768-hidden, 12-heads, 109M parameters. MPNet Base Model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:438
#: ../model_zoo/transformers/all/transformers.rst:708
msgid "NeZha_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:438
msgid "``nezha-base-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:442
msgid "``nezha-large-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:442
#: ../model_zoo/transformers/all/transformers.rst:450
msgid "24-layer, 1024-hidden, 16-heads, 336M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:446
msgid "``nezha-base-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:446
msgid "12-layer, 768-hidden, 16-heads, 108M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:450
msgid "``nezha-large-wwm-chinese``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:454
msgid "Reformer_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:454
msgid "``reformer-enwik8``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:454
msgid "12-layer, 1024-hidden, 8-heads, 148M parameters."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:457
msgid "``reformer-crime-and-punishment``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:457
msgid "6-layer, 256-hidden, 2-heads, 3M parameters."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:460
#: ../model_zoo/transformers/all/transformers.rst:712
msgid "RoBERTa_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:460
msgid "``roberta-wwm-ext``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:460
msgid ""
"12-layer, 768-hidden, 12-heads, 102M parameters. Trained on English Text "
"using Whole-Word-Masking with extended data."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:465
msgid "``roberta-wwm-ext-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:465
msgid ""
"24-layer, 1024-hidden, 16-heads, 325M parameters. Trained on English Text"
" using Whole-Word-Masking with extended data."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:470
msgid "``rbt3``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:470
msgid "3-layer, 768-hidden, 12-heads, 38M parameters."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:473
msgid "``rbtl3``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:473
msgid "3-layer, 1024-hidden, 16-heads, 61M parameters."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:476
msgid "``nosaydomore/deepset-roberta-base-squad2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:480
msgid "``nosaydomore/roberta-en-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:480
msgid "12-layer, 768-hidden, 12-heads, 163M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:484
msgid "``nosaydomore/roberta-en-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:484
msgid "24-layer, 1024-hidden, 16-heads, 408M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:488
msgid "``nosaydomore/sshleifei-tiny-distilroberta-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:488
msgid "2-layer, 2-hidden, 2-heads, 0.25M parameters. Trained on English text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:492
msgid "``nosaydomore/uer-roberta-base-chn-extractive-qa``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:492
#: ../model_zoo/transformers/all/transformers.rst:500
msgid "12-layer, 768-hidden, 12-heads, 101M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:496
msgid "``nosaydomore/uer-roberta-base-ft-chinanews-chn``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:500
msgid "``nosaydomore/uer-roberta-base-ft-cluener2020-chn``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:504
#: ../model_zoo/transformers/all/transformers.rst:714
msgid "RoFormer_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:504
msgid "``roformer-chinese-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:504
msgid ""
"6-layer, 384-hidden, 6-heads, 30M parameters. Roformer Small Chinese "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:508
msgid "``roformer-chinese-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:508
msgid ""
"12-layer, 768-hidden, 12-heads, 124M parameters. Roformer Base Chinese "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:512
msgid "``roformer-chinese-char-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:512
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Char Small"
" model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:516
msgid "``roformer-chinese-char-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:516
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Char "
"Base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:520
msgid "``roformer-chinese-sim-char-ft-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:520
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Char Ft "
"Small model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:524
msgid "``roformer-chinese-sim-char-ft-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:524
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Char Ft "
"Base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:528
msgid "``roformer-chinese-sim-char-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:528
msgid ""
"6-layer, 384-hidden, 6-heads, 15M parameters. Roformer Chinese Sim Char "
"Small model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:532
msgid "``roformer-chinese-sim-char-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:532
msgid ""
"12-layer, 768-hidden, 12-heads, 95M parameters. Roformer Chinese Sim Char"
" Base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:536
msgid "``roformer-english-small-discriminator``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:536
msgid ""
"12-layer, 256-hidden, 4-heads, 13M parameters. Roformer English Small "
"Discriminator."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:540
msgid "``roformer-english-small-generator``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:540
msgid ""
"12-layer, 64-hidden, 1-heads, 5M parameters. Roformer English Small "
"Generator."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:544
#: ../model_zoo/transformers/all/transformers.rst:716
msgid "SKEP_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:544
msgid "``skep_ernie_1.0_large_ch``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:544
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained using the Erine"
" model ``ernie_1.0``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:549
msgid "``skep_ernie_2.0_large_en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:549
msgid ""
"24-layer, 1024-hidden, 16-heads, 336M parameters. Trained using the Erine"
" model ``ernie_2.0_large_en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:554
msgid "``skep_roberta_large_en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:554
msgid ""
"24-layer, 1024-hidden, 16-heads, 355M parameters. Trained using the "
"RoBERTa model ``roberta_large_en``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:559
#: ../model_zoo/transformers/all/transformers.rst:718
msgid "SqueezeBert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:559
msgid "``squeezebert-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:559
msgid "12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Uncased model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:563
msgid "``squeezebert-mnli``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:563
msgid "12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Mnli model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:567
msgid "``squeezebert-mnli-headless``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:567
msgid ""
"12-layer, 768-hidden, 12-heads, 51M parameters. SqueezeBert Mnli Headless"
" model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:571
#: ../model_zoo/transformers/all/transformers.rst:720
msgid "T5_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:571
msgid "``t5-small``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:571
msgid "6-layer, 512-hidden, 8-heads, 93M parameters. T5 small model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:575
msgid "``t5-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:575
msgid "12-layer, 768-hidden, 12-heads, 272M parameters. T5 base model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:579
msgid "``t5-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:579
msgid "24-layer, 1024-hidden, 16-heads, 803M parameters. T5 large model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:583
#: ../model_zoo/transformers/all/transformers.rst:722
msgid "TinyBert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:583
msgid "``tinybert-4l-312d``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:583
#: ../model_zoo/transformers/all/transformers.rst:593
#: ../model_zoo/transformers/all/transformers.rst:603
msgid ""
"4-layer, 312-hidden, 12-heads, 14.5M parameters. The TinyBert model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:588
msgid "``tinybert-6l-768d``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:588
#: ../model_zoo/transformers/all/transformers.rst:598
#: ../model_zoo/transformers/all/transformers.rst:608
msgid ""
"6-layer, 768-hidden, 12-heads, 67M parameters. The TinyBert model "
"distilled from the BERT model ``bert-base-uncased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:593
msgid "``tinybert-4l-312d-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:598
msgid "``tinybert-6l-768d-v2``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:603
msgid "``tinybert-4l-312d-zh``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:608
msgid "``tinybert-6l-768d-zh``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:613
#: ../model_zoo/transformers/all/transformers.rst:724
msgid "UnifiedTransformer_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:613
msgid "``unified_transformer-12L-cn``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:617
msgid "``unified_transformer-12L-cn-luge``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:617
msgid ""
"12-layer, 768-hidden, 12-heads, 108M parameters. Trained on Chinese text "
"(LUGE.ai)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:621
msgid "``plato-mini``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:621
msgid "6-layer, 768-hidden, 12-heads, 66M parameters. Trained on Chinese text."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:625
msgid "UNIMO_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:625
msgid "``unimo-text-1.0``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:625
msgid "12-layer, 768-hidden, 12-heads, 99M parameters. UNIMO-text-1.0 model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:629
msgid "``unimo-text-1.0-lcsts-new``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:629
msgid ""
"12-layer, 768-hidden, 12-heads, 99M parameters. Finetuned on lcsts_new "
"dataset."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:633
msgid "``unimo-text-1.0-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:633
msgid ""
"24-layer, 768-hidden, 16-heads, 316M parameters. UNIMO-text-1.0 large "
"model."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:637
#: ../model_zoo/transformers/all/transformers.rst:726
msgid "XLNet_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:637
msgid "``xlnet-base-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:637
msgid "12-layer, 768-hidden, 12-heads, 110M parameters. XLNet English model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:641
msgid "``xlnet-large-cased``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:641
msgid ""
"24-layer, 1024-hidden, 16-heads, 340M parameters. XLNet Large English "
"model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:645
msgid "``chinese-xlnet-base``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:645
msgid "12-layer, 768-hidden, 12-heads, 117M parameters. XLNet Chinese model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:649
msgid "``chinese-xlnet-mid``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:649
msgid ""
"24-layer, 768-hidden, 12-heads, 209M parameters. XLNet Medium Chinese "
"model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:653
msgid "``chinese-xlnet-large``"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:653
msgid "24-layer, 1024-hidden, 16-heads, _M parameters. XLNet Large Chinese model"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:661
msgid "Transformer预训练模型适用任务汇总"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Sequence Classification"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Token Classification"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Question Answering"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Text Generation"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:664
msgid "Multiple Choice"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:666
#: ../model_zoo/transformers/all/transformers.rst:668
#: ../model_zoo/transformers/all/transformers.rst:670
#: ../model_zoo/transformers/all/transformers.rst:672
#: ../model_zoo/transformers/all/transformers.rst:674
#: ../model_zoo/transformers/all/transformers.rst:676
#: ../model_zoo/transformers/all/transformers.rst:678
#: ../model_zoo/transformers/all/transformers.rst:680
#: ../model_zoo/transformers/all/transformers.rst:682
#: ../model_zoo/transformers/all/transformers.rst:684
#: ../model_zoo/transformers/all/transformers.rst:686
#: ../model_zoo/transformers/all/transformers.rst:688
#: ../model_zoo/transformers/all/transformers.rst:690
#: ../model_zoo/transformers/all/transformers.rst:692
#: ../model_zoo/transformers/all/transformers.rst:694
#: ../model_zoo/transformers/all/transformers.rst:696
#: ../model_zoo/transformers/all/transformers.rst:698
#: ../model_zoo/transformers/all/transformers.rst:700
#: ../model_zoo/transformers/all/transformers.rst:702
#: ../model_zoo/transformers/all/transformers.rst:704
#: ../model_zoo/transformers/all/transformers.rst:706
#: ../model_zoo/transformers/all/transformers.rst:708
#: ../model_zoo/transformers/all/transformers.rst:710
#: ../model_zoo/transformers/all/transformers.rst:712
#: ../model_zoo/transformers/all/transformers.rst:714
#: ../model_zoo/transformers/all/transformers.rst:716
#: ../model_zoo/transformers/all/transformers.rst:718
#: ../model_zoo/transformers/all/transformers.rst:720
#: ../model_zoo/transformers/all/transformers.rst:722
#: ../model_zoo/transformers/all/transformers.rst:724
#: ../model_zoo/transformers/all/transformers.rst:726
msgid "✅"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:666
#: ../model_zoo/transformers/all/transformers.rst:668
#: ../model_zoo/transformers/all/transformers.rst:670
#: ../model_zoo/transformers/all/transformers.rst:672
#: ../model_zoo/transformers/all/transformers.rst:674
#: ../model_zoo/transformers/all/transformers.rst:676
#: ../model_zoo/transformers/all/transformers.rst:680
#: ../model_zoo/transformers/all/transformers.rst:682
#: ../model_zoo/transformers/all/transformers.rst:684
#: ../model_zoo/transformers/all/transformers.rst:686
#: ../model_zoo/transformers/all/transformers.rst:688
#: ../model_zoo/transformers/all/transformers.rst:690
#: ../model_zoo/transformers/all/transformers.rst:692
#: ../model_zoo/transformers/all/transformers.rst:694
#: ../model_zoo/transformers/all/transformers.rst:696
#: ../model_zoo/transformers/all/transformers.rst:698
#: ../model_zoo/transformers/all/transformers.rst:700
#: ../model_zoo/transformers/all/transformers.rst:702
#: ../model_zoo/transformers/all/transformers.rst:704
#: ../model_zoo/transformers/all/transformers.rst:706
#: ../model_zoo/transformers/all/transformers.rst:708
#: ../model_zoo/transformers/all/transformers.rst:710
#: ../model_zoo/transformers/all/transformers.rst:712
#: ../model_zoo/transformers/all/transformers.rst:714
#: ../model_zoo/transformers/all/transformers.rst:716
#: ../model_zoo/transformers/all/transformers.rst:718
#: ../model_zoo/transformers/all/transformers.rst:720
#: ../model_zoo/transformers/all/transformers.rst:722
#: ../model_zoo/transformers/all/transformers.rst:724
#: ../model_zoo/transformers/all/transformers.rst:726
msgid "❌"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:702
msgid "Mbart_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:704
msgid "MobileBert_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:710
msgid "ReFormer_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:765
msgid "预训练模型使用方法"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:767
msgid ""
"PaddleNLP Transformer API在提丰富预训练模型的同时，也降低了用户的使用门槛。 "
"使用Auto模块，可以加载不同网络结构的预训练模型，无需查找 模型对应的类别。只需十几行代码，用户即可完成模型加载和下游任务Fine-"
"tuning。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:806
msgid ""
"上面的代码给出使用预训练模型的简要示例，更完整详细的示例代码， 可以参考：`使用预训练模型Fine-tune完成中文文本分类任务 "
"<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/text_classification/pretrained_models/>`_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:809
msgid "加载数据集：PaddleNLP内置了多种数据集，用户可以一键导入所需的数据集。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:810
msgid ""
"加载预训练模型：PaddleNLP的预训练模型可以很容易地通过 ``from_pretrained()`` 方法加载。 "
"Auto模块（包括AutoModel, AutoTokenizer, 及各种下游任务类）提供了方便易用的接口， "
"无需指定类别，即可调用不同网络结构的预训练模型。 第一个参数是汇总表中对应的 ``Pretrained Weight``，可加载对应的预训练权重。"
" ``AutoModelForSequenceClassification`` 初始化 ``__init__`` 所需的其他参数，如 "
"``num_classes`` 等， 也是通过 ``from_pretrained()`` 传入。``Tokenizer`` 使用同样的 "
"``from_pretrained`` 方法加载。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:816
msgid "通过 ``Dataset`` 的 ``map`` 函数，使用 ``tokenizer`` 将 ``dataset`` 从原始文本处理成模型的输入。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:817
msgid "定义 ``BatchSampler`` 和 ``DataLoader``，shuffle数据、组合Batch。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:818
msgid "定义训练所需的优化器，loss函数等，就可以开始进行模型fine-tune任务。"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:822
msgid "Reference"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:823
msgid ""
"部分中文预训练模型来自： `brightmart/albert_zh "
"<https://github.com/brightmart/albert_zh>`_, `ymcui/Chinese-BERT-wwm "
"<https://github.com/ymcui/Chinese-BERT-wwm>`_, `huawei-noah/Pretrained-"
"Language-Model/TinyBERT <https://github.com/huawei-noah/Pretrained-"
"Language-Model/tree/master/TinyBERT>`_, `ymcui/Chinese-XLNet "
"<https://github.com/ymcui/Chinese-XLNet>`_, "
"`huggingface/xlnet_chinese_large "
"<https://huggingface.co/clue/xlnet_chinese_large>`_, `Knover/luge-"
"dialogue <https://github.com/PaddlePaddle/Knover/tree/luge-dialogue/luge-"
"dialogue>`_, `huawei-noah/Pretrained-Language-Model/NEZHA-PyTorch/ "
"<https://github.com/huawei-noah/Pretrained-Language-Model/tree/master"
"/NEZHA-PyTorch>`_, `ZhuiyiTechnology/simbert "
"<https://github.com/ZhuiyiTechnology/simbert>`_"
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:832
msgid ""
"Lan, Zhenzhong, et al. \"Albert: A lite bert for self-supervised learning"
" of language representations.\" arXiv preprint arXiv:1909.11942 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:833
msgid ""
"Lewis, Mike, et al. \"BART: Denoising Sequence-to-Sequence Pre-training "
"for Natural Language Generation, Translation, and Comprehension.\" arXiv "
"preprint arXiv:1910.13461 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:834
msgid ""
"Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional "
"transformers for language understanding.\" arXiv preprint "
"arXiv:1810.04805 (2018)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:835
msgid ""
"Zaheer, Manzil, et al. \"Big bird: Transformers for longer sequences.\" "
"arXiv preprint arXiv:2007.14062 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:836
msgid ""
"Stephon, Emily, et al. \"Blenderbot: Recipes for building an open-domain "
"chatbot.\" arXiv preprint arXiv:2004.13637 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:837
msgid ""
"Stephon, Emily, et al. \"Blenderbot-Small: Recipes for building an open-"
"domain chatbot.\" arXiv preprint arXiv:2004.13637 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:838
msgid ""
"Zhang, zhengyan, et al. \"CPM: A Large-scale Generative Chinese Pre-"
"trained Language Model.\" arXiv preprint arXiv:2012.00413 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:839
msgid ""
"Jiang, Zihang, et al. \"ConvBERT: Improving BERT with Span-based Dynamic "
"Convolution.\" arXiv preprint arXiv:2008.02496 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:840
msgid ""
"Nitish, Bryan, et al. \"CTRL: A Conditional Transformer Language Model "
"for Controllable Generation.\" arXiv preprint arXiv:1909.05858 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:841
msgid ""
"Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, "
"faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:842
msgid ""
"Clark, Kevin, et al. \"Electra: Pre-training text encoders as "
"discriminators rather than generators.\" arXiv preprint arXiv:2003.10555 "
"(2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:843
msgid ""
"Sun, Yu, et al. \"Ernie: Enhanced representation through knowledge "
"integration.\" arXiv preprint arXiv:1904.09223 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:844
msgid ""
"Xiao, Dongling, et al. \"Ernie-gen: An enhanced multi-flow pre-training "
"and fine-tuning framework for natural language generation.\" arXiv "
"preprint arXiv:2001.11314 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:845
msgid ""
"Xiao, Dongling, et al. \"ERNIE-Gram: Pre-Training with Explicitly N-Gram "
"Masked Language Modeling for Natural Language Understanding.\" arXiv "
"preprint arXiv:2010.12148 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:846
msgid ""
"Radford, Alec, et al. \"Language models are unsupervised multitask "
"learners.\" OpenAI blog 1.8 (2019): 9."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:847
msgid ""
"Xu, Yiheng, et al. \"LayoutLM: Pre-training of Text and Layout for "
"Document Image Understanding.\" arXiv preprint arXiv:1912.13318 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:848
msgid ""
"Xu, Yang, et al. \"LayoutLMv2: Multi-modal Pre-training for Visually-Rich"
" Document Understanding\" arXiv preprint arXiv:2012.14740 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:849
msgid ""
"Xu, Yiheng, et al. \"LayoutXLM: Multimodal Pre-training for Multilingual "
"Visually-rich Document Understanding\" arXiv preprint arXiv:2104.08836 "
"(2021)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:850
msgid ""
"Liu, Yinhan, et al. \"MBart: Multilingual Denoising Pre-training for "
"Neural Machine Translation\" arXiv preprint arXiv:2001.08210 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:851
msgid ""
"Sun, Zhiqing, et al. \"MobileBERT: a Compact Task-Agnostic BERT for "
"Resource-Limited Devices\" arXiv preprint arXiv:2004.02984 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:852
msgid ""
"Song, Kaitao, et al. \"MPNet: Masked and Permuted Pre-training for "
"Language Understanding.\" arXiv preprint arXiv:2004.09297 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:853
msgid ""
"Wei, Junqiu, et al. \"NEZHA: Neural contextualized representation for "
"chinese language understanding.\" arXiv preprint arXiv:1909.00204 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:854
msgid ""
"Kitaev, Nikita, et al. \"Reformer: The efficient Transformer.\" arXiv "
"preprint arXiv:2001.04451 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:855
msgid ""
"Liu, Yinhan, et al. \"Roberta: A robustly optimized bert pretraining "
"approach.\" arXiv preprint arXiv:1907.11692 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:856
msgid ""
"Su Jianlin, et al. \"RoFormer: Enhanced Transformer with Rotary Position "
"Embedding.\" arXiv preprint arXiv:2104.09864 (2021)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:857
msgid ""
"Tian, Hao, et al. \"SKEP: Sentiment knowledge enhanced pre-training for "
"sentiment analysis.\" arXiv preprint arXiv:2005.05635 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:858
msgid ""
"Forrest, ALbert, et al. \"SqueezeBERT: What can computer vision teach NLP"
" about efficient neural networks?\" arXiv preprint arXiv:2006.11316 "
"(2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:859
msgid ""
"Raffel, Colin, et al. \"T5: Exploring the Limits of Transfer Learning "
"with a Unified Text-to-Text Transformer.\" arXiv preprint "
"arXiv:1910.10683 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:860
msgid ""
"Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint "
"arXiv:1706.03762 (2017)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:861
msgid ""
"Jiao, Xiaoqi, et al. \"Tinybert: Distilling bert for natural language "
"understanding.\" arXiv preprint arXiv:1909.10351 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:862
msgid ""
"Bao, Siqi, et al. \"Plato-2: Towards building an open-domain chatbot via "
"curriculum learning.\" arXiv preprint arXiv:2006.16779 (2020)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:863
msgid ""
"Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for "
"language understanding.\" arXiv preprint arXiv:1906.08237 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:864
msgid ""
"Cui, Yiming, et al. \"Pre-training with whole word masking for chinese "
"bert.\" arXiv preprint arXiv:1906.08101 (2019)."
msgstr ""

#: ../model_zoo/transformers/all/transformers.rst:865
msgid ""
"Wang, Quan, et al. “Building Chinese Biomedical Language Models via "
"Multi-Level Text Discrimination.” arXiv preprint arXiv:2110.07244 (2021)."
msgstr ""

