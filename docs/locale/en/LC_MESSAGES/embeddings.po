# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../embeddings.md:1
msgid "PaddleNLP Embedding API"
msgstr "PaddleNLP Embedding API"

#: ../../embeddings.md:3
msgid "介绍"
msgstr "Introduction"

#: ../../embeddings.md:5
msgid "PaddleNLP提供多个开源的预训练词向量模型，用户仅需在使用paddlenlp.embeddings.TokenEmbedding时，指定预训练模型的名称，即可加载相对应的预训练模型。以下为PaddleNLP所支持的预训练Embedding模型，其名称用作paddlenlp.embeddings.TokenEmbedding的参数。"
msgstr ""
"PaddleNLP provides multiple open source pre-training word vector models."
"Users only need to specify the name of the pre-training model when using paddlenlp.embeddings.TokenEmbedding to load the corresponding pre-training model. "
" The following are pre-trained Embedding models supported by PaddleNLP, whose names are used as parameters for PaddleNLP.embeddings.TokenEmbedding."

#: ../../embeddings.md:6
msgid "命名方式为：${训练模型}.${语料}.${词向量类型}.${co-occurrence type}.dim${维度}。"
msgstr "The naming method is: ${training model}.${corpus}.${word vector type}.${co-occurrence type}.dim${dimension}."

#: ../../embeddings.md:7
msgid "模型有三种，分别是Word2Vec(w2v, skip-gram), GloVe(glove)和FastText(fasttext)。"
msgstr "Model has three types, which are Word2Vec(w2v, skip-gram), GloVe(glove), and FastText(fasttext)."

#: ../../embeddings.md:9
msgid "在使用方式这一节中，将介绍如何通过模型名称使用paddlenlp.embeddings.TokenEmbedding加载预训练模型。"
msgstr "In the How To Use section, we will introduce how to use paddlenlp.embeddings.TokenEmbedding to load a pre-trained model by model name."

#: ../../embeddings.md:12
msgid "中文词向量"
msgstr "Chinese word vector"

#: ../../embeddings.md:14
msgid "以下预训练词向量由Chinese-Word-Vectors提供。"
msgstr "The pre-trained word vectors below are provided by Chinese-Word-Vectors."

#: ../../embeddings.md:16
msgid "根据不同类型的上下文为每个语料训练多个目标词向量，第二列开始表示不同类型的上下文。以下为上下文类别："
msgstr ""
"Multiple target word vectors are trained for each corpus according to different types of contexts,"
"the second coloum is different types of contexts. Below are the context categories:"

#: ../../embeddings.md:18
msgid "Word表示训练时目标词预测的上下文是一个Word。"
msgstr "During the training, Word indicates that the context predicted by the target word is a Word."

#: ../../embeddings.md:19
msgid ""
"Word + "
"N-gram表示训练时目标词预测的上下文是一个Word或者Ngram，其中bigram表示2-grams，ngram.1-2表示1-gram或者2-grams。"
msgstr ""
"Word + N-gram indicates that the predicted context is a Word or Ngram,"
"in which, bigram indicates 2-grams, ngram.1-2 indicates 1-gram or 2-gram."

#: ../../embeddings.md:20
msgid ""
"Word + Character表示训练时目标词预测的上下文是一个Word或者Character，其中word-"
"character.char1-2表示上下文是1个或2个Character。"
msgstr ""
"Word + Character indicates that the predicted context is a Word or Character,"
"in which, word-character.char1-2 indicates that the context is 1 or 2 Character. "

#: ../../embeddings.md:21
msgid ""
"Word + Character + Ngram表示训练时目标词预测的上下文是一个Word、Character或者Ngram。bigram-"
"char表示上下文是2-grams或者1个Character。"
msgstr ""
"Word + Character + Ngram indicates that the predicted context is a Word, Character, or Ngram."
"bigram-char indicates that the context is 2-grams or 1 Character. "

#: ../../embeddings.md:107
msgid "特别地，对于百度百科语料，在不同的 Co-occurrence类型下分别提供了目标词与上下文向量："
msgstr "In particular, for the Baidu Baike corpus, target words and context vectors are provided under different Co-occurrence types."


#: ../../embeddings.md:166
msgid "英文词向量"
msgstr "English word vector"

#: ../../embeddings.md:168
msgid "Word2Vec"
msgstr "Word2Vec"

#: ../../embeddings.md:185
msgid "GloVe"
msgstr "GloVe"

#: ../../embeddings.md:218
msgid "FastText"
msgstr "FastText"

#: ../../embeddings.md:239
msgid "使用方式"
msgstr "Instructions"

#: ../../embeddings.md:241
msgid ""
"以上所述的模型名称可直接以参数形式传入padddlenlp.embeddings.TokenEmbedding，加载相对应的模型。比如要加载语料为Wiki2017，通过FastText训练的预训练模型（fasttext"
".wiki-news.target.word-word.dim300.en），只需执行以下代码："
msgstr 
"The model name mentioned above can be directly passed to padddlenlp.embeddings.TokenEmbedding as a parameter to load the corresponding model."
"For example, to load the corpus of Wiki2017, the pre-trained model trained by FastText (fasttext.wiki-news.target.word-word.dim300.en), just need to execute the code below:"

#: ../../embeddings.md:250
msgid "模型信息"
msgstr "Model Information"

#: ../../embeddings.md:574
msgid "致谢"
msgstr "Thanks"

#: ../../embeddings.md:575
msgid "感谢 Chinese-Word-Vectors提供Word2Vec中文预训练词向量。"
msgstr "Thanks to Chinese-Word-Vectors for providing Word2Vec Chinese pre-trained word vectors"

#: ../../embeddings.md:576
msgid "感谢 GloVe Project提供的GloVe英文预训练词向量。"
msgstr "Thanks to the GloVe Project for the GloVe English pre-trained word vectors"

#: ../../embeddings.md:577
msgid "感谢 FastText Project提供的英文预训练词向量。"
msgstr "Thanks to FastText Project for providing English pre-trained word vectors."

#: ../../embeddings.md:579
msgid "参考论文"
msgstr "References"

#: ../../embeddings.md:580
msgid ""
"Li, Shen, et al. \"Analogical reasoning on chinese morphological and "
"semantic relations.\" arXiv preprint arXiv:1805.06504 (2018)."
msgstr ""
"Li, Shen, et al. \"Analogical reasoning on chinese morphological and "
"semantic relations.\" arXiv preprint arXiv:1805.06504 (2018)."

#: ../../embeddings.md:581
msgid ""
"Qiu, Yuanyuan, et al. \"Revisiting correlations between intrinsic and "
"extrinsic evaluations of word embeddings.\" Chinese Computational "
"Linguistics and Natural Language Processing Based on Naturally Annotated "
"Big Data. Springer, Cham, 2018. 209-221."
msgstr ""
"Qiu, Yuanyuan, et al. \"Revisiting correlations between intrinsic and "
"extrinsic evaluations of word embeddings.\" Chinese Computational "
"Linguistics and Natural Language Processing Based on Naturally Annotated "
"Big Data. Springer, Cham, 2018. 209-221."

#: ../../embeddings.md:582
msgid ""
"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. "
"GloVe: Global Vectors for Word Representation."
msgstr ""
"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. "
"GloVe: Global Vectors for Word Representation."

#: ../../embeddings.md:583
msgid ""
"T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin. Advances in "
"Pre-Training Distributed Word Representations."
msgstr ""
"T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin. Advances in "
"Pre-Training Distributed Word Representations."

