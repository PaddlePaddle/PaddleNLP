# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.ops.faster_transformer.transformer.encoder.rst:2
msgid "encoder"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.infer_transformer_encoder:1
msgid ""
"Fusion Encoder API intergrating Encoder inference in FasterTransformer. "
"It accepts the weight and bias of TransformerEncoder and some other "
"parameters for inference."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:1
msgid ""
"Redefines `forward` function of `paddle.nn.TransformerEncoderLayer` for "
"integrating FasterTransformer for inference."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:4
msgid ""
"The original `forward` function would not be replaced unless "
"`enable_faster_encoder` is called by objects of its base class. After "
"replacing, objects of `paddle.nn.TransformerEncoderLayer` also have the "
"same member variables as before."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:9
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:9
msgid ""
"After inference, `disable_faster_encoder` could be called to restore the "
"`forward` function of `paddle.nn.TransformerEncoder` and "
"`paddle.nn.TransformerEncoderLayer`."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.convert_to_fp16
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward
msgid "参数"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:13
msgid ""
"The input of Transformer encoder layer. It is a tensor with shape "
"`[batch_size, sequence_length, d_model]`. The data type should be float32"
" or float64."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:17
msgid ""
"A tensor used in multi-head attention to prevents attention to some "
"unwanted positions, usually the paddings or the subsequent positions. It "
"is a tensor with shape `[batch_size, 1, 1, sequence_length]`. When the "
"data type is bool, the unwanted positions have `False` values and the "
"others have `True` values. When the data type is int, the unwanted "
"positions have 0 values and the others have 1 values. When the data type "
"is float, the unwanted positions have `-INF` values and the others have 0"
" values. It can be None when nothing wanted or needed to be prevented "
"attention to. Defaults to None."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward
msgid "返回"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward:28
msgid ""
"It is a tensor that has the same shape and data type as `enc_input`, "
"representing the output of Transformer encoder layer. Or a tuple if "
"`cache` is not None, except for encoder layer output, the tuple includes "
"the new cache which is same as input `cache` argument but "
"`incremental_cache` has an incremental length. See "
"`paddle.nn.MultiHeadAttention.gen_cache` and "
"`paddle.nn.MultiHeadAttention.forward` for more details."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward
#: paddlenlp.ops.faster_transformer.transformer.encoder.encoder_layer_forward
msgid "返回类型"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:1
msgid ""
"Redefines `forward` function of `paddle.nn.TransformerEncoder` for "
"integrating FasterTransformer for inference."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:4
msgid ""
"The original `forward` function would not be replaced unless "
"`enable_faster_encoder` is called by objects of its base class. After "
"replacing, objects of `paddle.nn.TransformerEncoder` also have the same "
"member variables as before."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:13
msgid ""
"The input of Transformer encoder. It is a tensor with shape `[batch_size,"
" sequence_length, d_model]`. The data type should be float32 or float16."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:17
msgid ""
"A tensor used in multi-head attention to prevents attention to some "
"unwanted positions, usually the paddings or the subsequent positions. It "
"is a tensor with shape `[batch_size, 1, 1, sequence_length]`. The data "
"type must be float, the unwanted positions have `-INF` values or other "
"non-zeros and the wanted positions must be 0.0."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.encoder_forward:24
msgid ""
"It is a tensor that has the same shape and data type as `src`, "
"representing the output of Transformer encoder. Or a tuple if `cache` is "
"not None, except for encoder output, the tuple includes the new cache "
"which is same as input `cache` argument but `incremental_cache` in it has"
" an incremental length. See `paddle.nn.MultiHeadAttention.gen_cache` and "
"`paddle.nn.MultiHeadAttention.forward` for more details."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.enable_faster_encoder:1
msgid ""
"Compiles fusion encoder operator intergrated FasterTransformer using the "
"method of JIT(Just-In-Time) and replaces the `forward` function of "
"`paddle.nn.TransformerEncoder` and `paddle.nn.TransformerEncoderLayer` "
"objects inherited from `self` to support inference using "
"FasterTransformer."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.disable_faster_encoder:5
#: paddlenlp.ops.faster_transformer.transformer.encoder.enable_faster_encoder:7
msgid "实际案例"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.encoder.disable_faster_encoder:1
msgid ""
"Restores the original `forward` function of "
"`paddle.nn.TransformerEncoder` and `paddle.nn.TransformerEncoderLayer` "
"objects inherited from `self`."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.convert_to_fp16:1
msgid "Convert paddle.nn.TransformerEncoder's parameter from float32 to float16"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.encoder.convert_to_fp16:3
msgid ""
"The object to be converted to float16 inplaced, it must be an isinstance "
"of paddle.nn.TransformerEncoder."
msgstr ""

