# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.ernie_ctm.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmPretrainedModel:1
msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmPretrainedModel:1
msgid ""
"An abstract class for pretrained ErnieCtm models. It provides ErnieCtm "
"related `model_config_file`, `pretrained_init_configuration`, "
"`resource_files_names`, `pretrained_resource_files_map`, "
"`base_model_prefix` for downloading"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmPretrainedModel:4
msgid "and loading pretrained models."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmPretrainedModel:5
msgid ""
"See :class:`~paddlenlp.transformers.model_utils.PretrainedModel` for more"
" details."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel:1
msgid "基类：:class:`paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmPretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:1
msgid "The bare ErnieCtm Model transformer outputting raw hidden-states."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:3
msgid ""
"This model inherits from "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. Refer to "
"the superclass documentation for the generic methods."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:6
msgid ""
"This model is also a Paddle `paddle.nn.Layer "
"<https://www.paddlepaddle.org.cn/documentation "
"/docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ subclass. Use "
"it as a regular Paddle Layer and refer to the Paddle documentation for "
"all matter related to general usage and behavior."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:10
msgid ""
"Vocabulary size of `inputs_ids` in `ErnieCtmModel`. Also is the vocab "
"size of token embedding matrix. Defines the number of different tokens "
"that can be represented by the `inputs_ids` passed when calling "
"`ErnieCtmModel`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:13
msgid "Dimensionality of the embedding layer. Defaults to `128`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:16
msgid ""
"Dimensionality of the encoder layers and the pooler layer. Defaults to "
"`768`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:19
msgid "Number of hidden layers in the Transformer encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:21
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:24
msgid ""
"Dimensionality of the feed-forward (ff) layer in the encoder. Input "
"tensors to ff layers are firstly projected from `hidden_size` to "
"`intermediate_size`, and then projected back to `hidden_size`. Typically "
"`intermediate_size` is larger than `hidden_size`. Defaults to `3072`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:29
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:32
msgid ""
"The dropout probability used in MultiHeadAttention in all encoder layers "
"to drop some attention target. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:35
msgid ""
"The maximum value of the dimensionality of position encoding, which "
"dictates the maximum supported length of an input sequence. Defaults to "
"`512`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:38
msgid "The vocabulary size of the `token_type_ids`. Defaults to `16`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:41
msgid ""
"The standard deviation of the normal initializer for initializing all "
"weight matrices. Defaults to `0.02`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:44
msgid "The index of padding token in the token vocabulary. Defaults to `0`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:47
msgid "Whether or not to add content summary tokens. Defaults to `True`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:50
msgid ""
"The number of the content summary tokens. Only valid when "
"use_content_summary is True. Defaults to `1`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel:53
msgid ""
"The number of the CLS tokens. Only valid when use_content_summary is "
"True. Defaults to `2`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:1
msgid "The ErnieCtmModel forward method, overrides the __call__() special method."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:3
msgid ""
"Indices of input sequence tokens in the vocabulary. They are numerical "
"representations of tokens that build the input sequence. It's data type "
"should be `int64` and has a shape of [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:7
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range ``[0, type_vocab_size - 1]``. If `type_vocab_size` "
"is 2, which means the inputs have two portions. Indices can either be 0 "
"or 1:  - 0 corresponds to a *sentence A* token, - 1 corresponds to a "
"*sentence B* token.  Its data type should be `int64` and it has a shape "
"of [batch_size, sequence_length]. Defaults to `None`, which means we "
"don't add segment embeddings."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:7
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range ``[0, type_vocab_size - 1]``. If `type_vocab_size` "
"is 2, which means the inputs have two portions. Indices can either be 0 "
"or 1:"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:12
msgid "0 corresponds to a *sentence A* token,"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:13
msgid "1 corresponds to a *sentence B* token."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:15
msgid ""
"Its data type should be `int64` and it has a shape of [batch_size, "
"sequence_length]. Defaults to `None`, which means we don't add segment "
"embeddings."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:18
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, max_position_embeddings - 1]``. "
"Shape as `[batch_size, num_tokens]` and dtype as int64. Defaults to "
"`None`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:22
msgid ""
"Mask used in multi-head attention to avoid performing attention on to "
"some unwanted positions, usually the paddings or the subsequent "
"positions. Its data type can be int, float and bool. When the data type "
"is bool, the `masked` tokens have `False` values and the others have "
"`True` values. When the data type is int, the `masked` tokens have `0` "
"values and the others have `1` values. When the data type is float, the "
"`masked` tokens have `-INF` values and the others have `0` values. It is "
"a tensor with shape broadcasted to `[batch_size, num_attention_heads, "
"sequence_length, sequence_length]`. For example, its shape can be  "
"[batch_size, sequence_length], [batch_size, sequence_length, "
"sequence_length], [batch_size, num_attention_heads, sequence_length, "
"sequence_length]. We use whole-word-mask in ERNIE, so the whole word will"
" have the same value. For example, \"使用\" as a word, \"使\" and \"用\" will"
" have the same value. Defaults to `None`, which means nothing needed to "
"be prevented attention to."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:35
msgid ""
"Whether the `content_output` is clone from `sequence_output`. If set to "
"`True`, the content_output is clone from sequence_output, which may cause"
" the classification task impact on the sequence labeling task. Defaults "
"to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:40
msgid ""
"Returns tuple (``sequence_output``, ``pooled_output``, "
"``content_output``).  With the fields:  - `sequence_output` (Tensor):"
"     Sequence of output at the last layer of the model. Its data type "
"should be float32 and     has a shape of [batch_size, sequence_length, "
"hidden_size].  - `pooled_output` (Tensor):     The output of first token "
"(`[CLS]`) in sequence.     We \"pool\" the model by simply taking the "
"hidden state corresponding to the first token.     Its data type should "
"be float32 and its shape is [batch_size, hidden_size].  - "
"`content_output` (Tensor):     The output of content summary token "
"(`[CLS1]` in sequence). Its data type should be float32 and     has a "
"shape of [batch_size, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:40
msgid ""
"Returns tuple (``sequence_output``, ``pooled_output``, "
"``content_output``)."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:42
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:19
msgid "With the fields:"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:46
msgid "`sequence_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:45
msgid ""
"Sequence of output at the last layer of the model. Its data type should "
"be float32 and has a shape of [batch_size, sequence_length, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:51
msgid "`pooled_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:49
msgid ""
"The output of first token (`[CLS]`) in sequence. We \"pool\" the model by"
" simply taking the hidden state corresponding to the first token. Its "
"data type should be float32 and its shape is [batch_size, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:54
msgid "`content_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:54
msgid ""
"The output of content summary token (`[CLS1]` in sequence). Its data type"
" should be float32 and has a shape of [batch_size, hidden_size]."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:15
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmModel.forward:59
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:15
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:27
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel:1
msgid ""
"ErnieCtmWordtag Model with a token classification head on top (a crf "
"layer on top of the hidden-states output) . e.g. for Named-Entity-"
"Recognition (NER) tasks."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel:3
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel:4
msgid "An instance of :class:`ErnieCtmModel`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel:6
msgid "The number of different tags."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel:8
msgid "The learning rate of the crf. Defaults to `100`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:3
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:5
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:7
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:3
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:5
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:7
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:1
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:3
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:5
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:7
msgid "See :class:`ErnieCtmModel`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:9
msgid ""
"The input length. Its dtype is int64 and has a shape of `[batch_size]`. "
"Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:12
msgid ""
"The input predicted tensor. Its dtype is float32 and has a shape of "
"`[batch_size, sequence_length, num_tags]`. Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:17
msgid ""
"Returns tuple (`seq_logits`, `cls_logits`).  With the fields:  - "
"`seq_logits` (Tensor):     A tensor of next sentence prediction logits."
"     Its data type should be float32 and its shape is [batch_size, "
"sequence_length, num_tag]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:17
msgid "Returns tuple (`seq_logits`, `cls_logits`)."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:22
msgid "`seq_logits` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmWordtagModel.forward:22
msgid ""
"A tensor of next sentence prediction logits. Its data type should be "
"float32 and its shape is [batch_size, sequence_length, num_tag]."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel:1
msgid "ErnieCtmNptag Model with a `masked language modeling` head on top."
msgstr ""

#: of paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmNptagModel.forward:10
msgid ""
"Returns tensor `logits`, the scores of masked token prediction. Its data "
"type should be float32 and shape is [batch_size, sequence_length, "
"vocab_size]."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification:1
msgid ""
"ERNIECtm Model with a linear layer on top of the hidden-states output "
"layer, designed for token classification tasks like NER tasks."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification:4
msgid "An instance of `ErnieModel`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification:6
msgid "The number of classes. Defaults to `2`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification:8
msgid ""
"The dropout probability for output of ERNIE. If None, use the same value "
"as `hidden_dropout_prob` of `ErnieCtmModel` instance `ernie`. Defaults to"
" `None`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_ctm.modeling.ErnieCtmForTokenClassification.forward:10
msgid ""
"Returns tensor `logits`, a tensor of the input token classification "
"logits. Shape as `[sequence_length, num_classes]` and dtype as `float32`."
msgstr ""

