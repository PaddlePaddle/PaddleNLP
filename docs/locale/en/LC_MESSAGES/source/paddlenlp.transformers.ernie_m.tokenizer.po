# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.ernie_m.tokenizer.rst:2
msgid "tokenizer"
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:1
msgid "基类：:class:`paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer`"
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:1
msgid ""
"Constructs a ErnieM tokenizer. It uses the `sentencepiece` tools to cut "
"the words to sub-words."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:3
msgid "The file path of the vocabulary."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:5
msgid "The file path of sentencepiece model."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:7
msgid "Whether or not to lowercase the input when tokenizing. Defaults to`True`."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:10
msgid ""
"A special token representing the *unknown (out-of-vocabulary)* token. An "
"unknown token is set to be `unk_token` inorder to be converted to an ID. "
"Defaults to \"[UNK]\"."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:14
msgid ""
"A special token separating two different sentences in the same input. "
"Defaults to \"[SEP]\"."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:17
msgid ""
"A special token used to make arrays of tokens the same size for batching "
"purposes. Defaults to \"[PAD]\"."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:20
msgid ""
"A special token used for sequence classification. It is the last token of"
" the sequence when built with special tokens. Defaults to \"[CLS]\"."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer:23
msgid ""
"A special token representing a masked token. This is the token used in "
"the masked language modeling task which the model tries to predict the "
"original unmasked ones. Defaults to \"[MASK]\"."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.vocab_size:1
msgid "Return the size of vocabulary."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.vocab_size
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.vocab_size:3
msgid "The size of vocabulary."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.vocab_size
msgid "返回类型"
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.clean_text:1
msgid "Performs invalid character removal and whitespace cleanup on text."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize:1
msgid "Converts a string to a list of tokens."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize:3
msgid "The text to be tokenized."
msgstr ""

#: of paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.tokenize:6
msgid "A list of string representing converted tokens."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.convert_ids_to_string:1
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.convert_tokens_to_string:1
msgid "Converts a sequence of tokens (strings for sub-words) in a single string."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens:4
msgid ""
"An ERNIE-M sequence has the following format: - single sequence:       "
"``[CLS] X [SEP]`` - pair of sequences:        ``[CLS] A [SEP] [SEP] B "
"[SEP]`` :param token_ids_0: List of IDs to which the special tokens will "
"be added. :type token_ids_0: List[int] :param token_ids_1: Optional "
"second list of IDs for sequence pairs."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens:10
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask:6
msgid "Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_inputs_with_special_tokens:13
msgid "List of input_id with the appropriate special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens:1
msgid ""
"Build offset map from a pair of offset map by concatenating and adding "
"offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens:3
msgid ""
"An ERNIE-M offset_mapping has the following format: - single sequence:"
"      ``(0,0) X (0,0)`` - pair of sequences:        ``(0,0) A (0,0) (0,0)"
" B (0,0)``"
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens:7
msgid "List of char offsets to which the special tokens will be added."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens:9
msgid ""
"Optional second list of wordpiece offsets for offset mapping pairs. "
"Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.build_offset_mapping_with_special_tokens:13
msgid "List of wordpiece offsets with the appropriate offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieves sequence ids from a token list that has no special tokens "
"added. This method is called when adding special tokens using the "
"tokenizer ``encode`` methods. :param token_ids_0: List of ids of the "
"first sequence. :type token_ids_0: List[int] :param token_ids_1: Optional"
" second list of IDs for sequence pairs."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie_m.tokenizer.ErnieMTokenizer.get_special_tokens_mask:12
msgid ""
"The list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

