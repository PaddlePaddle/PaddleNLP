# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.chinesebert.tokenizer.rst:2
msgid "tokenizer"
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:1
msgid "基类：:class:`paddlenlp.transformers.bert.tokenizer.BertTokenizer`"
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:1
msgid ""
"Construct a ChineseBert tokenizer. `ChineseBertTokenizer` is similar to "
"`BertTokenizerr`. The difference between them is that ChineseBert has the"
" extra process about pinyin id. For more information regarding those "
"methods, please refer to this superclass."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:5
msgid ""
"The vocabulary file path (ends with '.txt') required to instantiate a "
"`WordpieceTokenizer`."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:8
msgid "Whether or not to lowercase the input when tokenizing. Defaults to `True`."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:11
msgid ""
"A dict of pinyin map, the map between pinyin char and id. pinyin char is "
"26 Romanian characters and 0-5 numbers. Defaults to None."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:14
msgid "A dict of char id map tensor. Defaults to None."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:17
msgid "A dict of pinyin map tensor. Defaults to None."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:20
msgid ""
"A special token representing the *unknown (out-of-vocabulary)* token. An "
"unknown token is set to be `unk_token` inorder to be converted to an ID. "
"Defaults to \"[UNK]\"."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:24
msgid ""
"A special token separating two different sentences in the same input. "
"Defaults to \"[SEP]\"."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:27
msgid ""
"A special token used to make arrays of tokens the same size for batching "
"purposes. Defaults to \"[PAD]\"."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:30
msgid ""
"A special token used for sequence classification. It is the last token of"
" the sequence when built with special tokens. Defaults to \"[CLS]\"."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:33
msgid ""
"A special token representing a masked token. This is the token used in "
"the masked language modeling task which the model tries to predict the "
"original unmasked ones. Defaults to \"[MASK]\"."
msgstr ""

#: of paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer:39
msgid "实际案例"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:1
msgid ""
"Performs tokenization and uses the tokenized tokens to prepare model "
"inputs. It supports sequence or sequence pair as input, and batch input "
"is not allowed."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:5
msgid ""
"The sequence to be processed. One sequence is a string, a list of "
"strings, or a list of integers depending on whether it has been "
"pretokenized and converted to ids."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:9
msgid ""
"Same as `text` argument, while it represents for the latter sequence of "
"the sequence pair."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:10
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:12
msgid ""
"If set to a number, will limit the total sequence returned so that it has"
" a maximum length. If there are overflowing tokens, those overflowing "
"tokens will be added to the returned dictionary when "
"`return_overflowing_tokens` is `True`. Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:15
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:17
msgid ""
"Only available for batch input of sequence pair and mainly for question "
"answering usage. When for QA, `text` represents questions and `text_pair`"
" represents contexts. If `stride` is set to a positive number, the "
"context will be split into multiple spans where `stride` defines the "
"number of (tokenized) tokens to skip from the start of one span to get "
"the next span, thus will produce a bigger batch than inputs to include "
"all spans. Moreover, 'overflow_to_sample' and 'offset_mapping' preserving"
" the original example and position information will be added to the "
"returned dictionary. Defaults to 0."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:25
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:27
msgid ""
"If set to `True`, the returned sequences would be padded up to "
"`max_seq_len` specified length according to padding side "
"(`self.padding_side`) and padding token id. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:29
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:31
msgid ""
"String selected in the following options:  - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"`max_seq_len` starting from the longest one at each token (when there is "
"a pair of input sequences). - 'only_first': Only truncate the first "
"sequence. - 'only_second': Only truncate the second sequence. - "
"'do_not_truncate': Do not truncate (raise an error if the input sequence "
"is longer than `max_seq_len`).  Defaults to 'longest_first'."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:29
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:31
msgid "String selected in the following options:"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:31
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:33
msgid "'longest_first' (default) Iteratively reduce the inputs sequence"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:32
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:34
msgid ""
"until the input is under `max_seq_len` starting from the longest one at "
"each token (when there is a pair of input sequences). - 'only_first': "
"Only truncate the first sequence. - 'only_second': Only truncate the "
"second sequence. - 'do_not_truncate': Do not truncate (raise an error if "
"the input sequence is longer than `max_seq_len`)."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:39
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:41
msgid "Defaults to 'longest_first'."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:41
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:43
msgid ""
"Whether to include tokens position ids in the returned dictionary. "
"Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:44
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:46
msgid ""
"Whether to include token type ids in the returned dictionary. Defaults to"
" `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:47
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:49
msgid ""
"Whether to include the attention mask in the returned dictionary. "
"Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:50
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:52
msgid ""
"Whether to include the length of each encoded inputs in the returned "
"dictionary. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:53
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:55
msgid ""
"Whether to include overflowing token information in the returned "
"dictionary. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:56
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:58
msgid ""
"Whether to include special tokens mask information in the returned "
"dictionary. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map
msgid "返回"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:62
msgid ""
"The dict has the following optional items:  - **input_ids** (list[int]): "
"List of token ids to be fed to a model. - **pinyin_ids** (list[int]): "
"List of pinyin ids to be fed to a model. - **position_ids** (list[int], "
"optional): List of token position ids to be   fed to a model. Included "
"when `return_position_ids` is `True` - **token_type_ids** (list[int], "
"optional): List of token type ids to be   fed to a model. Included when "
"`return_token_type_ids` is `True`. - **attention_mask** (list[int], "
"optional): List of integers valued 0 or 1,   where 0 specifies paddings "
"and should not be attended to by the   model. Included when "
"`return_attention_mask` is `True`. - **seq_len** (int, optional): The "
"input_ids length. Included when `return_length`   is `True`. - "
"**overflowing_tokens** (list[int], optional): List of overflowing tokens."
"   Included when if `max_seq_len` is specified and "
"`return_overflowing_tokens`   is True. - **num_truncated_tokens** (int, "
"optional): The number of overflowing tokens.   Included when if "
"`max_seq_len` is specified and `return_overflowing_tokens`   is True. - "
"**special_tokens_mask** (list[int], optional): List of integers valued 0 "
"or 1,   with 0 specifying special added tokens and 1 specifying sequence "
"tokens.   Included when `return_special_tokens_mask` is `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:60
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:62
msgid "The dict has the following optional items:"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:62
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:64
msgid "**input_ids** (list[int]): List of token ids to be fed to a model."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:63
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:65
msgid "**pinyin_ids** (list[int]): List of pinyin ids to be fed to a model."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:64
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:66
msgid ""
"**position_ids** (list[int], optional): List of token position ids to be "
"fed to a model. Included when `return_position_ids` is `True`"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:66
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:68
msgid ""
"**token_type_ids** (list[int], optional): List of token type ids to be "
"fed to a model. Included when `return_token_type_ids` is `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:68
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:70
msgid ""
"**attention_mask** (list[int], optional): List of integers valued 0 or 1,"
" where 0 specifies paddings and should not be attended to by the model. "
"Included when `return_attention_mask` is `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:71
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:73
msgid ""
"**seq_len** (int, optional): The input_ids length. Included when "
"`return_length` is `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:73
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:75
msgid ""
"**overflowing_tokens** (list[int], optional): List of overflowing tokens."
" Included when if `max_seq_len` is specified and "
"`return_overflowing_tokens` is True."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:76
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:78
msgid ""
"**num_truncated_tokens** (int, optional): The number of overflowing "
"tokens. Included when if `max_seq_len` is specified and "
"`return_overflowing_tokens` is True."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:79
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode:81
msgid ""
"**special_tokens_mask** (list[int], optional): List of integers valued 0 "
"or 1, with 0 specifying special added tokens and 1 specifying sequence "
"tokens. Included when `return_special_tokens_mask` is `True`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.encode
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:1
msgid ""
"Performs tokenization and uses the tokenized tokens to prepare model "
"inputs. It supports batch inputs of sequence or sequence pair."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:4
msgid ""
"The element of list can be sequence or sequence pair, and the sequence is"
" a string or a list of strings depending on whether it has been "
"pretokenized. If each sequence is provided as a list of strings "
"(pretokenized), you must set `is_split_into_words` as `True` to "
"disambiguate with a sequence pair."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:60
msgid ""
"The dict has the following optional items:  - **input_ids** (list[int]): "
"List of token ids to be fed to a model. - **pinyin_ids** (list[int]): "
"List of pinyin ids to be fed to a model. - **position_ids** (list[int], "
"optional): List of token position ids to be   fed to a model. Included "
"when `return_position_ids` is `True` - **token_type_ids** (list[int], "
"optional): List of token type ids to be   fed to a model. Included when "
"`return_token_type_ids` is `True`. - **attention_mask** (list[int], "
"optional): List of integers valued 0 or 1,   where 0 specifies paddings "
"and should not be attended to by the   model. Included when "
"`return_attention_mask` is `True`. - **seq_len** (int, optional): The "
"input_ids length. Included when `return_length`   is `True`. - "
"**overflowing_tokens** (list[int], optional): List of overflowing tokens."
"   Included when if `max_seq_len` is specified and "
"`return_overflowing_tokens`   is True. - **num_truncated_tokens** (int, "
"optional): The number of overflowing tokens.   Included when if "
"`max_seq_len` is specified and `return_overflowing_tokens`   is True. - "
"**special_tokens_mask** (list[int], optional): List of integers valued 0 "
"or 1,   with 0 specifying special added tokens and 1 specifying sequence "
"tokens.   Included when `return_special_tokens_mask` is `True`. - "
"**offset_mapping** (list[int], optional): list of pair preserving the   "
"index of start and end char in original input for each token.   For a "
"sqecial token, the index pair is `(0, 0)`. Included when   `stride` "
"works. - **overflow_to_sample** (int, optional): Index of example from "
"which this   feature is generated. Included when `stride` works."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:82
msgid ""
"**offset_mapping** (list[int], optional): list of pair preserving the "
"index of start and end char in original input for each token. For a "
"sqecial token, the index pair is `(0, 0)`. Included when `stride` works."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.batch_encode:86
msgid ""
"**overflow_to_sample** (int, optional): Index of example from which this "
"feature is generated. Included when `stride` works."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:1
msgid "Truncates a sequence pair in place to the maximum length."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:3
msgid ""
"list of tokenized input ids. Can be obtained from a string by chaining "
"the `tokenize` and `convert_tokens_to_ids` methods."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:5
msgid ""
"Optional second list of input ids. Can be obtained from a string by "
"chaining the `tokenize` and `convert_tokens_to_ids` methods."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:7
msgid ""
"The map of tokens and the start and end index of their start and end "
"character"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:9
msgid ""
"The map of token pairs and the start and end index of their start and end"
" character"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:11
msgid "number of tokens to remove using the truncation strategy"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:13
msgid ""
"string selected in the following options: - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"max_seq_len     starting from the longest one at each token (when there "
"is a pair of input sequences).     Overflowing tokens only contains "
"overflow from the first sequence. - 'only_first': Only truncate the first"
" sequence. raise an error if the first sequence is shorter or equal to "
"than num_tokens_to_remove. - 'only_second': Only truncate the second "
"sequence - 'do_not_truncate': Does not truncate (raise an error if the "
"input sequence is longer than max_seq_len)"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:13
msgid ""
"string selected in the following options: - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"max_seq_len"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:15
msgid ""
"starting from the longest one at each token (when there is a pair of "
"input sequences). Overflowing tokens only contains overflow from the "
"first sequence."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:17
msgid ""
"'only_first': Only truncate the first sequence. raise an error if the "
"first sequence is shorter or equal to than num_tokens_to_remove."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:18
msgid "'only_second': Only truncate the second sequence"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:19
msgid ""
"'do_not_truncate': Does not truncate (raise an error if the input "
"sequence is longer than max_seq_len)"
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.truncate_sequences:20
msgid ""
"If set to a number along with max_seq_len, the overflowing tokens "
"returned will contain some tokens from the main sequence returned. The "
"value of this argument defines the number of additional tokens."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map:1
msgid "Get the map of pinyin locations and pinyin tensor."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids:3
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map:3
msgid "The sequence to be processed."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.pinyin_locs_map:6
msgid "the map of pinyin locations and pinyin tensor."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids:1
msgid "Find chinese character location, and generate pinyin ids."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids:5
msgid ""
"Same as `text` argument, while it represents for the latter sequence of "
"the sequence pair. Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids:8
msgid ""
"A list of wordpiece offsets with the appropriate offsets of special "
"tokens. Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.chinesebert.tokenizer.ChineseBertTokenizer.get_pinyin_ids:12
msgid "The list of pinyin id tensor."
msgstr ""

