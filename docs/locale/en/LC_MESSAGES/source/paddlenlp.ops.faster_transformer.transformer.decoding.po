# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.ops.faster_transformer.transformer.decoding.rst:2
msgid "decoding"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:1
msgid ""
"Convert parameters included in Transformer layer (`nn.TransformerEncoder`"
" and `gpt.modeling.TransformerDecoder`) from original models to the "
"format of faster models."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
msgid "参数"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:5
msgid "The faster model object."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:7
msgid ""
"The Transformer layer. It can be an instance of `nn.TransformerEncoder` "
"or `gpt.modeling.TransformerDecoder` currently, and "
"`nn.TransformerDecoder` would be supported soon."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:11
msgid ""
"0 for nofuse, 1 for fuse, 2 for fuse and delete the unfused parameters. "
"If environment variable `PPFG_QKV_MEM_OPT` is set and the weights of "
"q/k/v is fused, it will try to delete the original unfused weights. Note "
"the rollback to original model would not be guarantee anymore when the "
"faster model failed if the original weights are deleted. Default to 1."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:18
msgid ""
"Whether to use float16. Maybe we should use the default dtype as the "
"highest priority later. Default to `False`."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:21
msgid ""
"If `False`, need to reload the weight values. It should be `True` for "
"weight loaded models. Default to `False`."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
msgid "返回"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:25
msgid ""
"Each value is a list including converted parameters in all     layers. "
"For other parameters not included in Transformer module to     be "
"converted, such as embeddings, you can achieve it by using the     "
"returned dict `params` though `params['word_emb'].append()` directly     "
"which would do CPU/GPU and fp32/fp16 transfer automatically."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:30
msgid "Each value is a list including converted parameters in all"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:28
msgid ""
"layers. For other parameters not included in Transformer module to be "
"converted, such as embeddings, you can achieve it by using the returned "
"dict `params` though `params['word_emb'].append()` directly which would "
"do CPU/GPU and fp32/fp16 transfer automatically."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:6
msgid "unpacked dict arguments"
msgstr ""

