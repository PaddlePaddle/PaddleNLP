# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-19 14:17+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.1\n"

#: ../source/paddlenlp.ops.faster_transformer.transformer.decoding.rst:2
msgid "decoding"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:1
msgid ""
"Convert parameters included in Transformer layer (`nn.TransformerEncoder`"
" and `gpt.modeling.TransformerDecoder`) from original models to the "
"format of faster models."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.set_partial_model
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward
#: paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
#: paddlenlp.ops.faster_transformer.transformer.decoding.enable_ft_para
msgid "参数"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:5
msgid "The faster model object."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:7
msgid ""
"The Transformer layer. It can be an instance of `nn.TransformerEncoder` "
"or `gpt.modeling.TransformerDecoder` currently, and "
"`nn.TransformerDecoder` would be supported soon."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:11
msgid ""
"0 for nofuse, 1 for fuse, 2 for fuse and delete the unfused parameters. "
"If environment variable `PPFG_QKV_MEM_OPT` is set and the weights of "
"q/k/v is fused, it will try to delete the original unfused weights. Note "
"the rollback to original model would not be guarantee anymore when the "
"faster model failed if the original weights are deleted. Default to 1."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:18
msgid ""
"Whether to use float16. Maybe we should use the default dtype as the "
"highest priority later. Default to `False`."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:21
msgid ""
"If `False`, need to reload the weight values. It should be `True` for "
"weight loaded models. Default to `False`."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight
#: paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
#: paddlenlp.ops.faster_transformer.transformer.decoding.get_ft_para_conf
msgid "返回"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:25
msgid ""
"Each value is a list including converted parameters in all     layers. "
"For other parameters not included in Transformer module to     be "
"converted, such as embeddings, you can achieve it by using the     "
"returned dict `params` though `params['word_emb'].append()` directly     "
"which would do CPU/GPU and fp32/fp16 transfer automatically."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:30
msgid "Each value is a list including converted parameters in all"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.convert_params:28
msgid ""
"layers. For other parameters not included in Transformer module to be "
"converted, such as embeddings, you can achieve it by using the returned "
"dict `params` though `params['word_emb'].append()` directly which would "
"do CPU/GPU and fp32/fp16 transfer automatically."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight
#: paddlenlp.ops.faster_transformer.transformer.decoding.convert_params
#: paddlenlp.ops.faster_transformer.transformer.decoding.get_ft_para_conf
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:1
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:4
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferBartDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferGptDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferMBartDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferTransformerDecoding.forward:6
#: paddlenlp.ops.faster_transformer.transformer.decoding.InferUnifiedDecoding.forward:6
msgid "unpacked dict arguments"
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf:1
msgid ""
"Configurations for model parallel in FasterTransformer. Currently only "
"support GPT. Please refer to  `Megatron "
"<https://arxiv.org/pdf/2104.04473.pdf>`__ for details."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf:5
#: paddlenlp.ops.faster_transformer.transformer.decoding.enable_ft_para:5
msgid ""
"The size for tensor parallel. If it is 1, tensor parallel would not be "
"used. Default to 1."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf:8
#: paddlenlp.ops.faster_transformer.transformer.decoding.enable_ft_para:8
msgid ""
"The size for layer parallel. If it is 1, layer parallel would not be "
"used. Default to 1."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf:11
#: paddlenlp.ops.faster_transformer.transformer.decoding.enable_ft_para:11
msgid ""
"The local batch size for pipeline parallel. It is suggested to use "
"`batch_size // layer_para_size`. Default to 1."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_last_group:1
msgid ""
"For layer parallel, only the process corresponding to the last layer "
"group can get the predict results. It is used to check whether this is "
"the process corresponding to the last layer group."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:1
msgid ""
"Whether or not the given transformer layer of should be loaded to the "
"current parallel model. For layer parallel, there is no need not to load "
"other layer groups."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:5
msgid "The index of Transformer layer."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:7
msgid "The number of Transformer layers."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:10
msgid ""
"Indicate whether or not the given transformer layer of should     be "
"loaded to the current parallel model."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:12
msgid "Indicate whether or not the given transformer layer of should"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.is_load:13
msgid "be loaded to the current parallel model."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:1
msgid "Get the weight slice for tensor parallel."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:3
msgid "The weight or bias to be sliced."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:5
msgid "The axis to perform slice."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:7
msgid ""
"0 is used for creating partial model when initializing and "
"`from_pretrained`. While 1 is used in converting parameters to "
"FasterTransformer. No slice would be performed if it is 1, since "
"parameters have been sliced in `phase=0`."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:12
msgid ""
"If true, `weight` should be a Parameter and force the output to be a "
"Parameter."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.slice_weight:16
msgid "The sliced weight."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.set_partial_model:1
msgid ""
"This is used to set whether or not the current model has complete "
"parameters."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.set_partial_model:4
msgid ""
"It is used to set whether or not the current model has complete "
"parameters."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model:1
msgid ""
"Slice every values included in `state_to_load` according to the shape of "
"corresponding parameters in `model`. This is used in `from_pratrained` to"
" get sliced parameter values."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model:5
msgid "The model to use."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model:7
msgid "The state dict including complete parameter values of model."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.decoding.FTParaConf.fit_partial_model:11
msgid "The state dict contains adjusted values."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.get_ft_para_conf:1
msgid "Get settings for model parallel."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.get_ft_para_conf:3
msgid "The settings for model parallel."
msgstr ""

#: of paddlenlp.ops.faster_transformer.transformer.decoding.enable_ft_para:1
msgid ""
"Enable model parallel with the given settings in FasterTransformer. "
"Currently only support GPT. Please refer to `Megatron "
"<https://arxiv.org/pdf/2104.04473.pdf>`__ for details."
msgstr ""

