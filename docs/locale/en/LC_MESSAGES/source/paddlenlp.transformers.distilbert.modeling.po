# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.distilbert.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM:1
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering:1
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification:1
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification:1
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel:1
msgid "基类：:class:`paddlenlp.transformers.distilbert.modeling.DistilBertPretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:1
msgid "The bare DistilBert Model transformer outputting raw hidden-states."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:3
msgid ""
"This model inherits from "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. Refer to "
"the superclass documentation for the generic methods."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:6
msgid ""
"This model is also a Paddle `paddle.nn.Layer "
"<https://www.paddlepaddle.org.cn/documentation "
"/docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ subclass. Use "
"it as a regular Paddle Layer and refer to the Paddle documentation for "
"all matter related to general usage and behavior."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:10
msgid ""
"Vocabulary size of `inputs_ids` in `DistilBertModel`. Defines the number "
"of different tokens that can be represented by the `inputs_ids` passed "
"when calling `DistilBertModel`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:13
msgid ""
"Dimensionality of the embedding layer, encoder layers and the pooler "
"layer. Defaults to `768`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:15
msgid "Number of hidden layers in the Transformer encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:17
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:20
msgid ""
"Dimensionality of the feed-forward (ff) layer in the encoder. Input "
"tensors to ff layers are firstly projected from `hidden_size` to "
"`intermediate_size`, and then projected back to `hidden_size`. Typically "
"`intermediate_size` is larger than `hidden_size`. Defaults to `3072`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:25
msgid ""
"The non-linear activation function in the feed-forward layer. "
"``\"gelu\"``, ``\"relu\"`` and any other paddle supported activation "
"functions are supported. Defaults to `\"gelu\"`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:29
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:32
msgid ""
"The dropout probability used in MultiHeadAttention in all encoder layers "
"to drop some attention target. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:35
msgid ""
"The maximum value of the dimensionality of position encoding, which "
"dictates the maximum supported length of an input sequence. Defaults to "
"`512`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:38
msgid "The vocabulary size of `token_type_ids`. Defaults to `16`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:41
msgid ""
"The standard deviation of the normal initializer. Defaults to `0.02`.  .."
" note::     A normal_initializer initializes weight matrices as normal "
"distributions.     See :meth:`DistilBertPretrainedModel.init_weights()` "
"for how weights are initialized in `DistilBertModel`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:41
msgid "The standard deviation of the normal initializer. Defaults to `0.02`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:45
msgid ""
"A normal_initializer initializes weight matrices as normal distributions."
" See :meth:`DistilBertPretrainedModel.init_weights()` for how weights are"
" initialized in `DistilBertModel`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel:48
msgid "The index of padding token in the token vocabulary. Defaults to `0`."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward:1
msgid ""
"The DistilBertModel forward method, overrides the `__call__()` special "
"method."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward:3
msgid ""
"Indices of input sequence tokens in the vocabulary. They are numerical "
"representations of tokens that build the input sequence. Its data type "
"should be `int64` and it has a shape of [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward:7
msgid ""
"Mask used in multi-head attention to avoid performing attention to some "
"unwanted positions, usually the paddings or the subsequent positions. Its"
" data type can be int, float and bool. When the data type is bool, the "
"`masked` tokens have `False` values and the others have `True` values. "
"When the data type is int, the `masked` tokens have `0` values and the "
"others have `1` values. When the data type is float, the `masked` tokens "
"have `-INF` values and the others have `0` values. It is a tensor with "
"shape broadcasted to `[batch_size, num_attention_heads, sequence_length, "
"sequence_length]`. For example, its shape can be  [batch_size, "
"sequence_length], [batch_size, sequence_length, sequence_length], "
"[batch_size, num_attention_heads, sequence_length, sequence_length]. "
"Defaults to `None`, which means nothing needed to be prevented attention "
"to."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward:19
msgid ""
"Returns tensor `encoder_output`, which means the sequence of hidden-"
"states at the last layer of the model. Its data type should be float32 "
"and its shape is [batch_size, sequence_length, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward:13
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:22
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward:13
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward:13
#: paddlenlp.transformers.distilbert.modeling.DistilBertModel.forward:24
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertPretrainedModel:1
msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertPretrainedModel:1
msgid ""
"An abstract class for pretrained DistilBert models. It provides "
"DistilBert related `model_config_file`, `pretrained_init_configuration`, "
"`resource_files_names`, `pretrained_resource_files_map`, "
"`base_model_prefix` for downloading and loading pretrained models. See "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel` for more "
"details."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertPretrainedModel.init_weights:1
msgid "Initialization hook"
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification:1
msgid ""
"DistilBert Model with a linear layer on top of the output layer, designed"
" for sequence classification/regression tasks like GLUE tasks."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM:3
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering:4
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification:4
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification:4
msgid "An instance of DistilBertModel."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification:6
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification:6
msgid "The number of classes. Defaults to `2`."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering:6
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification:8
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification:8
msgid ""
"The dropout probability for output of DistilBert. If None, use the same "
"value as `hidden_dropout_prob` of `DistilBertModel` instance "
"`distilbert`. Defaults to None."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward:1
msgid ""
"The DistilBertForSequenceClassification forward method, overrides the "
"__call__() special method."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward:3
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward:5
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:3
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:5
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward:3
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward:5
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward:3
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward:5
msgid "See :class:`DistilBertModel`."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForSequenceClassification.forward:8
msgid ""
"Returns tensor `logits`, a tensor of the input text classification "
"logits. Shape as `[batch_size, num_classes]` and dtype as `float32`."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification:1
msgid ""
"DistilBert Model with a linear layer on top of the hidden-states output "
"layer, designed for token classification tasks like NER tasks."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward:1
msgid ""
"The DistilBertForTokenClassification forward method, overrides the "
"__call__() special method."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForTokenClassification.forward:8
msgid ""
"Returns tensor `logits`, a tensor of the input token classification "
"logits. Shape as `[batch_size, sequence_length, num_classes]` and dtype "
"as `float32`."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering:1
msgid ""
"DistilBert Model with a linear layer on top of the hidden-states output "
"to compute `span_start_logits` and `span_end_logits`, designed for "
"question-answering tasks like SQuAD."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:1
msgid ""
"The DistilBertForQuestionAnswering forward method, overrides the "
"__call__() special method."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:8
msgid ""
"Returns tuple (`start_logits`, `end_logits`).  With the fields:  - "
"start_logits(Tensor):     A tensor of the input token classification "
"logits, indicates the start position of the labelled span.     Its data "
"type should be float32 and its shape is [batch_size, sequence_length].  -"
" end_logits(Tensor):     A tensor of the input token classification "
"logits, indicates the end position of the labelled span.     Its data "
"type should be float32 and its shape is [batch_size, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:8
msgid "Returns tuple (`start_logits`, `end_logits`)."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:10
msgid "With the fields:"
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:14
msgid "start_logits(Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:13
msgid ""
"A tensor of the input token classification logits, indicates the start "
"position of the labelled span. Its data type should be float32 and its "
"shape is [batch_size, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:17
msgid "end_logits(Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForQuestionAnswering.forward:17
msgid ""
"A tensor of the input token classification logits, indicates the end "
"position of the labelled span. Its data type should be float32 and its "
"shape is [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM:1
msgid "DistilBert Model with a `language modeling` head on top."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward:1
msgid ""
"The DistilBertForMaskedLM forward method, overrides the `__call__()` "
"special method."
msgstr ""

#: of
#: paddlenlp.transformers.distilbert.modeling.DistilBertForMaskedLM.forward:8
msgid ""
"Returns tensor `prediction_logits`, the scores of masked token "
"prediction. Its data type should be float32 and its shape is [batch_size,"
" sequence_length, vocab_size]."
msgstr ""

