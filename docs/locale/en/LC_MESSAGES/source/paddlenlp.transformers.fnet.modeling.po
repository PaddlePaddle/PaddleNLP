# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.fnet.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling:1
msgid "Modeling classes for FNet model."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetPretrainedModel:1
msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetPretrainedModel:1
msgid ""
"An abstract class for pretrained FNet models. It provides FNet related "
"`model_config_file`, `pretrained_init_configuration`, "
"`resource_files_names`, `pretrained_resource_files_map`, "
"`base_model_prefix` for downloading and loading pretrained models. See "
"`PretrainedModel` for more details."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM:1
#: paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice:1
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction:1
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining:1
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering:1
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification:1
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification:1
#: paddlenlp.transformers.fnet.modeling.FNetModel:1
msgid "基类：:class:`paddlenlp.transformers.fnet.modeling.FNetPretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:1
msgid ""
"The model can behave as an encoder, following the architecture described "
"in `FNet: Mixing Tokens with Fourier Transforms "
"<https://arxiv.org/abs/2105.03824>`__ by James Lee-Thorp, Joshua Ainslie,"
" Ilya Eckstein, Santiago Ontanon."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward
#: paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice
#: paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice.forward
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction.forward
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering.forward
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification.forward
#: paddlenlp.transformers.fnet.modeling.FNetModel
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:5
msgid ""
"Vocabulary size of `inputs_ids` in `FNetModel`. Also is the vocab size of"
" token embedding matrix. Defines the number of different tokens that can "
"be represented by the `inputs_ids` passed when calling `FNetModel`. "
"Defaults to `32000`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:9
msgid "Dimensionality of the encoder layer and pooler layer. Defaults to `768`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:11
msgid "Number of hidden layers in the Transformer encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:13
msgid ""
"Dimensionality of the feed-forward (ff) layer in the encoder. Input "
"tensors to ff layers are firstly projected from `hidden_size` to "
"`intermediate_size`, and then projected back to `hidden_size`. Typically "
"`intermediate_size` is larger than `hidden_size`. Defaults to `3072`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:18
msgid ""
"The non-linear activation function in the feed-forward layer. "
"``\"gelu\"``, ``\"relu\"`` and any other paddle supported activation "
"functions are supported. Defaults to `glue_new`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:22
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:25
msgid ""
"The maximum value of the dimensionality of position encoding, which "
"dictates the maximum supported length of an input sequence. Defaults to "
"`512`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:28
msgid "The vocabulary size of `token_type_ids`. Defaults to `4`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:30
msgid ""
"The standard deviation of the normal initializer. Defaults to `0.02`. .. "
"note::     A normal_initializer initializes weight matrices as normal "
"distributions.     See :meth:`BertPretrainedModel.init_weights()` for how"
" weights are initialized in `ElectraModel`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:30
msgid ""
"The standard deviation of the normal initializer. Defaults to `0.02`. .. "
"note::"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:35
msgid ""
"The `epsilon` parameter used in :class:`paddle.nn.LayerNorm` for "
"initializing layer normalization layers. A small value to the variance "
"added to the normalization layer to prevent division by zero. Defaults to"
" `1e-12`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:39
msgid "The index of padding token in the token vocabulary. Defaults to `3`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel:41
msgid "Whether or not to add the pooling layer. Defaults to `True`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:1
msgid "The FNetModel forward method."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:3
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:3
msgid ""
"Indices of input sequence tokens in the vocabulary. They are numerical "
"representations of tokens that build the input sequence. Its data type "
"should be `int64` and it has a shape of [batch_size, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:7
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:7
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range ``[0, type_vocab_size - 1]``. If `type_vocab_size` "
"is 2, which means the inputs have two portions. Indices can either be 0 "
"or 1:  - 0 corresponds to a *sentence A* token, - 1 corresponds to a "
"*sentence B* token.  Its data type should be `int64` and it has a shape "
"of [batch_size, sequence_length]. Defaults to `None`, which means we "
"don't add segment embeddings."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:7
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:7
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range ``[0, type_vocab_size - 1]``. If `type_vocab_size` "
"is 2, which means the inputs have two portions. Indices can either be 0 "
"or 1:"
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:12
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:12
msgid "0 corresponds to a *sentence A* token,"
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:13
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:13
msgid "1 corresponds to a *sentence B* token."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:15
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:15
msgid ""
"Its data type should be `int64` and it has a shape of [batch_size, "
"sequence_length]. Defaults to `None`, which means we don't add segment "
"embeddings."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:18
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:18
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, max_position_embeddings - 1]``. "
"Shape as `(batch_size, num_tokens)` and dtype as int64. Defaults to "
"`None`."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:22
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:22
msgid ""
"If you want to control how to convert `inputs_ids` indices into "
"associated vectors, you can pass an embedded representation directly "
"instead of passing `inputs_ids`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:32
msgid ""
"Returns tuple (`sequence_output`, `pooled_output`, `encoder_outputs[1:]`)"
" or a dict with last_hidden_state`, `pooled_output`, `all_hidden_states`,"
" fields.  With the fields:  - `sequence_output` (Tensor):    Sequence of "
"hidden-states at the last layer of the model.    It's data type should be"
" float32 and has a shape of [`batch_size, sequence_length, hidden_size`]."
"  - `pooled_output` (Tensor):    The output of first token (`[CLS]`) in "
"sequence.    We \"pool\" the model by simply taking the hidden state "
"corresponding to the first token.    Its data type should be float32 and"
"    has a shape of [batch_size, hidden_size].  - `last_hidden_state` "
"(Tensor):    The output of the last encoder layer, it is also the "
"`sequence_output`.    It's data type should be float32 and has a shape of"
" [batch_size, sequence_length, hidden_size].  - `all_hidden_states` "
"(Tensor):    Hidden_states of all layers in the Transformer encoder. The "
"length of `all_hidden_states` is `num_hidden_layers + 1`.    For all "
"element in the tuple, its data type should be float32 and its shape is "
"[`batch_size, sequence_length, hidden_size`]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:32
msgid ""
"Returns tuple (`sequence_output`, `pooled_output`, `encoder_outputs[1:]`)"
" or a dict with last_hidden_state`, `pooled_output`, `all_hidden_states`,"
" fields."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:22
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:34
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:35
msgid "With the fields:"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:39
msgid "`sequence_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:38
msgid ""
"Sequence of hidden-states at the last layer of the model. It's data type "
"should be float32 and has a shape of [`batch_size, sequence_length, "
"hidden_size`]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:45
msgid "`pooled_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:42
msgid ""
"The output of first token (`[CLS]`) in sequence. We \"pool\" the model by"
" simply taking the hidden state corresponding to the first token. Its "
"data type should be float32 and has a shape of [batch_size, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:49
msgid "`last_hidden_state` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:48
msgid ""
"The output of the last encoder layer, it is also the `sequence_output`. "
"It's data type should be float32 and has a shape of [batch_size, "
"sequence_length, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:52
msgid "`all_hidden_states` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetModel.forward:52
msgid ""
"Hidden_states of all layers in the Transformer encoder. The length of "
"`all_hidden_states` is `num_hidden_layers + 1`. For all element in the "
"tuple, its data type should be float32 and its shape is [`batch_size, "
"sequence_length, hidden_size`]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:46
#: paddlenlp.transformers.fnet.modeling.FNetModel.forward:57
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification:1
msgid ""
"FNet Model with a linear layer on top of the output layer, designed for "
"sequence classification/regression tasks like GLUE tasks."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice:4
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering:4
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification:4
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification:4
msgid "An instance of FNetModel."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification:6
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification:6
msgid "The number of classes. Defaults to `2`."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:1
msgid "The FNetForSequenceClassification forward method."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:32
msgid ""
"Returns tensor `logits`, or a dict with `logits`, `hidden_states`, "
"`attentions` fields.  With the fields:  - `logits` (Tensor):     A tensor"
" of the input text classification logits.     Shape as `[batch_size, "
"num_classes]` and dtype as float32.  - `hidden_states` (Tensor):     "
"Hidden_states of all layers in the Transformer encoder. The length of "
"`hidden_states` is `num_hidden_layers + 1`.     For all element in the "
"tuple, its data type should be float32 and its shape is [`batch_size, "
"sequence_length, hidden_size`]."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:32
msgid ""
"Returns tensor `logits`, or a dict with `logits`, `hidden_states`, "
"`attentions` fields."
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:38
msgid "`logits` (Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:37
msgid ""
"A tensor of the input text classification logits. Shape as `[batch_size, "
"num_classes]` and dtype as float32."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:29
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:41
msgid "`hidden_states` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:29
#: paddlenlp.transformers.fnet.modeling.FNetForSequenceClassification.forward:41
msgid ""
"Hidden_states of all layers in the Transformer encoder. The length of "
"`hidden_states` is `num_hidden_layers + 1`. For all element in the tuple,"
" its data type should be float32 and its shape is [`batch_size, "
"sequence_length, hidden_size`]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForPreTraining:1
msgid ""
"FNet Model with two heads on top as done during the pretraining: a "
"`masked language modeling` head and a `next sentence prediction "
"(classification)` head."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:1
msgid "The FNetForPretraining forward method."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:3
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:5
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:7
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:9
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:15
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:17
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:3
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:5
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:7
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:11
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:17
#: paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:19
msgid "See :class:`FNetModel`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:9
msgid "Labels for computing the masked language modeling loss."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:13
msgid ""
"The labels of the next sentence prediction task, the dimensionality of "
"`next_sentence_labels` is equal to `seq_relation_labels`. Its data type "
"should be int64 and its shape is [batch_size, 1]"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForPreTraining.forward:22
msgid ""
"Returns tuple (`prediction_scores`, `seq_relationship_score`) or a dict "
"with `prediction_logits`, `seq_relationship_logits`,  `hidden_states` "
"fields."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM:1
msgid "FNet Model with a `masked language modeling` head on top."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM:3
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction:3
msgid "An instance of :class:`FNetModel`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:1
msgid "The FNetForMaskedLM forward method."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:11
#: paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:13
msgid "See :class:`FNetForPreTraining`."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:20
msgid ""
"Returns tensor `prediction_scores` or a dict with `prediction_logits`, "
"`hidden_states` fields.  With the fields:  - `prediction_scores` "
"(Tensor):     The scores of masked token prediction. Its data type should"
" be float32.     and its shape is [batch_size, sequence_length, "
"vocab_size].  - `hidden_states` (Tensor):     Hidden_states of all layers"
" in the Transformer encoder. The length of `hidden_states` is "
"`num_hidden_layers + 1`.     For all element in the tuple, its data type "
"should be float32 and its shape is [`batch_size, sequence_length, "
"hidden_size`]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:20
msgid ""
"Returns tensor `prediction_scores` or a dict with `prediction_logits`, "
"`hidden_states` fields."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:26
msgid "`prediction_scores` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMaskedLM.forward:25
msgid ""
"The scores of masked token prediction. Its data type should be float32. "
"and its shape is [batch_size, sequence_length, vocab_size]."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction:1
msgid "FNet Model with a `next sentence prediction` head on top."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice.forward:1
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction.forward:1
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering.forward:1
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice.forward:4
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction.forward:4
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering.forward:4
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice.forward:6
#: paddlenlp.transformers.fnet.modeling.FNetForNextSentencePrediction.forward:6
#: paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering.forward:6
#: paddlenlp.transformers.fnet.modeling.FNetForTokenClassification.forward:6
msgid "unpacked dict arguments"
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForMultipleChoice:1
msgid ""
"FNet Model with a linear layer on top of the hidden-states output layer, "
"designed for multiple choice tasks like SWAG tasks ."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForTokenClassification:1
msgid ""
"FNet Model with a linear layer on top of the hidden-states output layer, "
"designed for token classification tasks like NER tasks."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering:1
msgid ""
"FNet Model with a linear layer on top of the hidden-states output to "
"compute `span_start_logits` and `span_end_logits`, designed for question-"
"answering tasks like SQuAD."
msgstr ""

#: of paddlenlp.transformers.fnet.modeling.FNetForQuestionAnswering:6
msgid "The number of labels."
msgstr ""

