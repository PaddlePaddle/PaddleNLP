# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../source/paddlenlp.transformers.xlnet.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling:1
msgid "Modeling classes for XLNet model."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification:1
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification:1
#: paddlenlp.transformers.xlnet.modeling.XLNetModel:1
msgid "基类：:class:`paddlenlp.transformers.xlnet.modeling.XLNetPretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:1
msgid ""
"The bare XLNet Model transformer outputting raw hidden-states without any"
" specific head on top."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:3
msgid ""
"This model inherits from "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. Check the "
"superclass documentation for the generic methods and the library "
"implements for all its model."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:6
msgid ""
"This model is also a Paddle `paddle.nn.Layer "
"<https://www.paddlepaddle.org.cn/documentation "
"/docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ subclass. Use "
"it as a regular Paddle Layer and refer to the Paddle documentation for "
"all matter related to general usage and behavior."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetModel
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:10
msgid ""
"Vocabulary size of the XLNet model. Defines the number of different "
"tokens that can be represented by the `inputs_ids` passed when calling "
"XLNetModel."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:13
msgid ""
"The number of tokens to cache. The key/value pairs that have already been"
" pre-computed in a previous forward pass won't be re-computed. Defaults "
"to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:16
msgid ""
"The number of tokens in the current batch to be cached and reused in the "
"future. Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:18
msgid ""
"Dimensionality of the encoder layers and the pooler layer. Defaults to "
"``768``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:20
msgid ""
"Whether or not to use the same attention length for each token. Defaults "
"to ``False``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:22
msgid ""
"The attention type used by the model. Set `\"bi\"` for XLNet, `\"uni\"` "
"for Transformer-XL. Defaults to ``\"bi\"``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:25
msgid ""
"Whether or not to use bidirectional input pipeline. Usually set to `True`"
" during pretraining and `False` during fine-tuning. Defaults to "
"``False``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:28
msgid ""
"Clamp all relative distances larger than clamp_len. Setting this "
"attribute to -1 means no clamping. Defaults to ``-1``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:31
msgid "Number of hidden layers in the Transformer encoder. Defaults to ``12``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:33
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to ``0.1``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:36
msgid ""
"The dropout probability for all fully connected layers in the pooler. "
"Defaults to ``0.1``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:39
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to ``12``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:42
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in the Transformer encoder. Defaults to ``64``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:45
msgid "The epsilon used by the layer normalization layers. Defaults to ``1e-12``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:48
msgid ""
"Dimensionality of the \"intermediate\" (often named feed-forward) layer "
"in the Transformer encoder. Defaults to ``3072``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:51
msgid ""
"The non-linear activation function in the feed-forward layer. "
"``\"gelu\"``, ``\"relu\"``, ``\"silu\"`` and ``\"gelu_new\"`` are "
"supported. Defaults to ``\"gelu\"``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel:55
msgid ""
"The standard deviation of the truncated_normal_initializer for "
"initializing all weight matrices. Defaults to ``0.02``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:1
msgid "The XLNetModel forward method, overrides the __call__() special method."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:3
msgid ""
"Indices of input sequence tokens in the vocabulary. It's data type should"
" be int64 and it has a shape of [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:6
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices can either be 0 or 1:  - 0 corresponds to a *sentence A* "
"token, - 1 corresponds to a *sentence B* token.  It's data type should be"
" `int64` and it has a shape of [batch_size, sequence_length]. Defaults to"
" ``None``, which means we don't add segment embeddings."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:6
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices can either be 0 or 1:"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:9
msgid "0 corresponds to a *sentence A* token,"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:10
msgid "1 corresponds to a *sentence B* token."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:12
msgid ""
"It's data type should be `int64` and it has a shape of [batch_size, "
"sequence_length]. Defaults to ``None``, which means we don't add segment "
"embeddings."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:15
msgid ""
"Mask to avoid performing attention on padding token indices with values "
"being either 0 or 1:  - 1 for tokens that are **not masked**, - 0 for "
"tokens that are **masked**.  It's data type should be `float32` and it "
"has a shape of [batch_size, sequence_length]. Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:15
msgid ""
"Mask to avoid performing attention on padding token indices with values "
"being either 0 or 1:"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:17
msgid "1 for tokens that are **not masked**,"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:18
msgid "0 for tokens that are **masked**."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:20
msgid ""
"It's data type should be `float32` and it has a shape of [batch_size, "
"sequence_length]. Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:23
msgid ""
"Contains pre-computed hidden-states. Can be used to speed up sequential "
"decoding. It's a list (has a length of n_layers) of Tensors (has a data "
"type of `float32`). `use_mems` has to be set to `True` to make use of "
"`mems`. Defaults to ``None``, and we don't use mems."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:28
msgid ""
"Mask to indicate the attention pattern for each input token with values "
"being either 0 or 1.  - if ``perm_mask[k, i, j] = 0``, i attend to j in "
"batch k; - if ``perm_mask[k, i, j] = 1``, i does not attend to j in batch"
" k.  Only used during pretraining (to define factorization order) or for "
"sequential decoding (generation). It's data type should be `float32` and "
"it has a shape of [batch_size, sequence_length, sequence_length]. "
"Defaults to ``None``, and each token attends to all the others (full "
"bidirectional attention)."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:28
msgid ""
"Mask to indicate the attention pattern for each input token with values "
"being either 0 or 1."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:30
msgid "if ``perm_mask[k, i, j] = 0``, i attend to j in batch k;"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:31
msgid "if ``perm_mask[k, i, j] = 1``, i does not attend to j in batch k."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:33
msgid ""
"Only used during pretraining (to define factorization order) or for "
"sequential decoding (generation). It's data type should be `float32` and "
"it has a shape of [batch_size, sequence_length, sequence_length]. "
"Defaults to ``None``, and each token attends to all the others (full "
"bidirectional attention)."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:37
msgid ""
"Mask to indicate the output tokens to use with values being either 0 or "
"1. If ``target_mapping[k, i, j] = 1``, the i-th predict in batch k is on "
"the j-th token. Only used during pretraining for partial prediction or "
"for sequential decoding (generation). It's data type should be `float32` "
"and it has a shape of [batch_size, num_predict, sequence_length]. "
"Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:43
msgid ""
"Mask to avoid performing attention on padding token indices. Negative of "
"`attention_mask`, i.e. with 0 for real tokens and 1 for padding. Mask "
"values can either be 0 or 1:  - 1 for tokens that are **masked**, - 0 for"
" tokens that are **not masked**.  You can only uses one of `input_mask` "
"and `attention_mask`. It's data type should be `float32` and it has a "
"shape of [batch_size, sequence_length]. Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:43
msgid ""
"Mask to avoid performing attention on padding token indices. Negative of "
"`attention_mask`, i.e. with 0 for real tokens and 1 for padding. Mask "
"values can either be 0 or 1:"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:47
msgid "1 for tokens that are **masked**,"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:48
msgid "0 for tokens that are **not masked**."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:50
msgid ""
"You can only uses one of `input_mask` and `attention_mask`. It's data "
"type should be `float32` and it has a shape of [batch_size, "
"sequence_length]. Defaults to ``None``."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:54
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" can either be 0 or 1:  - 1 indicates the head is **not masked**, - 0 "
"indicates the head is **masked**.  It's data type should be `float32` and"
" has a shape of [num_heads] or [num_layers, num_heads]. Defaults to "
"``None``, which means we keep all heads."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:54
msgid ""
"Mask to nullify selected heads of the self-attention modules. Mask values"
" can either be 0 or 1:"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:57
msgid "1 indicates the head is **not masked**,"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:58
msgid "0 indicates the head is **masked**."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:60
msgid ""
"It's data type should be `float32` and has a shape of [num_heads] or "
"[num_layers, num_heads]. Defaults to ``None``, which means we keep all "
"heads."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:63
msgid ""
"An embedded representation tensor which is an alternative of `input_ids`."
" You should only specify one of them to avoid contradiction. It's data "
"type should be `float32` and has a shape of [batch_size, sequence_length,"
" hidden_size]. Defaults to ``None``, which means we only specify "
"`input_ids`."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:68
msgid ""
"Whether or not to use recurrent memory mechanism during training. "
"Defaults to ``False`` and we don't use recurrent memory mechanism in "
"training mode."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:71
msgid ""
"Whether or not to use recurrent memory mechanism during evaluation. "
"Defaults to ``False`` and we don't use recurrent memory mechanism in "
"evaluation mode."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:74
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"Defaults to ``False`` and we don't return the attentions tensors."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:77
msgid ""
"Whether or not to return the hidden states of all layers. Defaults to "
"``False`` and we don't return the hidden states."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:80
msgid ""
"Whether or not to format the output as a `dict`. Defaults to ``False``, "
"and the default output is a `tuple`."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:84
msgid ""
"A tuple of shape (``output``, ``new_mems``, ``hidden_states``, "
"``attentions``) or a dict of shape {\"last_hidden_state\": ``output``, "
"\"mems\": ``new_mems``, \"hidden_states\": ``hidden_states``, "
"\"attentions\": ``attentions``}.  With the fields:  - output (`Tensor`):"
"     Sequence of hidden-states at the last layer of the model.     It's "
"data type should be float32 and has a shape of [batch_size, num_predict, "
"hidden_size].     ``num_predict`` corresponds to "
"``target_mapping.shape[1]``. If ``target_mapping`` is ``None``,     then "
"``num_predict`` corresponds to ``sequence_length``. - mems "
"(`List[Tensor]`):     A Tensor list of length 'n_layers' containing pre-"
"computed hidden-states. - hidden_states (`List[Tensor]`, optional):     A"
" Tensor list containing hidden-states of the model at the output of each "
"layer plus     the initial embedding outputs. Each Tensor has a data type"
" of `float32` and     has a shape of [batch_size, sequence_length, "
"hidden_size]. - attentions (`List[Tensor]`, optional):     A Tensor list "
"containing attentions weights after the attention softmax, used to "
"compute     the weighted average in the self-attention heads. Each Tensor"
" (one for each layer) has a data type     of `float32` and has a shape of"
" [batch_size, num_heads, sequence_length, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:32
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:32
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:84
msgid ""
"A tuple of shape (``output``, ``new_mems``, ``hidden_states``, "
"``attentions``) or a dict of shape {\"last_hidden_state\": ``output``, "
"\"mems\": ``new_mems``, \"hidden_states\": ``hidden_states``, "
"\"attentions\": ``attentions``}."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:36
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:36
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:88
msgid "With the fields:"
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:39
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:39
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:93
msgid "output (`Tensor`):"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:91
msgid ""
"Sequence of hidden-states at the last layer of the model. It's data type "
"should be float32 and has a shape of [batch_size, num_predict, "
"hidden_size]. ``num_predict`` corresponds to ``target_mapping.shape[1]``."
" If ``target_mapping`` is ``None``, then ``num_predict`` corresponds to "
"``sequence_length``."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:41
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:41
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:95
msgid "mems (`List[Tensor]`):"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:96
msgid "A Tensor list of length 'n_layers' containing pre-computed hidden-states."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:43
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:43
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:99
msgid "hidden_states (`List[Tensor]`, optional):"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:98
msgid ""
"A Tensor list containing hidden-states of the model at the output of each"
" layer plus the initial embedding outputs. Each Tensor has a data type of"
" `float32` and has a shape of [batch_size, sequence_length, hidden_size]."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:45
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:45
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:103
msgid "attentions (`List[Tensor]`, optional):"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:102
msgid ""
"A Tensor list containing attentions weights after the attention softmax, "
"used to compute the weighted average in the self-attention heads. Each "
"Tensor (one for each layer) has a data type of `float32` and has a shape "
"of [batch_size, num_heads, sequence_length, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:47
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:47
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:105
msgid "A `tuple` or a `dict`"
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:50
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:50
#: paddlenlp.transformers.xlnet.modeling.XLNetModel.forward:108
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetPretrainedModel:1
msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetPretrainedModel:1
msgid ""
"An abstract class for pretrained XLNet models. It provides XLNet related "
"``model_config_file``, ``resource_files_names``, "
"``pretrained_resource_files_map``, ``pretrained_init_configuration``, "
"``base_model_prefix`` for downloading and loading pretrained models. See "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel` for more "
"details."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification:1
msgid ""
"XLNet Model with a sequence classification/regression head on top (a "
"linear layer on top of the pooled output) e.g. for GLUE tasks."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification:4
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification:4
msgid "An instance of :class:`XLNetModel`."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification:6
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification:6
msgid "The number of classes. Defaults to ``2``."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:1
msgid ""
"The XLNetForSequenceClassification forward method, overrides the "
"__call__() special method."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:3
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:5
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:7
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:9
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:11
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:13
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:15
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:17
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:19
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:21
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:23
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:25
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:27
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:29
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:42
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:44
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:46
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:3
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:5
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:7
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:9
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:11
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:13
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:15
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:17
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:19
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:21
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:23
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:25
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:27
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:29
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:42
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:44
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:46
msgid "See :class:`XLNetModel`."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:32
msgid ""
"A tuple of shape (``output``, ``new_mems``, ``hidden_states``, "
"``attentions``) or a dict of shape {\"last_hidden_state\": ``output``, "
"\"mems\": ``new_mems``, \"hidden_states\": ``hidden_states``, "
"\"attentions\": ``attentions``}.  With the fields:  output (`Tensor`):"
"     Classification scores before SoftMax (also called logits). It's data"
" type should be float32     and has a shape of [batch_size, num_classes]."
" mems (`List[Tensor]`):     See :class:`XLNetModel`. hidden_states "
"(`List[Tensor]`, optional):     See :class:`XLNetModel`. attentions "
"(`List[Tensor]`, optional):     See :class:`XLNetModel`."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForSequenceClassification.forward:39
msgid ""
"Classification scores before SoftMax (also called logits). It's data type"
" should be float32 and has a shape of [batch_size, num_classes]."
msgstr ""

#: of paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification:1
msgid ""
"XLNet Model with a token classification head on top (a linear layer on "
"top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) "
"tasks."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:1
msgid ""
"The XLNetForTokenClassification forward method, overrides the __call__() "
"special method."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:32
msgid ""
"A tuple of shape (``output``, ``new_mems``, ``hidden_states``, "
"``attentions``) or a dict of shape {\"last_hidden_state\": ``output``, "
"\"mems\": ``new_mems``, \"hidden_states\": ``hidden_states``, "
"\"attentions\": ``attentions``}.  With the fields:  - output (`Tensor`):"
"     Classification scores before SoftMax (also called logits). It's data"
" type should be float32     and has a shape of [batch_size, "
"sequence_length, num_classes]. - mems (`List[Tensor]`):     See "
":class:`XLNetModel`. - hidden_states (`List[Tensor]`, optional):     See "
":class:`XLNetModel`. - attentions (`List[Tensor]`, optional):     See "
":class:`XLNetModel`."
msgstr ""

#: of
#: paddlenlp.transformers.xlnet.modeling.XLNetForTokenClassification.forward:39
msgid ""
"Classification scores before SoftMax (also called logits). It's data type"
" should be float32 and has a shape of [batch_size, sequence_length, "
"num_classes]."
msgstr ""

