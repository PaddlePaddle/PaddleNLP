# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-16 16:04+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.xlnet.modeling.rst:2
msgid "modeling"
msgstr ""

#~ msgid "Modeling classes for XLNet model."
#~ msgstr ""

#~ msgid "基类：:class:`paddlenlp.transformers.xlnet.modeling.XLNetPretrainedModel`"
#~ msgstr ""

#~ msgid ""
#~ "The bare XLNet Model transformer "
#~ "outputting raw hidden-states without any"
#~ " specific head on top."
#~ msgstr ""

#~ msgid ""
#~ "This model inherits from "
#~ ":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. "
#~ "Check the superclass documentation for "
#~ "the generic methods and the library "
#~ "implements for all its model."
#~ msgstr ""

#~ msgid ""
#~ "This model is also a Paddle "
#~ "`paddle.nn.Layer <https://www.paddlepaddle.org.cn/documentation"
#~ " /docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ "
#~ "subclass. Use it as a regular "
#~ "Paddle Layer and refer to the "
#~ "Paddle documentation for all matter "
#~ "related to general usage and behavior."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid ""
#~ "Vocabulary size of the XLNet model. "
#~ "Defines the number of different tokens"
#~ " that can be represented by the "
#~ "`inputs_ids` passed when calling XLNetModel."
#~ msgstr ""

#~ msgid ""
#~ "The number of tokens to cache. The"
#~ " key/value pairs that have already "
#~ "been pre-computed in a previous "
#~ "forward pass won't be re-computed. "
#~ "Defaults to ``None``."
#~ msgstr ""

#~ msgid ""
#~ "The number of tokens in the "
#~ "current batch to be cached and "
#~ "reused in the future. Defaults to "
#~ "``None``."
#~ msgstr ""

#~ msgid ""
#~ "Dimensionality of the encoder layers and"
#~ " the pooler layer. Defaults to "
#~ "``768``."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to use the same "
#~ "attention length for each token. "
#~ "Defaults to ``False``."
#~ msgstr ""

#~ msgid ""
#~ "The attention type used by the "
#~ "model. Set `\"bi\"` for XLNet, `\"uni\"`"
#~ " for Transformer-XL. Defaults to "
#~ "``\"bi\"``."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to use bidirectional "
#~ "input pipeline. Usually set to `True`"
#~ " during pretraining and `False` during "
#~ "fine-tuning. Defaults to ``False``."
#~ msgstr ""

#~ msgid ""
#~ "Clamp all relative distances larger than"
#~ " clamp_len. Setting this attribute to "
#~ "-1 means no clamping. Defaults to "
#~ "``-1``."
#~ msgstr ""

#~ msgid "Number of hidden layers in the Transformer encoder. Defaults to ``12``."
#~ msgstr ""

#~ msgid ""
#~ "The dropout probability for all fully"
#~ " connected layers in the embeddings "
#~ "and encoder. Defaults to ``0.1``."
#~ msgstr ""

#~ msgid ""
#~ "The dropout probability for all fully"
#~ " connected layers in the pooler. "
#~ "Defaults to ``0.1``."
#~ msgstr ""

#~ msgid ""
#~ "Number of attention heads for each "
#~ "attention layer in the Transformer "
#~ "encoder. Defaults to ``12``."
#~ msgstr ""

#~ msgid ""
#~ "Dimensionality of the \"intermediate\" (often"
#~ " named feed-forward) layer in the "
#~ "Transformer encoder. Defaults to ``64``."
#~ msgstr ""

#~ msgid ""
#~ "The epsilon used by the layer "
#~ "normalization layers. Defaults to ``1e-12``."
#~ msgstr ""

#~ msgid ""
#~ "Dimensionality of the \"intermediate\" (often"
#~ " named feed-forward) layer in the "
#~ "Transformer encoder. Defaults to ``3072``."
#~ msgstr ""

#~ msgid ""
#~ "The non-linear activation function in"
#~ " the feed-forward layer. ``\"gelu\"``, "
#~ "``\"relu\"``, ``\"silu\"`` and ``\"gelu_new\"`` "
#~ "are supported. Defaults to ``\"gelu\"``."
#~ msgstr ""

#~ msgid ""
#~ "The standard deviation of the "
#~ "truncated_normal_initializer for initializing all"
#~ " weight matrices. Defaults to ``0.02``."
#~ msgstr ""

#~ msgid "The XLNetModel forward method, overrides the __call__() special method."
#~ msgstr ""

#~ msgid ""
#~ "Indices of input sequence tokens in "
#~ "the vocabulary. It's data type should"
#~ " be int64 and it has a shape"
#~ " of [batch_size, sequence_length]."
#~ msgstr ""

#~ msgid ""
#~ "Segment token indices to indicate first"
#~ " and second portions of the inputs."
#~ " Indices can either be 0 or 1:"
#~ "  - 0 corresponds to a *sentence "
#~ "A* token, - 1 corresponds to a "
#~ "*sentence B* token.  It's data type "
#~ "should be `int64` and it has a "
#~ "shape of [batch_size, sequence_length]. "
#~ "Defaults to ``None``, which means we "
#~ "don't add segment embeddings."
#~ msgstr ""

#~ msgid ""
#~ "Segment token indices to indicate first"
#~ " and second portions of the inputs."
#~ " Indices can either be 0 or 1:"
#~ msgstr ""

#~ msgid "0 corresponds to a *sentence A* token,"
#~ msgstr ""

#~ msgid "1 corresponds to a *sentence B* token."
#~ msgstr ""

#~ msgid ""
#~ "It's data type should be `int64` "
#~ "and it has a shape of [batch_size,"
#~ " sequence_length]. Defaults to ``None``, "
#~ "which means we don't add segment "
#~ "embeddings."
#~ msgstr ""

#~ msgid ""
#~ "Mask to avoid performing attention on"
#~ " padding token indices with values "
#~ "being either 0 or 1:  - 1 "
#~ "for tokens that are **not masked**, "
#~ "- 0 for tokens that are "
#~ "**masked**.  It's data type should be"
#~ " `float32` and it has a shape "
#~ "of [batch_size, sequence_length]. Defaults to"
#~ " ``None``."
#~ msgstr ""

#~ msgid ""
#~ "Mask to avoid performing attention on"
#~ " padding token indices with values "
#~ "being either 0 or 1:"
#~ msgstr ""

#~ msgid "1 for tokens that are **not masked**,"
#~ msgstr ""

#~ msgid "0 for tokens that are **masked**."
#~ msgstr ""

#~ msgid ""
#~ "It's data type should be `float32` "
#~ "and it has a shape of [batch_size,"
#~ " sequence_length]. Defaults to ``None``."
#~ msgstr ""

#~ msgid ""
#~ "Contains pre-computed hidden-states. Can"
#~ " be used to speed up sequential "
#~ "decoding. It's a list (has a "
#~ "length of n_layers) of Tensors (has "
#~ "a data type of `float32`). `use_mems`"
#~ " has to be set to `True` to "
#~ "make use of `mems`. Defaults to "
#~ "``None``, and we don't use mems."
#~ msgstr ""

#~ msgid ""
#~ "Mask to indicate the attention pattern"
#~ " for each input token with values "
#~ "being either 0 or 1.  - if "
#~ "``perm_mask[k, i, j] = 0``, i "
#~ "attend to j in batch k; - if"
#~ " ``perm_mask[k, i, j] = 1``, i "
#~ "does not attend to j in batch "
#~ "k.  Only used during pretraining (to "
#~ "define factorization order) or for "
#~ "sequential decoding (generation). It's data"
#~ " type should be `float32` and it "
#~ "has a shape of [batch_size, "
#~ "sequence_length, sequence_length]. Defaults to "
#~ "``None``, and each token attends to "
#~ "all the others (full bidirectional "
#~ "attention)."
#~ msgstr ""

#~ msgid ""
#~ "Mask to indicate the attention pattern"
#~ " for each input token with values "
#~ "being either 0 or 1."
#~ msgstr ""

#~ msgid "if ``perm_mask[k, i, j] = 0``, i attend to j in batch k;"
#~ msgstr ""

#~ msgid "if ``perm_mask[k, i, j] = 1``, i does not attend to j in batch k."
#~ msgstr ""

#~ msgid ""
#~ "Only used during pretraining (to define"
#~ " factorization order) or for sequential "
#~ "decoding (generation). It's data type "
#~ "should be `float32` and it has a"
#~ " shape of [batch_size, sequence_length, "
#~ "sequence_length]. Defaults to ``None``, and"
#~ " each token attends to all the "
#~ "others (full bidirectional attention)."
#~ msgstr ""

#~ msgid ""
#~ "Mask to indicate the output tokens "
#~ "to use with values being either 0"
#~ " or 1. If ``target_mapping[k, i, j]"
#~ " = 1``, the i-th predict in "
#~ "batch k is on the j-th token. "
#~ "Only used during pretraining for partial"
#~ " prediction or for sequential decoding "
#~ "(generation). It's data type should be"
#~ " `float32` and it has a shape "
#~ "of [batch_size, num_predict, sequence_length]. "
#~ "Defaults to ``None``."
#~ msgstr ""

#~ msgid ""
#~ "Mask to avoid performing attention on"
#~ " padding token indices. Negative of "
#~ "`attention_mask`, i.e. with 0 for real"
#~ " tokens and 1 for padding. Mask "
#~ "values can either be 0 or 1:  "
#~ "- 1 for tokens that are "
#~ "**masked**, - 0 for tokens that "
#~ "are **not masked**.  You can only "
#~ "uses one of `input_mask` and "
#~ "`attention_mask`. It's data type should "
#~ "be `float32` and it has a shape"
#~ " of [batch_size, sequence_length]. Defaults "
#~ "to ``None``."
#~ msgstr ""

#~ msgid ""
#~ "Mask to avoid performing attention on"
#~ " padding token indices. Negative of "
#~ "`attention_mask`, i.e. with 0 for real"
#~ " tokens and 1 for padding. Mask "
#~ "values can either be 0 or 1:"
#~ msgstr ""

#~ msgid "1 for tokens that are **masked**,"
#~ msgstr ""

#~ msgid "0 for tokens that are **not masked**."
#~ msgstr ""

#~ msgid ""
#~ "You can only uses one of "
#~ "`input_mask` and `attention_mask`. It's data"
#~ " type should be `float32` and it "
#~ "has a shape of [batch_size, "
#~ "sequence_length]. Defaults to ``None``."
#~ msgstr ""

#~ msgid ""
#~ "Mask to nullify selected heads of "
#~ "the self-attention modules. Mask values"
#~ " can either be 0 or 1:  - "
#~ "1 indicates the head is **not "
#~ "masked**, - 0 indicates the head "
#~ "is **masked**.  It's data type should"
#~ " be `float32` and has a shape "
#~ "of [num_heads] or [num_layers, num_heads]. "
#~ "Defaults to ``None``, which means we "
#~ "keep all heads."
#~ msgstr ""

#~ msgid ""
#~ "Mask to nullify selected heads of "
#~ "the self-attention modules. Mask values"
#~ " can either be 0 or 1:"
#~ msgstr ""

#~ msgid "1 indicates the head is **not masked**,"
#~ msgstr ""

#~ msgid "0 indicates the head is **masked**."
#~ msgstr ""

#~ msgid ""
#~ "It's data type should be `float32` "
#~ "and has a shape of [num_heads] or"
#~ " [num_layers, num_heads]. Defaults to "
#~ "``None``, which means we keep all "
#~ "heads."
#~ msgstr ""

#~ msgid ""
#~ "An embedded representation tensor which "
#~ "is an alternative of `input_ids`. You"
#~ " should only specify one of them "
#~ "to avoid contradiction. It's data type"
#~ " should be `float32` and has a "
#~ "shape of [batch_size, sequence_length, "
#~ "hidden_size]. Defaults to ``None``, which "
#~ "means we only specify `input_ids`."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to use recurrent "
#~ "memory mechanism during training. Defaults "
#~ "to ``False`` and we don't use "
#~ "recurrent memory mechanism in training "
#~ "mode."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to use recurrent "
#~ "memory mechanism during evaluation. Defaults"
#~ " to ``False`` and we don't use "
#~ "recurrent memory mechanism in evaluation "
#~ "mode."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to return the "
#~ "attentions tensors of all attention "
#~ "layers. Defaults to ``False`` and we "
#~ "don't return the attentions tensors."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to return the "
#~ "hidden states of all layers. Defaults"
#~ " to ``False`` and we don't return "
#~ "the hidden states."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to format the "
#~ "output as a `dict`. Defaults to "
#~ "``False``, and the default output is "
#~ "a `tuple`."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid ""
#~ "A tuple of shape (``output``, "
#~ "``new_mems``, ``hidden_states``, ``attentions``) or"
#~ " a dict of shape {\"last_hidden_state\":"
#~ " ``output``, \"mems\": ``new_mems``, "
#~ "\"hidden_states\": ``hidden_states``, \"attentions\": "
#~ "``attentions``}.  With the fields:  - "
#~ "output (`Tensor`):     Sequence of hidden-"
#~ "states at the last layer of the"
#~ " model.     It's data type should be"
#~ " float32 and has a shape of "
#~ "[batch_size, num_predict, hidden_size].     "
#~ "``num_predict`` corresponds to "
#~ "``target_mapping.shape[1]``. If ``target_mapping`` "
#~ "is ``None``,     then ``num_predict`` "
#~ "corresponds to ``sequence_length``. - mems "
#~ "(`List[Tensor]`):     A Tensor list of "
#~ "length 'n_layers' containing pre-computed "
#~ "hidden-states. - hidden_states "
#~ "(`List[Tensor]`, optional):     A Tensor list"
#~ " containing hidden-states of the "
#~ "model at the output of each layer"
#~ " plus     the initial embedding outputs."
#~ " Each Tensor has a data type of"
#~ " `float32` and     has a shape of "
#~ "[batch_size, sequence_length, hidden_size]. - "
#~ "attentions (`List[Tensor]`, optional):     A "
#~ "Tensor list containing attentions weights "
#~ "after the attention softmax, used to "
#~ "compute     the weighted average in the"
#~ " self-attention heads. Each Tensor "
#~ "(one for each layer) has a data"
#~ " type     of `float32` and has a "
#~ "shape of [batch_size, num_heads, "
#~ "sequence_length, sequence_length]."
#~ msgstr ""

#~ msgid ""
#~ "A tuple of shape (``output``, "
#~ "``new_mems``, ``hidden_states``, ``attentions``) or"
#~ " a dict of shape {\"last_hidden_state\":"
#~ " ``output``, \"mems\": ``new_mems``, "
#~ "\"hidden_states\": ``hidden_states``, \"attentions\": "
#~ "``attentions``}."
#~ msgstr ""

#~ msgid "With the fields:"
#~ msgstr ""

#~ msgid "output (`Tensor`):"
#~ msgstr ""

#~ msgid ""
#~ "Sequence of hidden-states at the "
#~ "last layer of the model. It's data"
#~ " type should be float32 and has "
#~ "a shape of [batch_size, num_predict, "
#~ "hidden_size]. ``num_predict`` corresponds to "
#~ "``target_mapping.shape[1]``. If ``target_mapping`` "
#~ "is ``None``, then ``num_predict`` corresponds"
#~ " to ``sequence_length``."
#~ msgstr ""

#~ msgid "mems (`List[Tensor]`):"
#~ msgstr ""

#~ msgid ""
#~ "A Tensor list of length 'n_layers' "
#~ "containing pre-computed hidden-states."
#~ msgstr ""

#~ msgid "hidden_states (`List[Tensor]`, optional):"
#~ msgstr ""

#~ msgid ""
#~ "A Tensor list containing hidden-states"
#~ " of the model at the output of"
#~ " each layer plus the initial "
#~ "embedding outputs. Each Tensor has a "
#~ "data type of `float32` and has a"
#~ " shape of [batch_size, sequence_length, "
#~ "hidden_size]."
#~ msgstr ""

#~ msgid "attentions (`List[Tensor]`, optional):"
#~ msgstr ""

#~ msgid ""
#~ "A Tensor list containing attentions "
#~ "weights after the attention softmax, "
#~ "used to compute the weighted average "
#~ "in the self-attention heads. Each "
#~ "Tensor (one for each layer) has a"
#~ " data type of `float32` and has "
#~ "a shape of [batch_size, num_heads, "
#~ "sequence_length, sequence_length]."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "A `tuple` or a `dict`"
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
#~ msgstr ""

#~ msgid ""
#~ "An abstract class for pretrained XLNet"
#~ " models. It provides XLNet related "
#~ "``model_config_file``, ``resource_files_names``, "
#~ "``pretrained_resource_files_map``, "
#~ "``pretrained_init_configuration``, ``base_model_prefix`` "
#~ "for downloading and loading pretrained "
#~ "models. See "
#~ ":class:`~paddlenlp.transformers.model_utils.PretrainedModel` for"
#~ " more details."
#~ msgstr ""

#~ msgid ""
#~ "XLNet Model with a sequence "
#~ "classification/regression head on top (a "
#~ "linear layer on top of the pooled"
#~ " output) e.g. for GLUE tasks."
#~ msgstr ""

#~ msgid "An instance of :class:`XLNetModel`."
#~ msgstr ""

#~ msgid "The number of classes. Defaults to ``2``."
#~ msgstr ""

#~ msgid ""
#~ "The XLNetForSequenceClassification forward method,"
#~ " overrides the __call__() special method."
#~ msgstr ""

#~ msgid "See :class:`XLNetModel`."
#~ msgstr ""

#~ msgid ""
#~ "A tuple of shape (``output``, "
#~ "``new_mems``, ``hidden_states``, ``attentions``) or"
#~ " a dict of shape {\"last_hidden_state\":"
#~ " ``output``, \"mems\": ``new_mems``, "
#~ "\"hidden_states\": ``hidden_states``, \"attentions\": "
#~ "``attentions``}.  With the fields:  output "
#~ "(`Tensor`):     Classification scores before "
#~ "SoftMax (also called logits). It's data"
#~ " type should be float32     and has"
#~ " a shape of [batch_size, num_classes]. "
#~ "mems (`List[Tensor]`):     See :class:`XLNetModel`."
#~ " hidden_states (`List[Tensor]`, optional):     "
#~ "See :class:`XLNetModel`. attentions (`List[Tensor]`,"
#~ " optional):     See :class:`XLNetModel`."
#~ msgstr ""

#~ msgid ""
#~ "Classification scores before SoftMax (also "
#~ "called logits). It's data type should"
#~ " be float32 and has a shape of"
#~ " [batch_size, num_classes]."
#~ msgstr ""

#~ msgid ""
#~ "XLNet Model with a token classification"
#~ " head on top (a linear layer on"
#~ " top of the hidden-states output) "
#~ "e.g. for Named-Entity-Recognition (NER)"
#~ " tasks."
#~ msgstr ""

#~ msgid ""
#~ "The XLNetForTokenClassification forward method, "
#~ "overrides the __call__() special method."
#~ msgstr ""

#~ msgid ""
#~ "A tuple of shape (``output``, "
#~ "``new_mems``, ``hidden_states``, ``attentions``) or"
#~ " a dict of shape {\"last_hidden_state\":"
#~ " ``output``, \"mems\": ``new_mems``, "
#~ "\"hidden_states\": ``hidden_states``, \"attentions\": "
#~ "``attentions``}.  With the fields:  - "
#~ "output (`Tensor`):     Classification scores "
#~ "before SoftMax (also called logits). "
#~ "It's data type should be float32     "
#~ "and has a shape of [batch_size, "
#~ "sequence_length, num_classes]. - mems "
#~ "(`List[Tensor]`):     See :class:`XLNetModel`. - "
#~ "hidden_states (`List[Tensor]`, optional):     See"
#~ " :class:`XLNetModel`. - attentions "
#~ "(`List[Tensor]`, optional):     See "
#~ ":class:`XLNetModel`."
#~ msgstr ""

#~ msgid ""
#~ "Classification scores before SoftMax (also "
#~ "called logits). It's data type should"
#~ " be float32 and has a shape of"
#~ " [batch_size, sequence_length, num_classes]."
#~ msgstr ""

