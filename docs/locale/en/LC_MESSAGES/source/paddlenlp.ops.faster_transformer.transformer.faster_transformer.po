# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-24 16:20+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.ops.faster_transformer.transformer.faster_transformer.rst:2
msgid "faster\\_transformer"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer:1
msgid "基类：:class:`paddlenlp.transformers.transformer.modeling.TransformerModel`"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward:1
msgid ""
"The Transformer forward methods. The input are source/target sequences, "
"and returns logits."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterGPT.forward
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward
msgid "参数"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward:4
msgid ""
"The ids of source sequences words. It is a tensor with shape "
"`[batch_size, source_sequence_length]` and its data type can be int or "
"int64."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward:8
msgid ""
"The ids of target sequences words. It is a tensor with shape "
"`[batch_size, target_sequence_length]` and its data type can be int or "
"int64."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward
msgid "返回"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward:13
msgid ""
"Output tensor of the final layer of the model whose data type can be "
"float32 or float64 with shape `[batch_size, sequence_length, "
"vocab_size]`."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterTransformer.forward:19
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward:14
msgid "示例"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterGPT:1
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:1
msgid "基类：:class:`paddle.fluid.dygraph.layers.Layer`"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:1
msgid ""
"The Transformer model for auto-regressive generation. It wraps "
"`FasterTransformer` and `InferTransformerModel`, and automatically "
"chioces using `FasterTransformer` (with jit building) or the slower "
"verison `InferTransformerModel`."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:5
msgid "The size of source vocabulary."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:7
msgid "The size of target vocabulary."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:9
msgid "The maximum length of input sequences."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:11
msgid "The number of sub-layers to be stacked in the encoder."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:13
msgid "The number of sub-layers to be stacked in the decoder."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:15
msgid "The number of head used in multi-head attention."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:17
msgid ""
"The dimension for word embeddings, which is also the last dimension of "
"the input and output of multi-head attention, position-wise feed-forward "
"networks, encoder and decoder."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:21
msgid "Size of the hidden layer in position-wise feed-forward networks."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:23
msgid "Dropout rates. Used for pre-process, activation and inside attention."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:25
msgid "Whether to use weight sharing."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:27
msgid "The start token id and also is used as padding id. Defaults to 0."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:29
msgid "The end token id. Defaults to 1."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:31
msgid "The beam width for beam search. Defaults to 4."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:33
msgid "The maximum output length. Defaults to 256."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator:35
msgid ""
"The key word arguments can be `output_time_major`, `use_fp16_decoding` "
"and `use_ft`. `output_time_major(bool, optional)`: Indicate the data "
"layout of predicted Tensor. If `False`, the data layout would be batch "
"major with shape `[batch_size, seq_len, beam_size]`. If  `True`, the data"
" layout would be time major with shape `[seq_len, batch_size, "
"beam_size]`. Default to `False`. `use_fp16_decoding(bool, optional)`: "
"Whether to use fp16 for decoding. `use_ft(bool, optional)`: Whether to "
"use Faster Transformer for decoding."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward:1
msgid "Performs decoding for transformer model."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward:3
msgid ""
"The ids of source sequence words. It is a tensor with shape `[batch_size,"
" source_sequence_length]` and its data type can be int or int64."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.TransformerGenerator.forward:8
msgid ""
"An int64 tensor shaped indicating the predicted ids. Its shape is "
"`[batch_size, seq_len, beam_size]` or `[seq_len, batch_size, beam_size]` "
"according to `output_time_major`."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterGPT.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterGPT.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of
#: paddlenlp.ops.faster_transformer.transformer.faster_transformer.FasterGPT.forward:6
msgid "unpacked dict arguments"
msgstr ""

