# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.attention_utils.rst:2
msgid "attention\\_utils"
msgstr ""

#: of paddlenlp.transformers.attention_utils.Attention.forward:1
#: paddlenlp.transformers.attention_utils.DefaultAttention.forward:1
#: paddlenlp.transformers.attention_utils.Linear3D.forward:1
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of paddlenlp.transformers.attention_utils.Attention.forward
#: paddlenlp.transformers.attention_utils.DefaultAttention.forward
#: paddlenlp.transformers.attention_utils.Linear3D.forward
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.attention_utils.Attention.forward:4
#: paddlenlp.transformers.attention_utils.DefaultAttention.forward:4
#: paddlenlp.transformers.attention_utils.Linear3D.forward:4
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of paddlenlp.transformers.attention_utils.Attention.forward:6
#: paddlenlp.transformers.attention_utils.DefaultAttention.forward:6
#: paddlenlp.transformers.attention_utils.Linear3D.forward:6
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.forward:6
msgid "unpacked dict arguments"
msgstr ""

#: of paddlenlp.transformers.attention_utils.BigBirdSparseAttention.forward:1
msgid ""
"query_matrix: [B, H, T, D] key_matrix: [B, H, T, D] value_matrix: [B, H, "
"T, D] query_mask: [B, 1, T, 1]  bool mask key_mask: [B, 1, 1, T]    bool "
"mask rand_mask_idx: [H, T//bs, bs] Global Attention Random Attention "
"Window Attention"
msgstr ""

#: ../docstring of
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.Cache.k:1
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.StaticCache.k:1
msgid "Alias for field number 0"
msgstr ""

#: ../docstring of
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.Cache.v:1
#: paddlenlp.transformers.attention_utils.MultiHeadAttention.StaticCache.v:1
msgid "Alias for field number 1"
msgstr ""

