# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-24 16:20+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.ernie.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining:1
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering:1
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification:1
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification:1
#: paddlenlp.transformers.ernie.modeling.ErnieModel:1
msgid "基类：:class:`paddlenlp.transformers.ernie.modeling.ErniePretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:1
msgid ""
"The bare ERNIE Model transformer outputting raw hidden-states without any"
" specific head on top."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:3
msgid ""
"This model inherits from "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. Refer to "
"the superclass documentation for the generic methods."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:6
msgid ""
"This model is also a Paddle `paddle.nn.Layer "
"<https://www.paddlepaddle.org.cn/documentation "
"/docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ subclass. Use "
"it as a regular Paddle Layer and refer to the Paddle documentation for "
"all matter related to general usage and behavior."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward
#: paddlenlp.transformers.ernie.modeling.ErnieModel
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward
#: paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:10
msgid ""
"Vocabulary size of the ERNIE model. Also is the vocab size of token "
"embedding matrix."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:12
msgid "Dimension of the encoder layers and the pooler layer. Defaults to ``768``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:14
msgid "Number of hidden layers in the Transformer encoder. Defaults to ``12``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:16
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to ``12``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:19
msgid ""
"Dimension of the \"intermediate\" (often named feed-forward) layer in the"
" Transformer encoder. Defaults to ``3072``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:22
msgid ""
"The non-linear activation function in the feed-forward layer. "
"``\"gelu\"``, ``\"relu\"`` and any other paddle supported activation "
"functions are supported. Defaults to ``\"gelu\"``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:26
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to ``0.1``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:29
msgid ""
"The dropout probability for all fully connected layers in the pooler. "
"Defaults to ``0.1``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:32
msgid "The max position index of an input sequence. Defaults to ``512``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:34
msgid ""
"The vocabulary size of the `token_type_ids` passed when calling "
"`~transformers.ErnieModel`. Defaults to ``2``."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:37
msgid ""
"The standard deviation of the normal initializer. Defaults to 0.02.  .. "
"note::     A normal_initializer initializes weight matrices as normal "
"distributions.     See :meth:`ErniePretrainedModel._init_weights()` for "
"how weights are initialized in `ErnieModel`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:37
msgid "The standard deviation of the normal initializer. Defaults to 0.02."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:40
msgid ""
"A normal_initializer initializes weight matrices as normal distributions."
" See :meth:`ErniePretrainedModel._init_weights()` for how weights are "
"initialized in `ErnieModel`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel:43
msgid "The pad token index in the token vocabulary."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:1
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:1
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:1
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:1
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:1
msgid ""
"Indices of input sequence tokens in the vocabulary. They are numerical "
"representations of tokens that build the input sequence. It's data type "
"should be `int64` and has a shape of [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:5
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices can be either 0 or 1:  - 0 corresponds to a **sentence "
"A** token, - 1 corresponds to a **sentence B** token.  It's data type "
"should be `int64` and has a shape of [batch_size, sequence_length]. "
"Defaults to None, which means no segment embeddings is added to token "
"embeddings."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:5
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:5
msgid ""
"Segment token indices to indicate first and second portions of the "
"inputs. Indices can be either 0 or 1:"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:8
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:8
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:8
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:8
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:8
msgid "0 corresponds to a **sentence A** token,"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:9
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:9
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:9
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:9
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:9
msgid "1 corresponds to a **sentence B** token."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:11
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:11
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:11
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:11
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:11
msgid ""
"It's data type should be `int64` and has a shape of [batch_size, "
"sequence_length]. Defaults to None, which means no segment embeddings is "
"added to token embeddings."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:14
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:14
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:14
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:14
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:14
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, config.max_position_embeddings - "
"1]``. Defaults to `None`. Shape as `(batch_sie, num_tokens)` and dtype as"
" `int32` or `int64`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:18
msgid ""
"Mask to indicate whether to perform attention on each input token or not."
" The values should be either 0 or 1. The attention scores will be set to "
"**-infinity** for any positions in the mask that are **0**, and will be "
"**unchanged** for positions that are **1**.  - **1** for tokens that are "
"**not masked**, - **0** for tokens that are **masked**.  It's data type "
"should be `float32` and has a shape of [batch_size, sequence_length]. "
"Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:18
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:18
msgid ""
"Mask to indicate whether to perform attention on each input token or not."
" The values should be either 0 or 1. The attention scores will be set to "
"**-infinity** for any positions in the mask that are **0**, and will be "
"**unchanged** for positions that are **1**."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:23
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:23
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:23
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:23
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:23
msgid "**1** for tokens that are **not masked**,"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:24
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:24
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:24
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:24
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:24
msgid "**0** for tokens that are **masked**."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:26
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:26
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:26
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:26
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:26
msgid ""
"It's data type should be `float32` and has a shape of [batch_size, "
"sequence_length]. Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:30
msgid ""
"A tuple of shape (``sequence_output``, ``pooled_output``).  With the "
"fields: - sequence_output (Tensor):     Sequence of hidden-states at the "
"last layer of the model.     It's data type should be `float` and has a "
"shape of `(batch_size, seq_lens, hidden_size)`.     ``seq_lens`` "
"corresponds to the length of input sequence. - pooled_output (Tensor):"
"     A Tensor of the first token representation.     It's data type "
"should be `float` and has a shape of `(batch_size, hidden_size]`.     We "
"\"pool\" the model by simply taking the hidden state corresponding to the"
" first token."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:30
msgid "A tuple of shape (``sequence_output``, ``pooled_output``)."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:32
msgid "With the fields: - sequence_output (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:34
msgid ""
"Sequence of hidden-states at the last layer of the model. It's data type "
"should be `float` and has a shape of `(batch_size, seq_lens, "
"hidden_size)`. ``seq_lens`` corresponds to the length of input sequence."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:40
msgid "pooled_output (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieModel.forward:38
msgid ""
"A Tensor of the first token representation. It's data type should be "
"`float` and has a shape of `(batch_size, hidden_size]`. We \"pool\" the "
"model by simply taking the hidden state corresponding to the first token."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:37
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:37
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:35
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:35
#: paddlenlp.transformers.ernie.modeling.ErnieModel.forward:43
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainedModel:1
msgid "基类：:class:`paddlenlp.transformers.model_utils.PretrainedModel`"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainedModel:1
msgid ""
"An abstract class for pretrained ERNIE models. It provides ERNIE related "
"`model_config_file`, `resource_files_names`, "
"`pretrained_resource_files_map`, `pretrained_init_configuration`, "
"`base_model_prefix` for downloading and loading pretrained models. Refer "
"to :class:`~paddlenlp.transformers.model_utils.PretrainedModel` for more "
"details."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainedModel.init_weights:1
msgid "Initialization hook"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification:1
msgid "Model for sentence (pair) classification task with ERNIE."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification:3
msgid "An instance of `paddlenlp.transformers.ErnieModel`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification:5
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification:7
msgid "The number of classes. Default to `2`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification:7
msgid ""
"The dropout probability for output of ERNIE. If None, use the same value "
"as `hidden_dropout_prob` of `paddlenlp.transformers.ErnieModel` instance."
" Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward:30
msgid ""
"A Tensor of the input text classification logits. Shape as `(batch_size, "
"num_classes)` and dtype as `float`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification.forward
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward
msgid "返回类型"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification:1
msgid ""
"ERNIE Model transformer with a sequence classification/regression head on"
" top (a linear layer on top of the pooledoutput) e.g. for GLUE tasks."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering:4
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification:5
msgid "An instance of `ErnieModel`."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification:9
msgid ""
"The dropout probability for output of ERNIE. If None, use the same value "
"as `hidden_dropout_prob` of `ErnieModel` instance `ernie`. Defaults to "
"`None`."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification.forward:30
msgid ""
"A Tensor of the input text classification logits, shape as (batch_size, "
"seq_lens, `num_classes`). seq_lens mean the number of tokens of the input"
" sequence."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering:1
msgid "Model for Question and Answering task with ERNIE."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:30
msgid ""
"A tuple of shape (``start_logits``, ``end_logits``).  With the fields: - "
"start_logits(Tensor): The logits of start position of prediction answer. "
"- end_logits(Tensor): The logits of end position of prediction answer."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:30
msgid "A tuple of shape (``start_logits``, ``end_logits``)."
msgstr ""

#: of
#: paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering.forward:32
msgid ""
"With the fields: - start_logits(Tensor): The logits of start position of "
"prediction answer. - end_logits(Tensor): The logits of end position of "
"prediction answer."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining:1
msgid ""
"Bert Model with two heads on top as done during the pretraining: a "
"`masked language modeling` head and a `next sentence prediction "
"(classification)` head."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:30
msgid ""
"A tuple of shape (``prediction_scores``, ``seq_relationship_score``).  "
"With the fields: - prediction_scores(Tensor): The scores of prediction on"
" masked token. - seq_relationship_score(Tensor): The scores of next "
"sentence prediction."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:30
msgid "A tuple of shape (``prediction_scores``, ``seq_relationship_score``)."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErnieForPretraining.forward:32
msgid ""
"With the fields: - prediction_scores(Tensor): The scores of prediction on"
" masked token. - seq_relationship_score(Tensor): The scores of next "
"sentence prediction."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion:1
msgid "基类：:class:`paddle.fluid.dygraph.layers.Layer`"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion:1
msgid ""
"The loss output of Bert Model during the pretraining: a `masked language "
"modeling` head and a `next sentence prediction (classification)` head."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of paddlenlp.transformers.ernie.modeling.ErniePretrainingCriterion.forward:6
msgid "unpacked dict arguments"
msgstr ""

