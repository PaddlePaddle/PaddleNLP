# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../source/paddlenlp.seq2vec.encoder.rst:2
msgid "encoder"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:1
#: paddlenlp.seq2vec.encoder.CNNEncoder:1
#: paddlenlp.seq2vec.encoder.GRUEncoder:1
#: paddlenlp.seq2vec.encoder.LSTMEncoder:1
#: paddlenlp.seq2vec.encoder.RNNEncoder:1
#: paddlenlp.seq2vec.encoder.TCNEncoder:1
msgid "基类：:class:`paddle.fluid.dygraph.layers.Layer`"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:1
msgid ""
"A `BoWEncoder` takes as input a sequence of vectors and returns a single "
"vector, which simply sums the embeddings of a sequence across the time "
"dimension. The input to this module is of shape `(batch_size, num_tokens,"
" emb_dim)`, and the output is of shape `(batch_size, emb_dim)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder
#: paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "参数"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:6
msgid "It is the input dimension to the encoder."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `BoWEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `BoWEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:1
msgid "It simply sums the embeddings of a sequence across the time dimension."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:3
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward:3
msgid "Shape as `(batch_size, num_tokens, emb_dim)`"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:5
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward:5
msgid ""
"`paddle.Tensor`, optional, defaults to `None`): Shape same as `inputs`. "
"Its each elements identify whether is padding token or not. If True, not "
"padding token. If False, padding token."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "返回"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:8
msgid "Shape of `(batch_size, emb_dim)`. The result vector of BagOfEmbedding."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "返回类型"
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:1
msgid ""
"A `CNNEncoder` takes as input a sequence of vectors and returns a single "
"vector, a combination of multiple convolution layers and max pooling "
"layers. The input to this module is of shape `(batch_size, num_tokens, "
"emb_dim)`, and the output is of shape `(batch_size, ouput_dim)` or "
"`(batch_size, len(ngram_filter_sizes) * num_filter)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:6
msgid ""
"The CNN has one convolution layer for each ngram filter size. Each "
"convolution operation gives out a vector of size num_filter. The number "
"of times a convolution layer will be used is `num_tokens - ngram_size + "
"1`. The corresponding maxpooling layer aggregates all these outputs from "
"the convolution layer and outputs the max."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:11
msgid ""
"This operation is repeated for every ngram size passed, and consequently "
"the dimensionality of the output after maxpooling is "
"`len(ngram_filter_sizes) * num_filter`.  This then gets (optionally) "
"projected down to a lower dimensional output, specified by `output_dim`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:15
msgid ""
"We then use a fully connected layer to project in back to the desired "
"output_dim.  For more details, refer to \"A Sensitivity Analysis of (and "
"Practitioners’ Guide to) Convolutional Neural Networks for Sentence "
"Classification\", Zhang and Wallace 2016, particularly Figure 1. ref: "
"https://arxiv.org/abs/1510.03820"
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:20
msgid "This is the input dimension to the encoder."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:22
msgid ""
"This is the output dim for each convolutional layer, which is the number "
"of \"filters\" learned by that layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:25
msgid ""
"This specifies both the number of convolutional layers we will create and"
" their sizes.  The default of `(2, 3, 4, 5)` will have four convolutional"
" layers, corresponding to encoding ngrams of size 2 to 5 with some number"
" of filters."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:29
msgid "Activation to use after the convolution layers."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:31
msgid ""
"After doing convolutions and pooling, we'll project the collected "
"features into a vector of this size.  If this value is `None`, we will "
"just return the result of the max pooling, giving an output of shape "
"`len(ngram_filter_sizes) * num_filter`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `CNNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `CNNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:1
msgid "The combination of multiple convolution layers and max pooling layers."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:9
msgid ""
"If output_dim is None, the result shape     is of `(batch_size, "
"output_dim)`; if not, the result shape     is of `(batch_size, "
"len(ngram_filter_sizes) * num_filter)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:12
msgid "If output_dim is None, the result shape"
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:12
msgid ""
"is of `(batch_size, output_dim)`; if not, the result shape is of "
"`(batch_size, len(ngram_filter_sizes) * num_filter)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:1
msgid ""
"A GRUEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple GRU layers. The input to this "
"module is of shape `(batch_size, num_tokens, input_size)`, The output is "
"of shape `(batch_size, hidden_size*2)` if GRU is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:7
msgid ""
"Paddle's GRU have two outputs: the hidden state for every time step at "
"last layer, and the hidden state at the last time step for every layer. "
"If `pooling_type` is None, we perform the pooling on the hidden state of "
"every time step at last layer to create a single vector. If not None, we "
"use the hidden state of the last time step at last layer as a single "
"output (shape of `(batch_size, hidden_size)`); And if direction is "
"bidirection, the we concat the hidden state of the last forward gru and "
"backward gru layer to create a single vector (shape of `(batch_size, "
"hidden_size*2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:15
#: paddlenlp.seq2vec.encoder.RNNEncoder:15
msgid ""
"`int`, required): The number of expected features in the input (the last "
"dimension)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:16
#: paddlenlp.seq2vec.encoder.RNNEncoder:16
msgid "`int`, required): The number of features in the hidden state."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:17
msgid ""
"`int`, optional, defaults to 1): Number of recurrent layers. E.g., "
"setting num_layers=2 would mean stacking two GRUs together to form a "
"stacked GRU, with the second GRU taking in outputs of the first GRU and "
"computing the final results."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:20
msgid ""
"`str`, optional, defaults to obj:`forward`): The direction of the "
"network. It can be `forward` and `bidirect` (it means bidirection "
"network). If `biderect`, it is a birectional GRU, and returns the concat "
"output from both directions."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:23
msgid ""
"`float`, optional, defaults to 0.0): If non-zero, introduces a Dropout "
"layer on the outputs of each GRU layer except the last layer, with "
"dropout probability equal to dropout."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:25
msgid ""
"`str`, optional, defaults to obj:`None`): If `pooling_type` is None, then"
" the GRUEncoder will return the hidden state of the last time step at "
"last layer as a single vector. If pooling_type is not None, it must be "
"one of `sum`, `max` and `mean`. Then it will be pooled on the GRU output "
"(the hidden state of every time step at last layer) to create a single "
"vector."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `GRUEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `GRUEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:1
msgid ""
"GRUEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple GRU layers. The input to this "
"module is of shape `(batch_size, num_tokens, input_size)`, The output is "
"of shape `(batch_size, hidden_size*2)` if GRU is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:7
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:7
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:7
msgid "Shape as `(batch_size, num_tokens, input_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:9
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:9
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:9
msgid "Shape as `(batch_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:12
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:12
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:12
msgid ""
"Shape as `(batch_size, hidden_size)`.     The hidden state at the last "
"time step for every layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:14
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:14
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:14
msgid "Shape as `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:15
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:15
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:15
msgid "The hidden state at the last time step for every layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:1
msgid ""
"A LSTMEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple LSTM layers. The input to this"
" module is of shape `(batch_size, num_tokens, input_size)`, The output is"
" of shape `(batch_size, hidden_size*2)` if LSTM is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:7
msgid ""
"Paddle's LSTM have two outputs: the hidden state for every time step at "
"last layer, and the hidden state and cell at the last time step for every"
" layer. If `pooling_type` is None, we perform the pooling on the hidden "
"state of every time step at last layer to create a single vector. If not "
"None, we use the hidden state of the last time step at last layer as a "
"single output (shape of `(batch_size, hidden_size)`); And if direction is"
" bidirection, the we concat the hidden state of the last forward lstm and"
" backward lstm layer to create a single vector (shape of `(batch_size, "
"hidden_size*2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:15
#: paddlenlp.seq2vec.encoder.TCNEncoder:14
msgid "The number of expected features in the input (the last dimension)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:17
msgid "The number of features in the hidden state."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:19
msgid ""
"Number of recurrent layers. E.g., setting num_layers=2 would mean "
"stacking two LSTMs together to form a stacked LSTM, with the second LSTM "
"taking in outputs of the first LSTM and computing the final results."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:23
msgid ""
"The direction of the network. It can be `forward` or `bidirect` (it means"
" bidirection network). If `biderect`, it is a birectional LSTM, and "
"returns the concat output from both directions."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:27
msgid ""
"If non-zero, introduces a Dropout layer on the outputs of each LSTM layer"
" except the last layer, with dropout probability equal to dropout."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:30
msgid ""
"If `pooling_type` is None, then the LSTMEncoder will return the hidden "
"state of the last time step at last layer as a single vector. If "
"pooling_type is not None, it must be one of `sum`, `max` and `mean`. Then"
" it will be pooled on the LSTM output (the hidden state of every time "
"step at last layer) to create a single vector."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `LSTMEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `LSTMEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.forward:1
msgid ""
"LSTMEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple LSTM layers. The input to this"
" module is of shape `(batch_size, num_tokens, input_size)`, The output is"
" of shape `(batch_size, hidden_size*2)` if LSTM is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:1
msgid ""
"A RNNEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple RNN layers. The input to this "
"module is of shape `(batch_size, num_tokens, input_size)`, The output is "
"of shape `(batch_size, hidden_size*2)` if RNN is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:7
msgid ""
"Paddle's RNN have two outputs: the hidden state for every time step at "
"last layer, and the hidden state at the last time step for every layer. "
"If `pooling_type` is None, we perform the pooling on the hidden state of "
"every time step at last layer to create a single vector. If not None, we "
"use the hidden state of the last time step at last layer as a single "
"output (shape of `(batch_size, hidden_size)`); And if direction is "
"bidirection, the we concat the hidden state of the last forward rnn and "
"backward rnn layer to create a single vector (shape of `(batch_size, "
"hidden_size*2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:17
msgid ""
"`int`, optional, defaults to 1): Number of recurrent layers. E.g., "
"setting num_layers=2 would mean stacking two RNNs together to form a "
"stacked RNN, with the second RNN taking in outputs of the first RNN and "
"computing the final results."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:20
msgid ""
"`str`, optional, defaults to obj:`forward`): The direction of the "
"network. It can be \"forward\" and \"bidirect\" (it means bidirection "
"network). If `biderect`, it is a birectional RNN, and returns the concat "
"output from both directions."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:23
msgid ""
"`float`, optional, defaults to 0.0): If non-zero, introduces a Dropout "
"layer on the outputs of each RNN layer except the last layer, with "
"dropout probability equal to dropout."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:25
msgid ""
"`str`, optional, defaults to obj:`None`): If `pooling_type` is None, then"
" the RNNEncoder will return the hidden state of the last time step at "
"last layer as a single vector. If pooling_type is not None, it must be "
"one of `sum`, `max` and `mean`. Then it will be pooled on the RNN output "
"(the hidden state of every time step at last layer) to create a single "
"vector."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `RNNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `RNNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.forward:1
msgid ""
"RNNEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple RNN layers. The input to this "
"module is of shape `(batch_size, num_tokens, input_size)`, The output is "
"of shape `(batch_size, hidden_size*2)` if RNN is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:1
msgid ""
"A `TCNEncoder` takes as input a sequence of vectors and returns a single "
"vector, which is the last one time step in the feature map. The input to "
"this module is of shape `(batch_size, num_tokens, input_size)`, and the "
"output is of shape `(batch_size, num_channels[-1])` with a receptive "
"filed:"
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:7
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward:7
msgid ""
"receptive filed = $2 * "
"\\sum_{i=0}^{len(num\\_channels)-1}2^i(kernel\\_size-1)$."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:11
msgid ""
"Temporal Convolutional Networks is a simple convolutional architecture. "
"It outperforms canonical recurrent networks such as LSTMs in many tasks. "
"See https://arxiv.org/pdf/1803.01271.pdf for more details."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:16
msgid "The number of channels in different layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:18
msgid "The kernel size. Defaults to 2."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:20
msgid "The dropout probability. Defaults to 0.2."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `TCNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `TCNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:1
msgid ""
"TCNEncoder takes as input a sequence of vectors and returns a single "
"vector, which is the last one time step in the feature map. The input to "
"this module is of shape `(batch_size, num_tokens, input_size)`, and the "
"output is of shape `(batch_size, num_channels[-1])` with a receptive "
"filed:"
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:11
msgid "The input tensor with shape `[batch_size, num_tokens, input_size]`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:14
msgid "The output tensor with shape `[batch_size, num_channels[-1]]`."
msgstr ""

