# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-16 16:04+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.seq2vec.encoder.rst:2
msgid "encoder"
msgstr ""

#~ msgid "基类：:class:`paddle.fluid.dygraph.layers.Layer`"
#~ msgstr ""

#~ msgid ""
#~ "A `BoWEncoder` takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, which simply sums the "
#~ "embeddings of a sequence across the "
#~ "time dimension. The input to this "
#~ "module is of shape `(batch_size, "
#~ "num_tokens, emb_dim)`, and the output is"
#~ " of shape `(batch_size, emb_dim)`."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "It is the input dimension to the encoder."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `BoWEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `BoWEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid "It simply sums the embeddings of a sequence across the time dimension."
#~ msgstr ""

#~ msgid "Shape as `(batch_size, num_tokens, emb_dim)`"
#~ msgstr ""

#~ msgid ""
#~ "`paddle.Tensor`, optional, defaults to "
#~ "`None`): Shape same as `inputs`. Its "
#~ "each elements identify whether is "
#~ "padding token or not. If True, not"
#~ " padding token. If False, padding "
#~ "token."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "Shape of `(batch_size, emb_dim)`. The result vector of BagOfEmbedding."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid ""
#~ "A `CNNEncoder` takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, a combination of multiple"
#~ " convolution layers and max pooling "
#~ "layers. The input to this module "
#~ "is of shape `(batch_size, num_tokens, "
#~ "emb_dim)`, and the output is of "
#~ "shape `(batch_size, ouput_dim)` or "
#~ "`(batch_size, len(ngram_filter_sizes) * "
#~ "num_filter)`."
#~ msgstr ""

#~ msgid ""
#~ "The CNN has one convolution layer "
#~ "for each ngram filter size. Each "
#~ "convolution operation gives out a vector"
#~ " of size num_filter. The number of"
#~ " times a convolution layer will be"
#~ " used is `num_tokens - ngram_size +"
#~ " 1`. The corresponding maxpooling layer "
#~ "aggregates all these outputs from the"
#~ " convolution layer and outputs the "
#~ "max."
#~ msgstr ""

#~ msgid ""
#~ "This operation is repeated for every "
#~ "ngram size passed, and consequently the"
#~ " dimensionality of the output after "
#~ "maxpooling is `len(ngram_filter_sizes) * "
#~ "num_filter`.  This then gets (optionally) "
#~ "projected down to a lower dimensional"
#~ " output, specified by `output_dim`."
#~ msgstr ""

#~ msgid ""
#~ "We then use a fully connected "
#~ "layer to project in back to the"
#~ " desired output_dim.  For more details, "
#~ "refer to \"A Sensitivity Analysis of "
#~ "(and Practitioners’ Guide to) Convolutional"
#~ " Neural Networks for Sentence "
#~ "Classification\", Zhang and Wallace 2016, "
#~ "particularly Figure 1. ref: "
#~ "https://arxiv.org/abs/1510.03820"
#~ msgstr ""

#~ msgid "This is the input dimension to the encoder."
#~ msgstr ""

#~ msgid ""
#~ "This is the output dim for each"
#~ " convolutional layer, which is the "
#~ "number of \"filters\" learned by that"
#~ " layer."
#~ msgstr ""

#~ msgid ""
#~ "This specifies both the number of "
#~ "convolutional layers we will create and"
#~ " their sizes.  The default of `(2,"
#~ " 3, 4, 5)` will have four "
#~ "convolutional layers, corresponding to "
#~ "encoding ngrams of size 2 to 5 "
#~ "with some number of filters."
#~ msgstr ""

#~ msgid "Activation to use after the convolution layers."
#~ msgstr ""

#~ msgid ""
#~ "After doing convolutions and pooling, "
#~ "we'll project the collected features "
#~ "into a vector of this size.  If"
#~ " this value is `None`, we will "
#~ "just return the result of the max"
#~ " pooling, giving an output of shape"
#~ " `len(ngram_filter_sizes) * num_filter`."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `CNNEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `CNNEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid "The combination of multiple convolution layers and max pooling layers."
#~ msgstr ""

#~ msgid ""
#~ "If output_dim is None, the result "
#~ "shape     is of `(batch_size, output_dim)`;"
#~ " if not, the result shape     is "
#~ "of `(batch_size, len(ngram_filter_sizes) * "
#~ "num_filter)`."
#~ msgstr ""

#~ msgid "If output_dim is None, the result shape"
#~ msgstr ""

#~ msgid ""
#~ "is of `(batch_size, output_dim)`; if "
#~ "not, the result shape is of "
#~ "`(batch_size, len(ngram_filter_sizes) * "
#~ "num_filter)`."
#~ msgstr ""

#~ msgid ""
#~ "A GRUEncoder takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, which is a combination"
#~ " of multiple GRU layers. The input"
#~ " to this module is of shape "
#~ "`(batch_size, num_tokens, input_size)`, The "
#~ "output is of shape `(batch_size, "
#~ "hidden_size*2)` if GRU is bidirection; "
#~ "If not, output is of shape "
#~ "`(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid ""
#~ "Paddle's GRU have two outputs: the "
#~ "hidden state for every time step "
#~ "at last layer, and the hidden "
#~ "state at the last time step for"
#~ " every layer. If `pooling_type` is "
#~ "None, we perform the pooling on "
#~ "the hidden state of every time "
#~ "step at last layer to create a "
#~ "single vector. If not None, we use"
#~ " the hidden state of the last "
#~ "time step at last layer as a "
#~ "single output (shape of `(batch_size, "
#~ "hidden_size)`); And if direction is "
#~ "bidirection, the we concat the hidden"
#~ " state of the last forward gru "
#~ "and backward gru layer to create a"
#~ " single vector (shape of `(batch_size, "
#~ "hidden_size*2)`)."
#~ msgstr ""

#~ msgid ""
#~ "`int`, required): The number of expected"
#~ " features in the input (the last "
#~ "dimension)."
#~ msgstr ""

#~ msgid "`int`, required): The number of features in the hidden state."
#~ msgstr ""

#~ msgid ""
#~ "`int`, optional, defaults to 1): Number"
#~ " of recurrent layers. E.g., setting "
#~ "num_layers=2 would mean stacking two "
#~ "GRUs together to form a stacked "
#~ "GRU, with the second GRU taking in"
#~ " outputs of the first GRU and "
#~ "computing the final results."
#~ msgstr ""

#~ msgid ""
#~ "`str`, optional, defaults to obj:`forward`):"
#~ " The direction of the network. It "
#~ "can be `forward` and `bidirect` (it "
#~ "means bidirection network). If `biderect`, "
#~ "it is a birectional GRU, and "
#~ "returns the concat output from both "
#~ "directions."
#~ msgstr ""

#~ msgid ""
#~ "`float`, optional, defaults to 0.0): If"
#~ " non-zero, introduces a Dropout layer"
#~ " on the outputs of each GRU "
#~ "layer except the last layer, with "
#~ "dropout probability equal to dropout."
#~ msgstr ""

#~ msgid ""
#~ "`str`, optional, defaults to obj:`None`): "
#~ "If `pooling_type` is None, then the "
#~ "GRUEncoder will return the hidden state"
#~ " of the last time step at last"
#~ " layer as a single vector. If "
#~ "pooling_type is not None, it must "
#~ "be one of `sum`, `max` and `mean`."
#~ " Then it will be pooled on the"
#~ " GRU output (the hidden state of "
#~ "every time step at last layer) to"
#~ " create a single vector."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `GRUEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `GRUEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid ""
#~ "GRUEncoder takes the a sequence of "
#~ "vectors and and returns a single "
#~ "vector, which is a combination of "
#~ "multiple GRU layers. The input to "
#~ "this module is of shape `(batch_size,"
#~ " num_tokens, input_size)`, The output is"
#~ " of shape `(batch_size, hidden_size*2)` if"
#~ " GRU is bidirection; If not, output"
#~ " is of shape `(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid "Shape as `(batch_size, num_tokens, input_size)`."
#~ msgstr ""

#~ msgid "Shape as `(batch_size)`."
#~ msgstr ""

#~ msgid ""
#~ "Shape as `(batch_size, hidden_size)`.     The"
#~ " hidden state at the last time "
#~ "step for every layer."
#~ msgstr ""

#~ msgid "Shape as `(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid "The hidden state at the last time step for every layer."
#~ msgstr ""

#~ msgid ""
#~ "A LSTMEncoder takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, which is a combination"
#~ " of multiple LSTM layers. The input"
#~ " to this module is of shape "
#~ "`(batch_size, num_tokens, input_size)`, The "
#~ "output is of shape `(batch_size, "
#~ "hidden_size*2)` if LSTM is bidirection; "
#~ "If not, output is of shape "
#~ "`(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid ""
#~ "Paddle's LSTM have two outputs: the "
#~ "hidden state for every time step "
#~ "at last layer, and the hidden "
#~ "state and cell at the last time"
#~ " step for every layer. If "
#~ "`pooling_type` is None, we perform the"
#~ " pooling on the hidden state of "
#~ "every time step at last layer to"
#~ " create a single vector. If not "
#~ "None, we use the hidden state of"
#~ " the last time step at last "
#~ "layer as a single output (shape of"
#~ " `(batch_size, hidden_size)`); And if "
#~ "direction is bidirection, the we concat"
#~ " the hidden state of the last "
#~ "forward lstm and backward lstm layer "
#~ "to create a single vector (shape "
#~ "of `(batch_size, hidden_size*2)`)."
#~ msgstr ""

#~ msgid "The number of expected features in the input (the last dimension)."
#~ msgstr ""

#~ msgid "The number of features in the hidden state."
#~ msgstr ""

#~ msgid ""
#~ "Number of recurrent layers. E.g., "
#~ "setting num_layers=2 would mean stacking "
#~ "two LSTMs together to form a "
#~ "stacked LSTM, with the second LSTM "
#~ "taking in outputs of the first "
#~ "LSTM and computing the final results."
#~ msgstr ""

#~ msgid ""
#~ "The direction of the network. It "
#~ "can be `forward` or `bidirect` (it "
#~ "means bidirection network). If `biderect`, "
#~ "it is a birectional LSTM, and "
#~ "returns the concat output from both "
#~ "directions."
#~ msgstr ""

#~ msgid ""
#~ "If non-zero, introduces a Dropout "
#~ "layer on the outputs of each LSTM"
#~ " layer except the last layer, with"
#~ " dropout probability equal to dropout."
#~ msgstr ""

#~ msgid ""
#~ "If `pooling_type` is None, then the "
#~ "LSTMEncoder will return the hidden state"
#~ " of the last time step at last"
#~ " layer as a single vector. If "
#~ "pooling_type is not None, it must "
#~ "be one of `sum`, `max` and `mean`."
#~ " Then it will be pooled on the"
#~ " LSTM output (the hidden state of "
#~ "every time step at last layer) to"
#~ " create a single vector."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `LSTMEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `LSTMEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid ""
#~ "LSTMEncoder takes the a sequence of "
#~ "vectors and and returns a single "
#~ "vector, which is a combination of "
#~ "multiple LSTM layers. The input to "
#~ "this module is of shape `(batch_size,"
#~ " num_tokens, input_size)`, The output is"
#~ " of shape `(batch_size, hidden_size*2)` if"
#~ " LSTM is bidirection; If not, output"
#~ " is of shape `(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid ""
#~ "A RNNEncoder takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, which is a combination"
#~ " of multiple RNN layers. The input"
#~ " to this module is of shape "
#~ "`(batch_size, num_tokens, input_size)`, The "
#~ "output is of shape `(batch_size, "
#~ "hidden_size*2)` if RNN is bidirection; "
#~ "If not, output is of shape "
#~ "`(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid ""
#~ "Paddle's RNN have two outputs: the "
#~ "hidden state for every time step "
#~ "at last layer, and the hidden "
#~ "state at the last time step for"
#~ " every layer. If `pooling_type` is "
#~ "None, we perform the pooling on "
#~ "the hidden state of every time "
#~ "step at last layer to create a "
#~ "single vector. If not None, we use"
#~ " the hidden state of the last "
#~ "time step at last layer as a "
#~ "single output (shape of `(batch_size, "
#~ "hidden_size)`); And if direction is "
#~ "bidirection, the we concat the hidden"
#~ " state of the last forward rnn "
#~ "and backward rnn layer to create a"
#~ " single vector (shape of `(batch_size, "
#~ "hidden_size*2)`)."
#~ msgstr ""

#~ msgid ""
#~ "`int`, optional, defaults to 1): Number"
#~ " of recurrent layers. E.g., setting "
#~ "num_layers=2 would mean stacking two "
#~ "RNNs together to form a stacked "
#~ "RNN, with the second RNN taking in"
#~ " outputs of the first RNN and "
#~ "computing the final results."
#~ msgstr ""

#~ msgid ""
#~ "`str`, optional, defaults to obj:`forward`):"
#~ " The direction of the network. It "
#~ "can be \"forward\" and \"bidirect\" (it"
#~ " means bidirection network). If `biderect`,"
#~ " it is a birectional RNN, and "
#~ "returns the concat output from both "
#~ "directions."
#~ msgstr ""

#~ msgid ""
#~ "`float`, optional, defaults to 0.0): If"
#~ " non-zero, introduces a Dropout layer"
#~ " on the outputs of each RNN "
#~ "layer except the last layer, with "
#~ "dropout probability equal to dropout."
#~ msgstr ""

#~ msgid ""
#~ "`str`, optional, defaults to obj:`None`): "
#~ "If `pooling_type` is None, then the "
#~ "RNNEncoder will return the hidden state"
#~ " of the last time step at last"
#~ " layer as a single vector. If "
#~ "pooling_type is not None, it must "
#~ "be one of `sum`, `max` and `mean`."
#~ " Then it will be pooled on the"
#~ " RNN output (the hidden state of "
#~ "every time step at last layer) to"
#~ " create a single vector."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `RNNEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `RNNEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid ""
#~ "RNNEncoder takes the a sequence of "
#~ "vectors and and returns a single "
#~ "vector, which is a combination of "
#~ "multiple RNN layers. The input to "
#~ "this module is of shape `(batch_size,"
#~ " num_tokens, input_size)`, The output is"
#~ " of shape `(batch_size, hidden_size*2)` if"
#~ " RNN is bidirection; If not, output"
#~ " is of shape `(batch_size, hidden_size)`."
#~ msgstr ""

#~ msgid ""
#~ "A `TCNEncoder` takes as input a "
#~ "sequence of vectors and returns a "
#~ "single vector, which is the last "
#~ "one time step in the feature map."
#~ " The input to this module is of"
#~ " shape `(batch_size, num_tokens, input_size)`,"
#~ " and the output is of shape "
#~ "`(batch_size, num_channels[-1])` with a "
#~ "receptive filed:"
#~ msgstr ""

#~ msgid ""
#~ "receptive filed = $2 * "
#~ "\\sum_{i=0}^{len(num\\_channels)-1}2^i(kernel\\_size-1)$."
#~ msgstr ""

#~ msgid ""
#~ "Temporal Convolutional Networks is a "
#~ "simple convolutional architecture. It "
#~ "outperforms canonical recurrent networks such"
#~ " as LSTMs in many tasks. See "
#~ "https://arxiv.org/pdf/1803.01271.pdf for more "
#~ "details."
#~ msgstr ""

#~ msgid "The number of channels in different layer."
#~ msgstr ""

#~ msgid "The kernel size. Defaults to 2."
#~ msgstr ""

#~ msgid "The dropout probability. Defaults to 0.2."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the vector "
#~ "input for each element in the "
#~ "sequence input to a `TCNEncoder`. This"
#~ " is not the shape of the input"
#~ " tensor, but the last element of "
#~ "that shape."
#~ msgstr ""

#~ msgid ""
#~ "Returns the dimension of the final "
#~ "vector output by this `TCNEncoder`.  "
#~ "This is not the shape of the "
#~ "returned tensor, but the last element"
#~ " of that shape."
#~ msgstr ""

#~ msgid ""
#~ "TCNEncoder takes as input a sequence "
#~ "of vectors and returns a single "
#~ "vector, which is the last one time"
#~ " step in the feature map. The "
#~ "input to this module is of shape"
#~ " `(batch_size, num_tokens, input_size)`, and "
#~ "the output is of shape `(batch_size, "
#~ "num_channels[-1])` with a receptive filed:"
#~ msgstr ""

#~ msgid "The input tensor with shape `[batch_size, num_tokens, input_size]`."
#~ msgstr ""

#~ msgid "The output tensor with shape `[batch_size, num_channels[-1]]`."
#~ msgstr ""

