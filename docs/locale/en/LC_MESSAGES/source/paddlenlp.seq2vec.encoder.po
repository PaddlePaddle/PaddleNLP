# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.seq2vec.encoder.rst:2
msgid "encoder"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:1
#: paddlenlp.seq2vec.encoder.CNNEncoder:1
#: paddlenlp.seq2vec.encoder.GRUEncoder:1
#: paddlenlp.seq2vec.encoder.LSTMEncoder:1
#: paddlenlp.seq2vec.encoder.RNNEncoder:1
#: paddlenlp.seq2vec.encoder.TCNEncoder:1
msgid "基类：:class:`paddle.fluid.dygraph.layers.Layer`"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:1
msgid ""
"A `BoWEncoder` takes as input a sequence of vectors and returns a single "
"vector, which simply sums the embeddings of a sequence across the time "
"dimension. The input to this encoder is of shape `(batch_size, "
"num_tokens, emb_dim)`, and the output is of shape `(batch_size, "
"emb_dim)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder
#: paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "参数"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:6
#: paddlenlp.seq2vec.encoder.CNNEncoder:20
msgid "The dimension of each vector in the input sequence."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder:10
#: paddlenlp.seq2vec.encoder.CNNEncoder:39
#: paddlenlp.seq2vec.encoder.GRUEncoder:44
#: paddlenlp.seq2vec.encoder.LSTMEncoder:43
#: paddlenlp.seq2vec.encoder.RNNEncoder:43
msgid "示例"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `BoWEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `BoWEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:1
msgid "It simply sums the embeddings of a sequence across the time dimension."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:3
msgid ""
"Shape as `(batch_size, num_tokens, emb_dim)` and dtype as `float32` or "
"`float64`. The sequence length of the input sequence."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:6
msgid ""
"Shape same as `inputs`. Its each elements identify whether the "
"corresponding input token is padding or not. If True, not padding token. "
"If False, padding token. Defaults to `None`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "返回"
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward:12
msgid ""
"Returns tensor `summed`, the result vector of BagOfEmbedding. Its data "
"type is same as `inputs` and its shape is `[batch_size, emb_dim]`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.BoWEncoder.forward
#: paddlenlp.seq2vec.encoder.CNNEncoder.forward
#: paddlenlp.seq2vec.encoder.GRUEncoder.forward
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward
msgid "返回类型"
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:1
msgid ""
"A `CNNEncoder` takes as input a sequence of vectors and returns a single "
"vector, a combination of multiple convolution layers and max pooling "
"layers. The input to this encoder is of shape `(batch_size, num_tokens, "
"emb_dim)`, and the output is of shape `(batch_size, ouput_dim)` or "
"`(batch_size, len(ngram_filter_sizes) * num_filter)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:6
msgid ""
"The CNN has one convolution layer for each ngram filter size. Each "
"convolution operation gives out a vector of size num_filter. The number "
"of times a convolution layer will be used is `num_tokens - ngram_size + "
"1`. The corresponding maxpooling layer aggregates all these outputs from "
"the convolution layer and outputs the max."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:11
msgid ""
"This operation is repeated for every ngram size passed, and consequently "
"the dimensionality of the output after maxpooling is "
"`len(ngram_filter_sizes) * num_filter`.  This then gets (optionally) "
"projected down to a lower dimensional output, specified by `output_dim`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:15
msgid ""
"We then use a fully connected layer to project in back to the desired "
"output_dim.  For more details, refer to `A Sensitivity Analysis of (and "
"Practitioners’ Guide to) Convolutional Neural Networks for Sentence "
"Classification <https://arxiv.org/abs/1510.03820>`__ , Zhang and Wallace "
"2016, particularly Figure 1."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:22
msgid ""
"This is the output dim for each convolutional layer, which is the number "
"of \"filters\" learned by that layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:25
msgid ""
"This specifies both the number of convolutional layers we will create and"
" their sizes.  The default of `(2, 3, 4, 5)` will have four convolutional"
" layers, corresponding to encoding ngrams of size 2 to 5 with some number"
" of filters."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:29
msgid ""
"Activation to use after the convolution layers. Defaults to "
"`paddle.nn.Tanh()`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder:32
msgid ""
"After doing convolutions and pooling, we'll project the collected "
"features into a vector of this size.  If this value is `None`, we will "
"just return the result of the max pooling, giving an output of shape "
"`len(ngram_filter_sizes) * num_filter`. Defaults to `None`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `CNNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `CNNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:1
msgid "The combination of multiple convolution layers and max pooling layers."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:3
msgid ""
"Shape as `(batch_size, num_tokens, emb_dim)` and dtype as `float32` or "
"`float64`. Tensor containing the features of the input sequence."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:6
msgid ""
"Shape shoule be same as `inputs` and dtype as `int32`, `int64`, `float32`"
" or `float64`. Its each elements identify whether the corresponding input"
" token is padding or not. If True, not padding token. If False, padding "
"token. Defaults to `None`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.CNNEncoder.forward:12
msgid ""
"Returns tensor `result`. If output_dim is None, the result shape is of "
"`(batch_size, output_dim)` and dtype is `float`; If not, the result shape"
" is of `(batch_size, len(ngram_filter_sizes) * num_filter)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:1
msgid ""
"A GRUEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple `paddle.nn.GRU "
"<https://www.paddlepaddle.org.cn/documentation/docs/en/api "
"/paddle/nn/layer/rnn/GRU_en.html>`__ subclass. The input to this encoder "
"is of shape `(batch_size, num_tokens, input_size)`, The output is of "
"shape `(batch_size, hidden_size * 2)` if GRU is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:9
msgid ""
"Paddle's GRU have two outputs: the hidden state for every time step at "
"last layer, and the hidden state at the last time step for every layer. "
"If `pooling_type` is not None, we perform the pooling on the hidden state"
" of every time step at last layer to create a single vector. If None, we "
"use the hidden state of the last time step at last layer as a single "
"output (shape of `(batch_size, hidden_size)`); And if direction is "
"bidirection, the we concat the hidden state of the last forward gru and "
"backward gru layer to create a single vector (shape of `(batch_size, "
"hidden_size * 2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:17
#: paddlenlp.seq2vec.encoder.LSTMEncoder:17
#: paddlenlp.seq2vec.encoder.RNNEncoder:17
#: paddlenlp.seq2vec.encoder.TCNEncoder:14
msgid "The number of expected features in the input (the last dimension)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:19
#: paddlenlp.seq2vec.encoder.LSTMEncoder:19
#: paddlenlp.seq2vec.encoder.RNNEncoder:19
msgid "The number of features in the hidden state."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:21
msgid ""
"Number of recurrent layers. E.g., setting num_layers=2 would mean "
"stacking two GRUs together to form a stacked GRU, with the second GRU "
"taking in outputs of the first GRU and computing the final results. "
"Defaults to 1."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:26
msgid ""
"The direction of the network. It can be \"forward\" and \"bidirect\" (it "
"means bidirection network). If \"bidirect\", it is a birectional GRU, and"
" returns the concat output from both directions. Defaults to \"forward\"."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:31
msgid ""
"If non-zero, introduces a Dropout layer on the outputs of each GRU layer "
"except the last layer, with dropout probability equal to dropout. "
"Defaults to 0.0."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder:35
msgid ""
"If `pooling_type` is None, then the GRUEncoder will return the hidden "
"state of the last time step at last layer as a single vector. If "
"pooling_type is not None, it must be one of \"sum\", \"max\" and "
"\"mean\". Then it will be pooled on the GRU output (the hidden state of "
"every time step at last layer) to create a single vector. Defaults to "
"`None`"
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `GRUEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `GRUEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:1
msgid ""
"GRUEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple GRU layers. The input to this "
"encoder is of shape `(batch_size, num_tokens, input_size)`, The output is"
" of shape `(batch_size, hidden_size * 2)` if GRU is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:7
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:7
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:7
msgid ""
"Shape as `(batch_size, num_tokens, input_size)`. Tensor containing the "
"features of the input sequence."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:10
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:10
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:10
msgid "Shape as `(batch_size)`. The sequence length of the input sequence."
msgstr ""

#: of paddlenlp.seq2vec.encoder.GRUEncoder.forward:14
#: paddlenlp.seq2vec.encoder.LSTMEncoder.forward:14
#: paddlenlp.seq2vec.encoder.RNNEncoder.forward:14
msgid ""
"Returns tensor `output`, the hidden state at the last time step for every"
" layer. Its data type is `float` and its shape is `[batch_size, "
"hidden_size]`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:1
msgid ""
"An LSTMEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple `paddle.nn.LSTM "
"<https://www.paddlepaddle.org.cn/documentation/docs/en/api "
"/paddle/nn/layer/rnn/LSTM_en.html>`__ subclass. The input to this encoder"
" is of shape `(batch_size, num_tokens, input_size)`. The output is of "
"shape `(batch_size, hidden_size * 2)` if LSTM is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:9
msgid ""
"Paddle's LSTM have two outputs: the hidden state for every time step at "
"last layer, and the hidden state and cell at the last time step for every"
" layer. If `pooling_type` is not None, we perform the pooling on the "
"hidden state of every time step at last layer to create a single vector. "
"If None, we use the hidden state of the last time step at last layer as a"
" single output (shape of `(batch_size, hidden_size)`); And if direction "
"is bidirection, the we concat the hidden state of the last forward lstm "
"and backward lstm layer to create a single vector (shape of `(batch_size,"
" hidden_size * 2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:21
msgid ""
"Number of recurrent layers. E.g., setting num_layers=2 would mean "
"stacking two LSTMs together to form a stacked LSTM, with the second LSTM "
"taking in outputs of the first LSTM and computing the final results. "
"Defaults to 1."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:26
msgid ""
"The direction of the network. It can be \"forward\" or \"bidirect\" (it "
"means bidirection network). If \"bidirect\", it is a birectional LSTM, "
"and returns the concat output from both directions. Defaults to "
"\"forward\"."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:30
msgid ""
"If non-zero, introduces a Dropout layer on the outputs of each LSTM layer"
" except the last layer, with dropout probability equal to dropout. "
"Defaults to 0.0 ."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder:34
msgid ""
"If `pooling_type` is None, then the LSTMEncoder will return the hidden "
"state of the last time step at last layer as a single vector. If "
"pooling_type is not None, it must be one of \"sum\", \"max\" and "
"\"mean\". Then it will be pooled on the LSTM output (the hidden state of "
"every time step at last layer) to create a single vector. Defaults to "
"`None`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `LSTMEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `LSTMEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.LSTMEncoder.forward:1
msgid ""
"LSTMEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple LSTM layers. The input to this"
" encoder is of shape `(batch_size, num_tokens, input_size)`, The output "
"is of shape `(batch_size, hidden_size * 2)` if LSTM is bidirection; If "
"not, output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:1
msgid ""
"A RNNEncoder takes as input a sequence of vectors and returns a single "
"vector, which is a combination of multiple `paddle.nn.RNN "
"<https://www.paddlepaddle.org.cn/documentation/docs/en/api "
"/paddle/nn/layer/rnn/RNN_en.html>`__ subclass. The input to this encoder "
"is of shape `(batch_size, num_tokens, input_size)`, The output is of "
"shape `(batch_size, hidden_size * 2)` if RNN is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:9
msgid ""
"Paddle's RNN have two outputs: the hidden state for every time step at "
"last layer, and the hidden state at the last time step for every layer. "
"If `pooling_type` is not None, we perform the pooling on the hidden state"
" of every time step at last layer to create a single vector. If None, we "
"use the hidden state of the last time step at last layer as a single "
"output (shape of `(batch_size, hidden_size)`); And if direction is "
"bidirection, the we concat the hidden state of the last forward rnn and "
"backward rnn layer to create a single vector (shape of `(batch_size, "
"hidden_size * 2)`)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:21
msgid ""
"Number of recurrent layers. E.g., setting num_layers=2 would mean "
"stacking two RNNs together to form a stacked RNN, with the second RNN "
"taking in outputs of the first RNN and computing the final results. "
"Defaults to 1."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:26
msgid ""
"The direction of the network. It can be \"forward\" and \"bidirect\" (it "
"means bidirection network). If \"biderect\", it is a birectional RNN, and"
" returns the concat output from both directions. Defaults to \"forward\""
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:30
msgid ""
"If non-zero, introduces a Dropout layer on the outputs of each RNN layer "
"except the last layer, with dropout probability equal to dropout. "
"Defaults to 0.0."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder:34
msgid ""
"If `pooling_type` is None, then the RNNEncoder will return the hidden "
"state of the last time step at last layer as a single vector. If "
"pooling_type is not None, it must be one of \"sum\", \"max\" and "
"\"mean\". Then it will be pooled on the RNN output (the hidden state of "
"every time step at last layer) to create a single vector. Defaults to "
"`None`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `RNNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `RNNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.RNNEncoder.forward:1
msgid ""
"RNNEncoder takes the a sequence of vectors and and returns a single "
"vector, which is a combination of multiple RNN layers. The input to this "
"encoder is of shape `(batch_size, num_tokens, input_size)`. The output is"
" of shape `(batch_size, hidden_size * 2)` if RNN is bidirection; If not, "
"output is of shape `(batch_size, hidden_size)`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:1
msgid ""
"A `TCNEncoder` takes as input a sequence of vectors and returns a single "
"vector, which is the last one time step in the feature map. The input to "
"this encoder is of shape `(batch_size, num_tokens, input_size)`, and the "
"output is of shape `(batch_size, num_channels[-1])` with a receptive "
"filed:"
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:7
#: paddlenlp.seq2vec.encoder.TCNEncoder.forward:7
msgid ""
"receptive filed = 2 * "
"\\sum_{i=0}^{len(num\\_channels)-1}2^i(kernel\\_size-1)."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:11
msgid ""
"Temporal Convolutional Networks is a simple convolutional architecture. "
"It outperforms canonical recurrent networks such as LSTMs in many tasks. "
"See https://arxiv.org/pdf/1803.01271.pdf for more details."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:16
msgid "The number of channels in different layer."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:18
msgid "The kernel size. Defaults to 2."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder:20
msgid "The dropout probability. Defaults to 0.2."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.get_input_dim:1
msgid ""
"Returns the dimension of the vector input for each element in the "
"sequence input to a `TCNEncoder`. This is not the shape of the input "
"tensor, but the last element of that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.get_output_dim:1
msgid ""
"Returns the dimension of the final vector output by this `TCNEncoder`.  "
"This is not the shape of the returned tensor, but the last element of "
"that shape."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:1
msgid ""
"TCNEncoder takes as input a sequence of vectors and returns a single "
"vector, which is the last one time step in the feature map. The input to "
"this encoder is of shape `(batch_size, num_tokens, input_size)`, and the "
"output is of shape `(batch_size, num_channels[-1])` with a receptive "
"filed:"
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:11
msgid "The input tensor with shape `[batch_size, num_tokens, input_size]`."
msgstr ""

#: of paddlenlp.seq2vec.encoder.TCNEncoder.forward:14
msgid "Returns tensor `output` with shape `[batch_size, num_channels[-1]]`."
msgstr ""

