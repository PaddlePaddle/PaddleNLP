# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-09-24 16:20+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.ops.optimizer.adamw.rst:2
msgid "adamw"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:1
msgid "基类：:class:`paddle.optimizer.optimizer.Optimizer`"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:1
msgid ""
"The AdamW optimizer is implemented based on the AdamW Optimization in "
"paper `DECOUPLED WEIGHT DECAY REGULARIZATION "
"<https://arxiv.org/pdf/1711.05101.pdf>`_. it can resolves the problem of "
"L2 regularization failure in the Adam optimizer. .. math::"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW
msgid "参数"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:12
msgid ""
"The learning rate used to update ``Parameter``. It can be a float value "
"or a LRScheduler. The default value is 0.001."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:15
msgid ""
"The exponential decay rate for the 1st moment estimates. It should be a "
"float number or a Tensor with shape [1] and data type as float32. The "
"default value is 0.9."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:19
msgid ""
"The exponential decay rate for the 2nd moment estimates. It should be a "
"float number or a Tensor with shape [1] and data type as float32. The "
"default value is 0.999."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:23
msgid ""
"A small float value for numerical stability. It should be a float number "
"or a Tensor with shape [1] and data type as float32. The default value is"
" 1e-08."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:27
msgid ""
"List/Tuple of ``Tensor`` to update to minimize ``loss``. \\ This "
"parameter is required in dygraph mode. \\ The default value is None in "
"static mode, at this time all parameters will be updated."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:31
msgid ""
"The weight decay coefficient, it can be float or Tensor. The default "
"value is 0.01."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:33
msgid ""
"If it is not None, only tensors that makes "
"apply_decay_param_fun(Tensor.name)==True will be updated. It only works "
"when we want to specify tensors. Default: None."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:38
msgid ""
"Gradient cliping strategy, it's an instance of some derived class of "
"``GradientClipBase`` . There are three cliping strategies ( "
":ref:`api_fluid_clip_GradientClipByGlobalNorm` , "
":ref:`api_fluid_clip_GradientClipByNorm` , "
":ref:`api_fluid_clip_GradientClipByValue` ). Default None, meaning there "
"is no gradient clipping."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:43
msgid ""
"The official Adam algorithm has two moving-average accumulators. The "
"accumulators are updated at every step. Every element of the two moving-"
"average is updated in both dense mode and sparse mode. If the size of "
"parameter is very large, then the update may be very slow. The lazy mode "
"only update the element that has gradient in current mini-batch, so it "
"will be much more faster. But this mode has different semantics with the "
"original Adam algorithm and may lead to different result. The default "
"value is False."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:51
msgid "Whether to use multi-precision during weight updating. Default is false."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:53
msgid ""
"Normally there is no need for user to set this property. For more "
"information, please refer to :ref:`api_guide_Name`. The default value is "
"None."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW:59
#: paddlenlp.ops.optimizer.adamw.AdamW.step:6
msgid "实际案例"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW.step:1
msgid "Execute the optimizer and update parameters once."
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW.step
msgid "返回"
msgstr ""

#: of paddlenlp.ops.optimizer.adamw.AdamW.step:3
msgid "None"
msgstr ""

