# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.gpt.tokenizer.rst:2
msgid "tokenizer"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:1
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:1
msgid "基类：:class:`paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer`"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:1
msgid "Constructs a GPT tokenizer based on byte-level Byte-Pair-Encoding."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:3
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:3
msgid ""
"This tokenizer inherits from "
":class:`~paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer` "
"which contains most of the main methods. For more information regarding "
"those methods, please refer to this superclass."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.save_resources
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.save_resources
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:7
msgid ""
"Path to the vocab file. The vocab file contains a mapping from vocabulary"
" strings to indices."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:10
msgid ""
"Path to the merge file. The merge file is used to split the input "
"sentence into \"subword\" units. The vocab file is then used to encode "
"those units as intices."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:14
msgid "Paradigm to follow when decoding bytes to UTF-8. Defaults to `'replace'`."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:17
msgid "The maximum value of the input sequence length. Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:19
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer:22
msgid "实际案例"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.vocab_size:1
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.vocab_size:1
msgid "Returns the size of vocabulary."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.vocab_size
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.vocab_size
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.vocab_size:3
msgid "The sum of size of vocabulary and the size of speical tokens."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.vocab_size
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.vocab_size
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string:1
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string:1
msgid "Converts a single index or a sequence of indices to texts."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string:3
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string:3
msgid "The token id (or token ids) to be converted to text."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string:6
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string:6
msgid "The decoded text."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_string:10
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens:11
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.vocab_size:7
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.convert_ids_to_string:10
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.save_resources:1
msgid ""
"Saves `SentencePiece <https://github.com/google/sentencepiece>`__ file "
"(ends with '.spm') under `save_directory`."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.save_resources:3
#: paddlenlp.transformers.gpt.tokenizer.GPTTokenizer.save_resources:4
msgid "Directory to save files into."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:1
msgid ""
"Constructs a GPT Chinese tokenizer based on `SentencePiece "
"<https://github.com/google/sentencepiece>`__."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:7
msgid ""
"The vocabulary file required to instantiate a `SentencePiece "
"<https://github.com/google/sentencepiece>`__ tokenizer."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:10
msgid "The maximum value of the input sequence length. Defaults to `512`."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer:13
msgid ""
"A special token representing the *unknown (out-of-vocabulary)* token. An "
"unknown token is set to be `unk_token` inorder to be converted to an ID. "
"Defaults to \"[UNK]\"."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens:1
msgid ""
"Converts a single index or a sequence of indices to a token or a sequence"
" of tokens."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens:4
msgid "The token id (or token ids) to be converted to token(s)."
msgstr ""

#: of
#: paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.convert_ids_to_tokens:7
msgid "The converted token or sequence of tokens."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.vocab_size:3
msgid "The size of vocabulary."
msgstr ""

#: of paddlenlp.transformers.gpt.tokenizer.GPTChineseTokenizer.save_resources:1
msgid "Save tokenizer related resources to files under `save_directory`."
msgstr ""

