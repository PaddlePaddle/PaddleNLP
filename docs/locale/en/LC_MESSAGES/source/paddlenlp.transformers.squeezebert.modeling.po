# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.squeezebert.modeling.rst:2
msgid "modeling"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering:1
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification:1
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification:1
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:1
msgid "基类：:class:`paddlenlp.transformers.squeezebert.modeling.SqueezeBertPreTrainedModel`"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:1
msgid ""
"Vocabulary size of `inputs_ids` in `SqueezeBertModel`. Also is the vocab "
"size of token embedding matrix. Defines the number of different tokens "
"that can be represented by the `inputs_ids` passed when calling "
"`BertModel`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:4
msgid ""
"Dimensionality of the embedding layer, encoder layer and pooler layer. "
"Defaults to `768`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:6
msgid "Number of hidden layers in the Transformer encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:8
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to `12`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:11
msgid "Output chans for intermediate layer."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:13
msgid ""
"The non-linear activation function in the feed-forward layer. "
"``\"gelu\"``, ``\"relu\"`` and any other paddle supported activation "
"functions are supported. Defaults to `\"gelu\"`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:17
msgid ""
"The dropout probability for all fully connected layers in the embeddings "
"and encoder. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:20
msgid ""
"The dropout probability used in MultiHeadAttention in all encoder layers "
"to drop some attention target. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:23
msgid ""
"The maximum value of the dimensionality of position encoding, which "
"dictates the maximum supported length of an input sequence. Defaults to "
"`512`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:26
msgid "The vocabulary size of `token_type_ids`. Defaults to `16`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:29
msgid ""
"number of query groups for all layers in the BertModule. (eventually we "
"could change the interface to allow different groups for different "
"layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:32
msgid ""
"number of key groups for all layers in the BertModule. (eventually we "
"could change the interface to allow different groups for different "
"layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:35
msgid ""
"number of value groups for all layers in the BertModule. (eventually we "
"could change the interface to allow different groups for different "
"layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:38
msgid ""
"number of output groups for all layers in the BertModule. (eventually we "
"could change the interface to allow different groups for different "
"layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:41
msgid ""
"number of intermediate groups for all layers in the BertModule. "
"(eventually we could change the interface to allow different groups for "
"different layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:44
msgid ""
"number of post groups for all layers in the BertModule. (eventually we "
"could change the interface to allow different groups for different "
"layers)"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:47
msgid ""
"The standard deviation of the normal initializer. Defaults to 0.02. .. "
"note::     A normal_initializer initializes weight matrices as normal "
"distributions.     See :meth:`BertPretrainedModel.init_weights()` for how"
" weights are initialized in `BertModel`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel:47
msgid ""
"The standard deviation of the normal initializer. Defaults to 0.02. .. "
"note::"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:1
msgid ""
"The  forward method, overrides the `__call__()` special method. :param "
"input_ids: Indices of input sequence tokens in the vocabulary. They are"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:3
msgid ""
"numerical representations of tokens that build the input sequence. Its "
"data type should be `int64` and it has a shape of [batch_size, "
"sequence_length]."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:6
msgid ""
"Mask used in multi-head attention to avoid performing attention on to "
"some unwanted positions, usually the paddings or the subsequent "
"positions. Its data type can be int, float and bool. If its data type is "
"int, the values should be either 0 or 1. - **1** for tokens that **not "
"masked**, - **0** for tokens that **masked**. It is a tensor with shape "
"broadcasted to `[batch_size, num_attention_heads, sequence_length, "
"sequence_length]`. Defaults to `None`, which means nothing needed to be "
"prevented attention to."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:15
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range ``[0, type_vocab_size - 1]``. If `type_vocab_size` "
"is 2, which means the inputs have two portions. Indices can either be 0 "
"or 1: - 0 corresponds to a *sentence A* token, - 1 corresponds to a "
"*sentence B* token. Its data type should be `int64` and it has a shape of"
" [batch_size, sequence_length]. Defaults to `None`, which means we don't "
"add segment embeddings."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:24
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range ``[0, max_position_embeddings - 1]``. "
"Shape as `(batch_size, num_tokens)` and dtype as int64. Defaults to "
"`None`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:28
msgid ""
"Whether to return the attention_weight of each hidden layers. Defaults to"
" `False`."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:31
msgid "Whether to return the output of each hidden layers. Defaults to `False`."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:35
msgid ""
"Returns tuple (`sequence_output`, `pooled_output`) with "
"(`encoder_outputs`, `encoder_attentions`) by optional. With the fields: -"
" `sequence_output` (Tensor):     Sequence of hidden-states at the last "
"layer of the model.     It's data type should be float32 and its shape is"
" [batch_size, sequence_length, hidden_size]. - `pooled_output` (Tensor):"
"     The output of first token (`[CLS]`) in sequence.     We \"pool\" the"
" model by simply taking the hidden state corresponding to the first "
"token.     Its data type should be float32 and its shape is [batch_size, "
"hidden_size]. - `encoder_outputs` (List(Tensor)):     A list of Tensor "
"containing hidden-states of the model at each hidden layer in the "
"Transformer encoder.     The length of the list is `num_hidden_layers` + "
"1 (Embedding Layer output).     Each Tensor has a data type of float32 "
"and its shape is [batch_size, sequence_length, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:35
msgid ""
"Returns tuple (`sequence_output`, `pooled_output`) with "
"(`encoder_outputs`, `encoder_attentions`) by optional. With the fields: -"
" `sequence_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:39
msgid ""
"Sequence of hidden-states at the last layer of the model. It's data type "
"should be float32 and its shape is [batch_size, sequence_length, "
"hidden_size]."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:43
msgid "`pooled_output` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:42
msgid ""
"The output of first token (`[CLS]`) in sequence. We \"pool\" the model by"
" simply taking the hidden state corresponding to the first token. Its "
"data type should be float32 and its shape is [batch_size, hidden_size]."
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:47
msgid "`encoder_outputs` (List(Tensor)):"
msgstr ""

#: of paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward:46
msgid ""
"A list of Tensor containing hidden-states of the model at each hidden "
"layer in the Transformer encoder. The length of the list is "
"`num_hidden_layers` + 1 (Embedding Layer output). Each Tensor has a data "
"type of float32 and its shape is [batch_size, sequence_length, "
"hidden_size]."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification.forward
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification:1
msgid ""
"SqueezeBert Model with a sequence classification/regression head on top "
"(a linear layer on top of the pooled output) e.g. for GLUE tasks. :param "
"squeezebert: An instance of SqueezeBert. :type squeezebert: "
":class:`SqueezeBertModel` :param num_classes: The number of classes. "
"Defaults to `2`. :type num_classes: int, optional :param dropout: The "
"dropout probability for output of SqueezeBertModel."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification:8
msgid ""
"If None, use the same value as `hidden_dropout_prob` of "
"`SqueezeBertModel` instance `squeezebert`. Defaults to None."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification.forward:1
msgid ""
"The SqueezeBertForSequenceClassification forward method, overrides the "
"__call__() special method. :param input_ids: See "
":class:`SqueezeBertModel`. :type input_ids: Tensor :param token_type_ids:"
" See :class:`SqueezeBertModel`. :type token_type_ids: Tensor, optional "
":param position_ids: See :class:`SqueezeBertModel`. :type position_ids: "
"Tensor, optional :param attention_mask: See :class:`SqueezeBertModel`. "
":type attention_mask: list, optional"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForSequenceClassification.forward:11
msgid ""
"Returns tensor `logits`, a tensor of the input text classification "
"logits. Shape as `[batch_size, num_classes]` and dtype as float32."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification:1
msgid ""
"SqueezeBert Model with a token classification head on top (a linear layer"
" on top of the hidden-states output) e.g. for Named-Entity-Recognition "
"(NER) tasks. :param squeezebert: An instance of SqueezeBertModel. :type "
"squeezebert: :class:`SqueezeBertModel` :param num_classes: The number of "
"classes. Defaults to `2`. :type num_classes: int, optional :param "
"dropout: The dropout probability for output of squeezebert."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification:8
msgid ""
"If None, use the same value as `hidden_dropout_prob` of `SqueezeBert` "
"instance `squeezebert`. Defaults to None."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification.forward:1
msgid ""
"The SqueezeBertForTokenClassification forward method, overrides the "
"__call__() special method. :param input_ids: See "
":class:`SqueezeBertModel`. :type input_ids: Tensor :param token_type_ids:"
" See :class:`SqueezeBertModel`. :type token_type_ids: Tensor, optional "
":param position_ids: See :class:`SqueezeBertModel`. :type position_ids: "
"Tensor, optional :param attention_mask: See :class:`SqueezeBertModel`. "
":type attention_mask: list, optional"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForTokenClassification.forward:11
msgid ""
"Returns tensor `logits`, a tensor of the input token classification "
"logits. Shape as `[batch_size, sequence_length, num_classes]` and dtype "
"as `float32`."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering:1
msgid ""
"SqueezeBert Model with a span classification head on top for extractive "
"question-answering tasks like SQuAD (a linear layers on top of the "
"hidden-states output to compute `span start logits` and `span end "
"logits`). :param squeezebert: An instance of SqueezeBertModel. :type "
"squeezebert: :class:`SqueezeBertModel` :param dropout: The dropout "
"probability for output of SqueezeBert."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering:7
msgid ""
"If None, use the same value as `hidden_dropout_prob` of "
"`SqueezeBertModel` instance `squeezebert`. Defaults to `None`."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:1
msgid ""
"The SqueezeBertForQuestionAnswering forward method, overrides the "
"__call__() special method. :param input_ids: See "
":class:`SqueezeBertModel`. :type input_ids: Tensor :param token_type_ids:"
" See :class:`SqueezeBertModel`. :type token_type_ids: Tensor, optional"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:7
msgid ""
"Returns tuple (`start_logits`, `end_logits`). With the fields: - "
"`start_logits` (Tensor):     A tensor of the input token classification "
"logits, indicates the start position of the labelled span.     Its data "
"type should be float32 and its shape is [batch_size, sequence_length]. - "
"`end_logits` (Tensor):     A tensor of the input token classification "
"logits, indicates the end position of the labelled span.     Its data "
"type should be float32 and its shape is [batch_size, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:7
msgid ""
"Returns tuple (`start_logits`, `end_logits`). With the fields: - "
"`start_logits` (Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:10
msgid ""
"A tensor of the input token classification logits, indicates the start "
"position of the labelled span. Its data type should be float32 and its "
"shape is [batch_size, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:13
msgid "`end_logits` (Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.squeezebert.modeling.SqueezeBertForQuestionAnswering.forward:13
msgid ""
"A tensor of the input token classification logits, indicates the end "
"position of the labelled span. Its data type should be float32 and its "
"shape is [batch_size, sequence_length]."
msgstr ""

