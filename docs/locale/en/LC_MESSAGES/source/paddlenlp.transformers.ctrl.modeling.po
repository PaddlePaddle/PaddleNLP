# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.ctrl.modeling.rst:2
msgid "modeling"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification:1
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel:1
#: paddlenlp.transformers.ctrl.modeling.CTRLModel:1
msgid "基类：:class:`paddlenlp.transformers.ctrl.modeling.CTRLPreTrainedModel`"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:1
msgid ""
"The bare CTRL Model transformer outputting raw hidden-states without any "
"specific head on top."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:3
msgid ""
"This model inherits from "
":class:`~paddlenlp.transformers.model_utils.PretrainedModel`. Refer to "
"the superclass documentation for the generic methods."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:6
msgid ""
"This model is also a Paddle `paddle.nn.Layer "
"<https://www.paddlepaddle.org.cn/documentation "
"/docs/en/api/paddle/fluid/dygraph/layers/Layer_en.html>`__ subclass. Use "
"it as a regular Paddle Layer and refer to the Paddle documentation for "
"all matter related to general usage and behavior."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLModel
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward
#: paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding.forward
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:10
msgid ""
"Vocabulary size of `inputs_ids` in `CTRLModel`. Also is the vocab size of"
" token embedding matrix. Defines the number of different tokens that can "
"be represented by the `inputs_ids` passed when calling `CTRLModel`. "
"Defaults to `246534`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:14
msgid ""
"The maximum sequence length that this model might ever be used with. "
"Typically set this to something large just in case (e.g., 512 or 1024 or "
"2048 or 50000). Defaults to `50000`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:17
msgid "Dimensionality of the embeddings and hidden states. Defaults to `1280`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:20
msgid ""
"Dimensionality of the inner dimension of the feed forward networks (FFN)."
" Defaults to `8192`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:23
msgid "Number of hidden layers in the Transformer encoder. Defaults to `48`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:26
msgid ""
"Number of attention heads for each attention layer in the Transformer "
"encoder. Defaults to `16`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:29
msgid ""
"The dropout ratio for all fully connected layers in the encoder. Defaults"
" to `0.1`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:32
msgid "The dropout ratio for the embeddings. Defaults to `0.1`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:35
msgid "The epsilon to use in the layer normalization layers. Defaults to `1e-6`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:38
msgid ""
"Whether the model's input and output word embeddings should be tied. Note"
" that this is only relevant if the model has a output word embedding "
"layer. Defaults to `True`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:41
msgid "The id of the `padding` token. Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:44
msgid ""
"The standard deviation of the normal initializer. Defaults to 0.02.  .. "
"note::     A normal_initializer initializes weight matrices as normal "
"distributions.     See :meth:`CTRLPreTrainedModel._init_weights()` for "
"how weights are initialized in `CTRLModel`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:44
msgid "The standard deviation of the normal initializer. Defaults to 0.02."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel:48
msgid ""
"A normal_initializer initializes weight matrices as normal distributions."
" See :meth:`CTRLPreTrainedModel._init_weights()` for how weights are "
"initialized in `CTRLModel`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:1
msgid "The CTRLModel forward method, overrides the `__call__()` special method."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:3
msgid ""
"Indices of input sequence tokens in the vocabulary. They are numerical "
"representations of tokens that build the input sequence. Its data type "
"should be `int64` and it has a shape of [batch_size, sequence_length]."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:7
msgid ""
"Contains pre-computed hidden-states (key and values in the attention "
"blocks) as computed by the model. Can be used to speed up sequential "
"decoding. The `input_ids` which have their past given to this model "
"should not be passed as input ids as they have already been computed. "
"Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:13
msgid ""
"Mask used in multi-head attention to avoid performing attention on to "
"some unwanted positions, usually the paddings or the subsequent "
"positions. Its data type can be int, float and bool. When the data type "
"is bool, the `masked` tokens have `False` values and the others have "
"`True` values. When the data type is int, the `masked` tokens have `0` "
"values and the others have `1` values. When the data type is float, the "
"`masked` tokens have `0.0` values and the others have `1.0` values. It is"
" a tensor with shape broadcasted to `[batch_size, num_attention_heads, "
"sequence_length, sequence_length]`. Defaults to `None`, which means "
"nothing needed to be prevented attention to."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:23
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range `[0, type_vocab_size - 1]`. If `type_vocab_size` is"
" 2, which means the inputs have two portions. Indices can either be 0 or "
"1:  - 0 corresponds to a *sentence A* token, - 1 corresponds to a "
"*sentence B* token.  Its data type should be `int64` and it has a shape "
"of [batch_size, sequence_length]. Defaults to `None`, which means we "
"don't add segment embeddings."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:23
msgid ""
"Segment token indices to indicate different portions of the inputs. "
"Selected in the range `[0, type_vocab_size - 1]`. If `type_vocab_size` is"
" 2, which means the inputs have two portions. Indices can either be 0 or "
"1:"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:28
msgid "0 corresponds to a *sentence A* token,"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:29
msgid "1 corresponds to a *sentence B* token."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:31
msgid ""
"Its data type should be `int64` and it has a shape of [batch_size, "
"sequence_length]. Defaults to `None`, which means we don't add segment "
"embeddings."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:34
msgid ""
"Indices of positions of each input sequence tokens in the position "
"embeddings. Selected in the range `[0, max_position_embeddings - 1]`. "
"Shape as [batch_size, num_tokens] and dtype as int64. Defaults to `None`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:38
msgid ""
"Whether or not to use cache. Defaults to `False`. If set to `True`, key "
"value states will be returned and can be used to speed up decoding."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:41
msgid ""
"Whether or not to return the attentions tensors of all attention layers. "
"Defaults to `False`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:44
msgid ""
"Whether or not to return the output of all hidden layers. Defaults to "
"`False`."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:48
msgid ""
"Returns tuple (`last_hidden_state`, `caches`, `hidden_states`, "
"`attentions`)  With the fields:  - `last_hidden_state` (Tensor):     "
"Sequence of hidden-states at the last layer of the model.     It's data "
"type should be float32 and its shape is [batch_size, sequence_length, "
"hidden_size].  - `caches` (tuple(tuple(Tensor), optional):     returned "
"when `use_cache=True` is passed.     Tuple of `tuple(Tensor)` of length "
"`num_hidden_layers`, with each tuple having 2     tensors of shape "
"[batch_size, num_heads, sequence_length, embed_size_per_head] and float32"
" dtype.  - `hidden_states` (tuple(Tensor), optional):     returned when "
"`output_hidden_states=True` is passed.     Tuple of `Tensor` (one for the"
" output of the embeddings + one for the output of     each layer). Each "
"Tensor has a data type of float32 and its shape is     [batch_size, "
"sequence_length, hidden_size].  - `attentions` (tuple(Tensor), optional):"
"     returned when `output_attentions=True` is passed.     Tuple of "
"`Tensor` (one for each layer) of shape. Each Tensor has a data type of"
"     float32 and its shape is [batch_size, num_heads, sequence_length, "
"sequence_length]."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:48
msgid ""
"Returns tuple (`last_hidden_state`, `caches`, `hidden_states`, "
"`attentions`)"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:50
msgid "With the fields:"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:54
msgid "`last_hidden_state` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:53
msgid ""
"Sequence of hidden-states at the last layer of the model. It's data type "
"should be float32 and its shape is [batch_size, sequence_length, "
"hidden_size]."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:38
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:39
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:59
msgid "`caches` (tuple(tuple(Tensor), optional):"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:57
msgid ""
"returned when `use_cache=True` is passed. Tuple of `tuple(Tensor)` of "
"length `num_hidden_layers`, with each tuple having 2 tensors of shape "
"[batch_size, num_heads, sequence_length, embed_size_per_head] and float32"
" dtype."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:41
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:42
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:65
msgid "`hidden_states` (tuple(Tensor), optional):"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:62
msgid ""
"returned when `output_hidden_states=True` is passed. Tuple of `Tensor` "
"(one for the output of the embeddings + one for the output of each "
"layer). Each Tensor has a data type of float32 and its shape is "
"[batch_size, sequence_length, hidden_size]."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:43
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:44
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:69
msgid "`attentions` (tuple(Tensor), optional):"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:68
msgid ""
"returned when `output_attentions=True` is passed. Tuple of `Tensor` (one "
"for each layer) of shape. Each Tensor has a data type of float32 and its "
"shape is [batch_size, num_heads, sequence_length, sequence_length]."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:48
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:49
#: paddlenlp.transformers.ctrl.modeling.CTRLModel.forward:74
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel:1
msgid ""
"The CTRL Model transformer with a language modeling head on top (linear "
"layer with weights tied to the input embeddings)."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification:8
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel:4
msgid "An instance of :class:`CTRLModel`."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:1
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:3
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:5
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:7
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:9
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:17
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:19
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:21
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:38
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:41
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:44
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:1
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:3
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:5
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:7
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:9
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:17
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:19
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:21
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:39
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:42
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:45
msgid "See :class:`CTRLModel`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:11
msgid ""
"Labels for language modeling. Note that the labels **are shifted** inside"
" the model, i.e. you can set `labels = input_ids` Indices are selected in"
" `[-100, 0, ..., vocab_size]` All labels set to `-100` are ignored "
"(masked), the loss is only computed for labels in `[0, ..., vocab_size]`."
" Shape is [batch_size, sequence_length] and dtype is int64."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:24
msgid ""
"Returns tuple `(loss, logits, caches, hidden_states, attentions)`. With "
"the fields:  - `loss` (Tensor):     returned when `labels` is provided."
"     Language modeling loss (for next-token prediction).     It's data "
"type should be float32 and its shape is [1,].  - `logits` (Tensor):     "
"Prediction scores of the language modeling head (scores for each "
"vocabulary     token before SoftMax).     It's data type should be "
"float32 and     its shape is [batch_size, sequence_length, vocab_size].  "
"- `caches` (tuple(tuple(Tensor), optional):     See :class:`CTRLModel`.  "
"- `hidden_states` (tuple(Tensor), optional):     See :class:`CTRLModel`."
"  - `attentions` (tuple(Tensor), optional):     See :class:`CTRLModel`."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:24
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:24
msgid ""
"Returns tuple `(loss, logits, caches, hidden_states, attentions)`. With "
"the fields:"
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:30
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:30
msgid "`loss` (Tensor):"
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:28
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:28
msgid ""
"returned when `labels` is provided. Language modeling loss (for next-"
"token prediction). It's data type should be float32 and its shape is "
"[1,]."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:35
#: paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:36
msgid "`logits` (Tensor):"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLLMHeadModel.forward:33
msgid ""
"Prediction scores of the language modeling head (scores for each "
"vocabulary token before SoftMax). It's data type should be float32 and "
"its shape is [batch_size, sequence_length, vocab_size]."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification:1
msgid ""
"The CTRL Model transformer with a sequence classification head on top "
"(linear layer). `CTRLForSequenceClassification` uses the last token in "
"order to do the classification, as other causal models (e.g. GPT-2) do. "
"Since it does classification on the last token, it requires to know the "
"position of the last token. If a `pad_token_id` is defined in the "
"configuration, it finds the last token that is not a padding token in "
"each row. If no `pad_token_id` is defined, it simply takes the last value"
" in each row of the batch."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification:10
msgid "The number of classes. Defaults to `2`."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification:12
msgid ""
"The dropout probability for output of CTRL. If None, use the same value "
"as `hidden_dropout_prob` of `CTRLModel` instance `ctrl`. Defaults to "
"None."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:11
msgid ""
"Labels for computing the sequence classification/regression loss. Indices"
" should be in `[0, ...,num_classes - 1]`. If `num_classes == 1` a "
"regression loss is computed (Mean-Square loss), If `num_classes > 1` a "
"classification loss is computed (Cross-Entropy). Shape is [batch_size,] "
"and dtype is int64."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:24
msgid ""
"Returns tuple `(loss, logits, caches, hidden_states, attentions)`. With "
"the fields:  - `loss` (Tensor):     returned when `labels` is provided."
"     Language modeling loss (for next-token prediction).     It's data "
"type should be float32 and its shape is [1,].  - `logits` (Tensor):     "
"Prediction scores of the language modeling head (scores for each "
"vocabulary     token before SoftMax).     It's data type should be "
"float32 and its shape is [batch_size, num_classes].  - `caches` "
"(tuple(tuple(Tensor), optional):     See :class:`CTRLModel`.  - "
"`hidden_states` (tuple(Tensor), optional):     See :class:`CTRLModel`.  -"
" `attentions` (tuple(Tensor), optional):     See :class:`CTRLModel`."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.CTRLForSequenceClassification.forward:33
msgid ""
"Prediction scores of the language modeling head (scores for each "
"vocabulary token before SoftMax). It's data type should be float32 and "
"its shape is [batch_size, num_classes]."
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding:1
msgid "基类：:class:`paddle.nn.layer.common.Embedding`"
msgstr ""

#: of paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding:1
msgid "This module produces sinusoidal positional embeddings of any length."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding.forward:1
msgid ""
"Defines the computation performed at every call. Should be overridden by "
"all subclasses."
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding.forward:4
msgid "unpacked tuple arguments"
msgstr ""

#: of
#: paddlenlp.transformers.ctrl.modeling.SinusoidalPositionalEmbedding.forward:6
msgid "unpacked dict arguments"
msgstr ""

