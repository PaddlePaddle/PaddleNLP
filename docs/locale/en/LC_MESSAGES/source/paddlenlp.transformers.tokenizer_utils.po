# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-16 16:04+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.tokenizer_utils.rst:2
msgid "tokenizer\\_utils"
msgstr ""

#~ msgid "基类：:class:`object`"
#~ msgstr ""

#~ msgid ""
#~ "The base class for all pretrained "
#~ "tokenizers. It provides some attributes "
#~ "and common methods for all pretrained"
#~ " tokenizers, including attributes for and"
#~ " special tokens (arguments of `__init__`"
#~ " whose name ends with `_token`) and"
#~ " methods for saving and loading. It"
#~ " also includes some class attributes "
#~ "(should be set by derived classes): "
#~ "- `tokenizer_config_file` (str): represents "
#~ "the file name for saving and "
#~ "loading"
#~ msgstr ""

#~ msgid "tokenizer configuration, it's value is `tokenizer_config.json`."
#~ msgstr ""

#~ msgid ""
#~ "`resource_files_names` (dict): use this to "
#~ "map resource related arguments of "
#~ "`__init__` to specific file names for"
#~ " saving and loading."
#~ msgstr ""

#~ msgid ""
#~ "`pretrained_resource_files_map` (dict): The dict "
#~ "has the same keys as "
#~ "`resource_files_names`, the values are also"
#~ " dict mapping specific pretrained model "
#~ "name to URL linking to vocabulary "
#~ "or other resources."
#~ msgstr ""

#~ msgid ""
#~ "`pretrained_init_configuration` (dict): The dict "
#~ "has pretrained model names as keys, "
#~ "and the values are also dict "
#~ "preserving corresponding configuration for "
#~ "tokenizer initialization."
#~ msgstr ""

#~ msgid ""
#~ "List all the special tokens ('<unk>',"
#~ " '<cls>'...) mapped to class attributes "
#~ "(cls_token, unk_token...)."
#~ msgstr ""

#~ msgid ""
#~ "List the vocabulary indices of the "
#~ "special tokens ('<unk>', '<cls>'...) mapped"
#~ " to class attributes (cls_token, "
#~ "unk_token...)."
#~ msgstr ""

#~ msgid ""
#~ "Converts a sequence of tokens into "
#~ "ids using the vocab. The tokenizer "
#~ "should has the `vocab` attribute. Args："
#~ msgstr ""

#~ msgid "tokens (list(str)): List of tokens."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "Converted id list."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid ""
#~ "Converts a sequence of tokens (list "
#~ "of string) to a single string by"
#~ " using :code:`' '.join(tokens)` . :param"
#~ " tokens: List of tokens. :type "
#~ "tokens: list(str)"
#~ msgstr ""

#~ msgid "Converted string."
#~ msgstr ""

#~ msgid ""
#~ "Converts a single index or a "
#~ "sequence of indices (integers) in a "
#~ "token or a sequence of tokens "
#~ "(str) by using the vocabulary."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "Don't decode special tokens (self.all_special_tokens). Default: False"
#~ msgstr ""

#~ msgid ""
#~ "Instantiate an instance of "
#~ "`PretrainedTokenizer` from a predefined "
#~ "tokenizer specified by name or path.,"
#~ " and it always corresponds to a "
#~ "pretrained model. :param "
#~ "pretrained_model_name_or_path: A name of or"
#~ " a file path to a"
#~ msgstr ""

#~ msgid "pretrained model."
#~ msgstr ""

#~ msgid ""
#~ "position arguments for `__init__`. If "
#~ "provide, use this as position argument"
#~ " values for tokenizer initialization."
#~ msgstr ""

#~ msgid ""
#~ "keyword arguments for `__init__`. If "
#~ "provide, use this to update pre-"
#~ "defined keyword argument values for "
#~ "tokenizer initialization."
#~ msgstr ""

#~ msgid "An instance of PretrainedTokenizer."
#~ msgstr ""

#~ msgid ""
#~ "Save tokenizer configuration and related "
#~ "resources to files under `save_directory`. "
#~ ":param save_directory: Directory to save "
#~ "files into. :type save_directory: str"
#~ msgstr ""

#~ msgid ""
#~ "Save tokenizer related resources to "
#~ "files under `save_directory`. :param "
#~ "save_directory: Directory to save files "
#~ "into. :type save_directory: str"
#~ msgstr ""

#~ msgid ""
#~ "Instantiate an instance of `Vocab` from"
#~ " a file reserving all tokens by "
#~ "using `Vocab.from_dict`. The file contains "
#~ "a token per line, and the line "
#~ "number would be the index of "
#~ "corresponding token. :param filepath: path "
#~ "of file to construct vocabulary. :type"
#~ " filepath: str :param unk_token: special"
#~ " token for unknown token. If no "
#~ "need, it also"
#~ msgstr ""

#~ msgid "could be None. Default: None."
#~ msgstr ""

#~ msgid ""
#~ "special token for padding token. If "
#~ "no need, it also could be None."
#~ " Default: None."
#~ msgstr ""

#~ msgid ""
#~ "special token for bos token. If no"
#~ " need, it also could be None. "
#~ "Default: None."
#~ msgstr ""

#~ msgid ""
#~ "special token for eos token. If no"
#~ " need, it also could be None. "
#~ "Default: None."
#~ msgstr ""

#~ msgid "keyword arguments for `Vocab.from_dict`."
#~ msgstr ""

#~ msgid "An instance of `Vocab`."
#~ msgstr ""

#~ msgid ""
#~ "Save all tokens to a vocabulary "
#~ "file. The file contains a token "
#~ "per line, and the line number "
#~ "would be the index of corresponding "
#~ "token. Agrs:"
#~ msgstr ""

#~ msgid ""
#~ "filepath (str): File path to be "
#~ "saved to. vocab (Vocab|dict): the Vocab"
#~ " or dict instance to be saved."
#~ msgstr ""

#~ msgid "Truncates a sequence pair in place to the maximum length."
#~ msgstr ""

#~ msgid ""
#~ "list of tokenized input ids. Can "
#~ "be obtained from a string by "
#~ "chaining the `tokenize` and "
#~ "`convert_tokens_to_ids` methods."
#~ msgstr ""

#~ msgid ""
#~ "Optional second list of input ids. "
#~ "Can be obtained from a string by"
#~ " chaining the `tokenize` and "
#~ "`convert_tokens_to_ids` methods."
#~ msgstr ""

#~ msgid "number of tokens to remove using the truncation strategy"
#~ msgstr ""

#~ msgid ""
#~ "string selected in the following "
#~ "options: - 'longest_first' (default) "
#~ "Iteratively reduce the inputs sequence "
#~ "until the input is under max_seq_len"
#~ "     starting from the longest one at"
#~ " each token (when there is a "
#~ "pair of input sequences).     Overflowing "
#~ "tokens only contains overflow from the"
#~ " first sequence. - 'only_first': Only "
#~ "truncate the first sequence. raise an"
#~ " error if the first sequence is "
#~ "shorter or equal to than "
#~ "num_tokens_to_remove. - 'only_second': Only "
#~ "truncate the second sequence - "
#~ "'do_not_truncate': Does not truncate (raise"
#~ " an error if the input sequence "
#~ "is longer than max_seq_len)"
#~ msgstr ""

#~ msgid ""
#~ "string selected in the following "
#~ "options: - 'longest_first' (default) "
#~ "Iteratively reduce the inputs sequence "
#~ "until the input is under max_seq_len"
#~ msgstr ""

#~ msgid ""
#~ "starting from the longest one at "
#~ "each token (when there is a pair"
#~ " of input sequences). Overflowing tokens"
#~ " only contains overflow from the "
#~ "first sequence."
#~ msgstr ""

#~ msgid ""
#~ "'only_first': Only truncate the first "
#~ "sequence. raise an error if the "
#~ "first sequence is shorter or equal "
#~ "to than num_tokens_to_remove."
#~ msgstr ""

#~ msgid "'only_second': Only truncate the second sequence"
#~ msgstr ""

#~ msgid ""
#~ "'do_not_truncate': Does not truncate (raise"
#~ " an error if the input sequence "
#~ "is longer than max_seq_len)"
#~ msgstr ""

#~ msgid ""
#~ "If set to a number along with "
#~ "max_seq_len, the overflowing tokens returned"
#~ " will contain some tokens from the"
#~ " main sequence returned. The value of"
#~ " this argument defines the number of"
#~ " additional tokens."
#~ msgstr ""

#~ msgid ""
#~ "Build model inputs from a sequence "
#~ "or a pair of sequence for sequence"
#~ " classification tasks by concatenating and"
#~ " adding special tokens."
#~ msgstr ""

#~ msgid ""
#~ "Should be overridden in a subclass "
#~ "if the model has a special way "
#~ "of building those."
#~ msgstr ""

#~ msgid "List of IDs to which the special tokens will be added."
#~ msgstr ""

#~ msgid "Optional second list of IDs for sequence pairs."
#~ msgstr ""

#~ msgid "List of input_id with the appropriate special tokens."
#~ msgstr ""

#~ msgid ":obj:`List[int]`"
#~ msgstr ""

#~ msgid ""
#~ "Build offset map from a pair of"
#~ " offset map by concatenating and "
#~ "adding offsets of special tokens."
#~ msgstr ""

#~ msgid "List of char offsets to which the special tokens will be added."
#~ msgstr ""

#~ msgid "Optional second list of char offsets for offset mapping pairs."
#~ msgstr ""

#~ msgid "List of char offsets with the appropriate offsets of special tokens."
#~ msgstr ""

#~ msgid ":obj:`List[tuple]`"
#~ msgstr ""

#~ msgid ""
#~ "Retrieves sequence ids from a token "
#~ "list that has no special tokens "
#~ "added. This method is called when "
#~ "adding special tokens using the "
#~ "tokenizer ``encode`` methods."
#~ msgstr ""

#~ msgid "List of ids of the first sequence."
#~ msgstr ""

#~ msgid "List of ids of the second sequence."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not the token list is"
#~ " already formatted with special tokens "
#~ "for the model. Defaults to None."
#~ msgstr ""

#~ msgid ""
#~ "The list of integers in the range"
#~ " [0, 1]: 1 for a special token,"
#~ " 0 for a sequence token."
#~ msgstr ""

#~ msgid ""
#~ "Create a mask from the two "
#~ "sequences passed to be used in a"
#~ " sequence-pair classification task."
#~ msgstr ""

#~ msgid ""
#~ "If :obj:`token_ids_1` is :obj:`None`, this "
#~ "method only returns the first portion"
#~ " of the mask (0s)."
#~ msgstr ""

#~ msgid "List of IDs."
#~ msgstr ""

#~ msgid "List of token_type_id according to the given sequence(s)."
#~ msgstr ""

#~ msgid ""
#~ "Returns a dictionary containing the "
#~ "encoded sequence or sequence pair and"
#~ " additional information: the mask for "
#~ "sequence classification and the overflowing"
#~ " elements if a ``max_seq_len`` is "
#~ "specified."
#~ msgstr ""

#~ msgid ""
#~ "The first sequence to be encoded. "
#~ "This can be a string, a list "
#~ "of strings (tokenized string using the"
#~ " `tokenize` method) or a list of "
#~ "integers (tokenized string ids using the"
#~ " `convert_tokens_to_ids` method)"
#~ msgstr ""

#~ msgid ""
#~ "Optional second sequence to be encoded."
#~ " This can be a string, a list"
#~ " of strings (tokenized string using "
#~ "the `tokenize` method) or a list "
#~ "of integers (tokenized string ids using"
#~ " the `convert_tokens_to_ids` method)"
#~ msgstr ""

#~ msgid ""
#~ "If set to a number, will limit "
#~ "the total sequence returned so that "
#~ "it has a maximum length. If there"
#~ " are overflowing tokens, those will "
#~ "be added to the returned dictionary"
#~ msgstr ""

#~ msgid ""
#~ "If set to True, the returned "
#~ "sequences will be padded according to"
#~ " the model's padding side and padding"
#~ " index, up to their max length. "
#~ "If no max length is specified, the"
#~ " padding is done up to the "
#~ "model's max length."
#~ msgstr ""

#~ msgid ""
#~ "String selected in the following "
#~ "options:  - 'longest_first' (default) "
#~ "Iteratively reduce the inputs sequence "
#~ "until the input is under max_seq_len"
#~ "   starting from the longest one at"
#~ " each token (when there is a "
#~ "pair of input sequences) - 'only_first':"
#~ " Only truncate the first sequence -"
#~ " 'only_second': Only truncate the second"
#~ " sequence - 'do_not_truncate': Does not "
#~ "truncate (raise an error if the "
#~ "input sequence is longer than "
#~ "max_seq_len)"
#~ msgstr ""

#~ msgid "String selected in the following options:"
#~ msgstr ""

#~ msgid ""
#~ "'longest_first' (default) Iteratively reduce "
#~ "the inputs sequence until the input "
#~ "is under max_seq_len starting from the"
#~ " longest one at each token (when "
#~ "there is a pair of input "
#~ "sequences)"
#~ msgstr ""

#~ msgid "'only_first': Only truncate the first sequence"
#~ msgstr ""

#~ msgid "Set to True to return tokens position ids (default True)."
#~ msgstr ""

#~ msgid "Whether to return token type IDs."
#~ msgstr ""

#~ msgid "Whether to return the attention mask."
#~ msgstr ""

#~ msgid ""
#~ "If set the resulting dictionary will "
#~ "include the length of each encoded "
#~ "inputs"
#~ msgstr ""

#~ msgid "Set to True to return overflowing token information (default False)."
#~ msgstr ""

#~ msgid "Set to True to return special tokens mask information (default False)."
#~ msgstr ""

#~ msgid ""
#~ "A Dictionary of shape::      {         "
#~ "input_ids: list[int],         position_ids: "
#~ "list[int] if return_position_ids is True"
#~ "         token_type_ids: list[int] if "
#~ "return_token_type_ids is True (default)         "
#~ "attention_mask: list[int] if return_attention_mask"
#~ " is True         seq_len: int if "
#~ "return_length is True         overflowing_tokens:"
#~ " list[int] if a ``max_seq_len`` is "
#~ "specified and return_overflowing_tokens is "
#~ "True         num_truncated_tokens: int if a"
#~ " ``max_seq_len`` is specified and "
#~ "return_overflowing_tokens is True         "
#~ "special_tokens_mask: list[int] if "
#~ "return_special_tokens_mask is True     }  With"
#~ " the fields:  - ``input_ids``: list "
#~ "of token ids to be fed to a"
#~ " model - ``position_ids``: list of "
#~ "token position ids to be fed to"
#~ " a model - ``token_type_ids``: list "
#~ "of token type ids to be fed "
#~ "to a model - ``attention_mask``: list"
#~ " of indices specifying which tokens "
#~ "should be attended to by the model"
#~ " - ``length``: the input_ids length -"
#~ " ``overflowing_tokens``: list of overflowing "
#~ "tokens if a max length is "
#~ "specified. - ``num_truncated_tokens``: number "
#~ "of overflowing tokens a ``max_seq_len`` "
#~ "is specified - ``special_tokens_mask``: list"
#~ " of [0, 1], with 0 specifying "
#~ "special added   tokens and 1 specifying"
#~ " sequence tokens."
#~ msgstr ""

#~ msgid "A Dictionary of shape::"
#~ msgstr ""

#~ msgid "With the fields:"
#~ msgstr ""

#~ msgid "``input_ids``: list of token ids to be fed to a model"
#~ msgstr ""

#~ msgid "``position_ids``: list of token position ids to be fed to a model"
#~ msgstr ""

#~ msgid "``token_type_ids``: list of token type ids to be fed to a model"
#~ msgstr ""

#~ msgid ""
#~ "``attention_mask``: list of indices specifying"
#~ " which tokens should be attended to"
#~ " by the model"
#~ msgstr ""

#~ msgid "``length``: the input_ids length"
#~ msgstr ""

#~ msgid ""
#~ "``overflowing_tokens``: list of overflowing "
#~ "tokens if a max length is "
#~ "specified."
#~ msgstr ""

#~ msgid ""
#~ "``num_truncated_tokens``: number of overflowing "
#~ "tokens a ``max_seq_len`` is specified"
#~ msgstr ""

#~ msgid ""
#~ "``special_tokens_mask``: list of [0, 1], "
#~ "with 0 specifying special added tokens"
#~ " and 1 specifying sequence tokens."
#~ msgstr ""

#~ msgid ""
#~ "Returns a list of dictionary containing"
#~ " the encoded sequence or sequence "
#~ "pair and additional information: the "
#~ "mask for sequence classification and the"
#~ " overflowing elements if a ``max_seq_len``"
#~ " is specified."
#~ msgstr ""

#~ msgid ""
#~ "Batch of sequences or pair of "
#~ "sequences to be encoded. This can "
#~ "be a list of string/string-sequences"
#~ "/int-sequences or a list of pair "
#~ "of string/string-sequences/int-sequence"
#~ msgstr ""

#~ msgid ""
#~ "If set to a positive number and"
#~ " batch_text_or_text_pairs is a list of "
#~ "pair sequences, the overflowing tokens "
#~ "which contain some tokens from the "
#~ "end of the truncated second sequence "
#~ "will be concatenated with the first "
#~ "sequence to generate new features. And"
#~ " The overflowing tokens would not be"
#~ " returned in dictionary. The value of"
#~ " this argument defines the number of"
#~ " overlapping tokens."
#~ msgstr ""

#~ msgid "Whether or not the text has been pretokenized."
#~ msgstr ""

#~ msgid ""
#~ "A List of dictionary of shape::      "
#~ "{         input_ids: list[int],         "
#~ "position_ids: list[int] if return_position_ids "
#~ "is True         token_type_ids: list[int] if"
#~ " return_token_type_ids is True (default)"
#~ "         attention_mask: list[int] if "
#~ "return_attention_mask is True         seq_len: "
#~ "int if return_length is True         "
#~ "overflowing_tokens: list[int] if a "
#~ "``max_seq_len`` is specified and "
#~ "return_overflowing_tokens is True and stride"
#~ " is 0         num_truncated_tokens: int if"
#~ " a ``max_seq_len`` is specified and "
#~ "return_overflowing_tokens is True and stride"
#~ " is 0         special_tokens_mask: list[int] "
#~ "if return_special_tokens_mask is True         "
#~ "offset_mapping: list[Tuple] if stride is "
#~ "a positive number and batch_text_or_text_pairs"
#~ " is a list of pair sequences"
#~ "         overflow_to_sample: int if stride "
#~ "is a positive number and "
#~ "batch_text_or_text_pairs is a list of "
#~ "pair sequences     }  With the fields:"
#~ "  - ``input_ids``: list of token ids"
#~ " to be fed to a model - "
#~ "``position_ids``: list of token position "
#~ "ids to be fed to a model -"
#~ " ``token_type_ids``: list of token type "
#~ "ids to be fed to a model -"
#~ " ``attention_mask``: list of indices "
#~ "specifying which tokens should be "
#~ "attended to by the model - "
#~ "``length``: the input_ids length - "
#~ "``overflowing_tokens``: list of overflowing "
#~ "tokens if a max length is "
#~ "specified. - ``num_truncated_tokens``: number "
#~ "of overflowing tokens a ``max_seq_len`` "
#~ "is specified - ``special_tokens_mask``: if "
#~ "adding special tokens, this is a "
#~ "list of [0, 1], with 0 specifying"
#~ " special added   tokens and 1 "
#~ "specifying sequence tokens. - "
#~ "``offset_mapping``: list of (index of "
#~ "start char in text,index of end "
#~ "char in text) of token. (0,0) if"
#~ " token is a sqecial token - "
#~ "``overflow_to_sample``: index of example from"
#~ " which this feature is generated"
#~ msgstr ""

#~ msgid "A List of dictionary of shape::"
#~ msgstr ""

#~ msgid ""
#~ "``special_tokens_mask``: if adding special "
#~ "tokens, this is a list of [0, "
#~ "1], with 0 specifying special added "
#~ "tokens and 1 specifying sequence tokens."
#~ msgstr ""

#~ msgid ""
#~ "``offset_mapping``: list of (index of "
#~ "start char in text,index of end "
#~ "char in text) of token. (0,0) if"
#~ " token is a sqecial token"
#~ msgstr ""

#~ msgid ""
#~ "``overflow_to_sample``: index of example from"
#~ " which this feature is generated"
#~ msgstr ""

