# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../source/paddlenlp.transformers.tokenizer_utils.rst:2
msgid "tokenizer\\_utils"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:1
msgid "基类：:class:`object`"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:1
msgid ""
"The base class for all pretrained tokenizers. It provides some attributes"
" and common methods for all pretrained tokenizers, including attributes "
"for and special tokens (arguments of `__init__` whose name ends with "
"`_token`) and methods for saving and loading. It also includes some class"
" attributes (should be set by derived classes): - `tokenizer_config_file`"
" (str): represents the file name for saving and loading"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:7
msgid "tokenizer configuration, it's value is `tokenizer_config.json`."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:8
msgid ""
"`resource_files_names` (dict): use this to map resource related arguments"
" of `__init__` to specific file names for saving and loading."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:10
msgid ""
"`pretrained_resource_files_map` (dict): The dict has the same keys as "
"`resource_files_names`, the values are also dict mapping specific "
"pretrained model name to URL linking to vocabulary or other resources."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer:13
msgid ""
"`pretrained_init_configuration` (dict): The dict has pretrained model "
"names as keys, and the values are also dict preserving corresponding "
"configuration for tokenizer initialization."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.all_special_tokens:1
msgid ""
"List all the special tokens ('<unk>', '<cls>'...) mapped to class "
"attributes (cls_token, unk_token...)."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.all_special_ids:1
msgid ""
"List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) "
"mapped to class attributes (cls_token, unk_token...)."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_ids:1
msgid ""
"Converts a sequence of tokens into ids using the vocab. The tokenizer "
"should has the `vocab` attribute. Args："
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_ids:4
msgid "tokens (list(str)): List of tokens."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_ids
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_string
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary
msgid "返回"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_ids:5
msgid "Converted id list."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_ids
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_string
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary
msgid "返回类型"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_string:1
msgid ""
"Converts a sequence of tokens (list of string) to a single string by "
"using :code:`' '.join(tokens)` . :param tokens: List of tokens. :type "
"tokens: list(str)"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_tokens_to_string:6
msgid "Converted string."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_ids_to_tokens:1
msgid ""
"Converts a single index or a sequence of indices (integers) in a token or"
" a sequence of tokens (str) by using the vocabulary."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_ids_to_tokens
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences
msgid "参数"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.convert_ids_to_tokens:4
msgid "Don't decode special tokens (self.all_special_tokens). Default: False"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained:1
msgid ""
"Instantiate an instance of `PretrainedTokenizer` from a predefined "
"tokenizer specified by name or path., and it always corresponds to a "
"pretrained model. :param pretrained_model_name_or_path: A name of or a "
"file path to a"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained:5
msgid "pretrained model."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained:7
msgid ""
"position arguments for `__init__`. If provide, use this as position "
"argument values for tokenizer initialization."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained:10
msgid ""
"keyword arguments for `__init__`. If provide, use this to update pre-"
"defined keyword argument values for tokenizer initialization."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.from_pretrained:15
msgid "An instance of PretrainedTokenizer."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.save_pretrained:1
msgid ""
"Save tokenizer configuration and related resources to files under "
"`save_directory`. :param save_directory: Directory to save files into. "
":type save_directory: str"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.save_resources:1
msgid ""
"Save tokenizer related resources to files under `save_directory`. :param "
"save_directory: Directory to save files into. :type save_directory: str"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:1
msgid ""
"Instantiate an instance of `Vocab` from a file reserving all tokens by "
"using `Vocab.from_dict`. The file contains a token per line, and the line"
" number would be the index of corresponding token. :param filepath: path "
"of file to construct vocabulary. :type filepath: str :param unk_token: "
"special token for unknown token. If no need, it also"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:7
msgid "could be None. Default: None."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:9
msgid ""
"special token for padding token. If no need, it also could be None. "
"Default: None."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:12
msgid ""
"special token for bos token. If no need, it also could be None. Default: "
"None."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:15
msgid ""
"special token for eos token. If no need, it also could be None. Default: "
"None."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:18
msgid "keyword arguments for `Vocab.from_dict`."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.load_vocabulary:21
msgid "An instance of `Vocab`."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.save_vocabulary:1
msgid ""
"Save all tokens to a vocabulary file. The file contains a token per line,"
" and the line number would be the index of corresponding token. Agrs:"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.save_vocabulary:4
msgid ""
"filepath (str): File path to be saved to. vocab (Vocab|dict): the Vocab "
"or dict instance to be saved."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:1
msgid "Truncates a sequence pair in place to the maximum length."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:3
msgid ""
"list of tokenized input ids. Can be obtained from a string by chaining "
"the `tokenize` and `convert_tokens_to_ids` methods."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:5
msgid ""
"Optional second list of input ids. Can be obtained from a string by "
"chaining the `tokenize` and `convert_tokens_to_ids` methods."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:7
msgid "number of tokens to remove using the truncation strategy"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:9
msgid ""
"string selected in the following options: - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"max_seq_len     starting from the longest one at each token (when there "
"is a pair of input sequences).     Overflowing tokens only contains "
"overflow from the first sequence. - 'only_first': Only truncate the first"
" sequence. raise an error if the first sequence is shorter or equal to "
"than num_tokens_to_remove. - 'only_second': Only truncate the second "
"sequence - 'do_not_truncate': Does not truncate (raise an error if the "
"input sequence is longer than max_seq_len)"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:9
msgid ""
"string selected in the following options: - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"max_seq_len"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:11
msgid ""
"starting from the longest one at each token (when there is a pair of "
"input sequences). Overflowing tokens only contains overflow from the "
"first sequence."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:13
msgid ""
"'only_first': Only truncate the first sequence. raise an error if the "
"first sequence is shorter or equal to than num_tokens_to_remove."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:26
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:24
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:14
msgid "'only_second': Only truncate the second sequence"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:27
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:25
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:15
msgid ""
"'do_not_truncate': Does not truncate (raise an error if the input "
"sequence is longer than max_seq_len)"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.truncate_sequences:16
msgid ""
"If set to a number along with max_seq_len, the overflowing tokens "
"returned will contain some tokens from the main sequence returned. The "
"value of this argument defines the number of additional tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:4
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:3
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:3
msgid ""
"Should be overridden in a subclass if the model has a special way of "
"building those."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:6
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:8
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:10
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:11
msgid "List of input_id with the appropriate special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_inputs_with_special_tokens:12
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:14
msgid ":obj:`List[int]`"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:1
msgid ""
"Build offset map from a pair of offset map by concatenating and adding "
"offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:5
msgid "List of char offsets to which the special tokens will be added."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:7
msgid "Optional second list of char offsets for offset mapping pairs."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:10
msgid "List of char offsets with the appropriate offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.build_offset_mapping_with_special_tokens:11
msgid ":obj:`List[tuple]`"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieves sequence ids from a token list that has no special tokens "
"added. This method is called when adding special tokens using the "
"tokenizer ``encode`` methods."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask:4
msgid "List of ids of the first sequence."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask:6
msgid "List of ids of the second sequence."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model. Defaults to None."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.get_special_tokens_mask:12
msgid ""
"The list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create a mask from the two sequences passed to be used in a sequence-pair"
" classification task."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:6
msgid ""
"If :obj:`token_ids_1` is :obj:`None`, this method only returns the first "
"portion of the mask (0s)."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:8
msgid "List of IDs."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.create_token_type_ids_from_sequences:13
msgid "List of token_type_id according to the given sequence(s)."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:1
msgid ""
"Returns a dictionary containing the encoded sequence or sequence pair and"
" additional information: the mask for sequence classification and the "
"overflowing elements if a ``max_seq_len`` is specified."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:4
msgid ""
"The first sequence to be encoded. This can be a string, a list of strings"
" (tokenized string using the `tokenize` method) or a list of integers "
"(tokenized string ids using the `convert_tokens_to_ids` method)"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:8
msgid ""
"Optional second sequence to be encoded. This can be a string, a list of "
"strings (tokenized string using the `tokenize` method) or a list of "
"integers (tokenized string ids using the `convert_tokens_to_ids` method)"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:7
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:12
msgid ""
"If set to a number, will limit the total sequence returned so that it has"
" a maximum length. If there are overflowing tokens, those will be added "
"to the returned dictionary"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:10
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:15
msgid ""
"If set to True, the returned sequences will be padded according to the "
"model's padding side and padding index, up to their max length. If no max"
" length is specified, the padding is done up to the model's max length."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:21
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:19
msgid ""
"String selected in the following options:  - 'longest_first' (default) "
"Iteratively reduce the inputs sequence until the input is under "
"max_seq_len   starting from the longest one at each token (when there is "
"a pair of input sequences) - 'only_first': Only truncate the first "
"sequence - 'only_second': Only truncate the second sequence - "
"'do_not_truncate': Does not truncate (raise an error if the input "
"sequence is longer than max_seq_len)"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:21
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:19
msgid "String selected in the following options:"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:23
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:21
msgid ""
"'longest_first' (default) Iteratively reduce the inputs sequence until "
"the input is under max_seq_len starting from the longest one at each "
"token (when there is a pair of input sequences)"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:25
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:23
msgid "'only_first': Only truncate the first sequence"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:29
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:27
msgid "Set to True to return tokens position ids (default True)."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:31
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:29
msgid "Whether to return token type IDs."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:33
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:31
msgid "Whether to return the attention mask."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:35
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:33
msgid ""
"If set the resulting dictionary will include the length of each encoded "
"inputs"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:37
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:35
msgid "Set to True to return overflowing token information (default False)."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:39
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:37
msgid "Set to True to return special tokens mask information (default False)."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:40
msgid ""
"A Dictionary of shape::      {         input_ids: list[int],         "
"position_ids: list[int] if return_position_ids is True         "
"token_type_ids: list[int] if return_token_type_ids is True (default)"
"         attention_mask: list[int] if return_attention_mask is True"
"         seq_len: int if return_length is True         "
"overflowing_tokens: list[int] if a ``max_seq_len`` is specified and "
"return_overflowing_tokens is True         num_truncated_tokens: int if a "
"``max_seq_len`` is specified and return_overflowing_tokens is True"
"         special_tokens_mask: list[int] if return_special_tokens_mask is "
"True     }  With the fields:  - ``input_ids``: list of token ids to be "
"fed to a model - ``position_ids``: list of token position ids to be fed "
"to a model - ``token_type_ids``: list of token type ids to be fed to a "
"model - ``attention_mask``: list of indices specifying which tokens "
"should be attended to by the model - ``length``: the input_ids length - "
"``overflowing_tokens``: list of overflowing tokens if a max length is "
"specified. - ``num_truncated_tokens``: number of overflowing tokens a "
"``max_seq_len`` is specified - ``special_tokens_mask``: list of [0, 1], "
"with 0 specifying special added   tokens and 1 specifying sequence "
"tokens."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:42
msgid "A Dictionary of shape::"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:59
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:55
msgid "With the fields:"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:61
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:57
msgid "``input_ids``: list of token ids to be fed to a model"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:62
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:58
msgid "``position_ids``: list of token position ids to be fed to a model"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:63
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:59
msgid "``token_type_ids``: list of token type ids to be fed to a model"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:64
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:60
msgid ""
"``attention_mask``: list of indices specifying which tokens should be "
"attended to by the model"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:65
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:61
msgid "``length``: the input_ids length"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:66
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:62
msgid ""
"``overflowing_tokens``: list of overflowing tokens if a max length is "
"specified."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:67
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:63
msgid ""
"``num_truncated_tokens``: number of overflowing tokens a ``max_seq_len`` "
"is specified"
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.encode:64
msgid ""
"``special_tokens_mask``: list of [0, 1], with 0 specifying special added "
"tokens and 1 specifying sequence tokens."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:1
msgid ""
"Returns a list of dictionary containing the encoded sequence or sequence "
"pair and additional information: the mask for sequence classification and"
" the overflowing elements if a ``max_seq_len`` is specified."
msgstr ""

#: of paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:4
msgid ""
"Batch of sequences or pair of sequences to be encoded. This can be a list"
" of string/string-sequences/int-sequences or a list of pair of string"
"/string-sequences/int-sequence"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:14
msgid ""
"If set to a positive number and batch_text_or_text_pairs is a list of "
"pair sequences, the overflowing tokens which contain some tokens from the"
" end of the truncated second sequence will be concatenated with the first"
" sequence to generate new features. And The overflowing tokens would not "
"be returned in dictionary. The value of this argument defines the number "
"of overlapping tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:19
msgid "Whether or not the text has been pretokenized."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:42
msgid ""
"A List of dictionary of shape::      {         input_ids: list[int],"
"         position_ids: list[int] if return_position_ids is True         "
"token_type_ids: list[int] if return_token_type_ids is True (default)"
"         attention_mask: list[int] if return_attention_mask is True"
"         seq_len: int if return_length is True         "
"overflowing_tokens: list[int] if a ``max_seq_len`` is specified and "
"return_overflowing_tokens is True and stride is 0         "
"num_truncated_tokens: int if a ``max_seq_len`` is specified and "
"return_overflowing_tokens is True and stride is 0         "
"special_tokens_mask: list[int] if return_special_tokens_mask is True"
"         offset_mapping: list[Tuple] if stride is a positive number and "
"batch_text_or_text_pairs is a list of pair sequences         "
"overflow_to_sample: int if stride is a positive number and "
"batch_text_or_text_pairs is a list of pair sequences     }  With the "
"fields:  - ``input_ids``: list of token ids to be fed to a model - "
"``position_ids``: list of token position ids to be fed to a model - "
"``token_type_ids``: list of token type ids to be fed to a model - "
"``attention_mask``: list of indices specifying which tokens should be "
"attended to by the model - ``length``: the input_ids length - "
"``overflowing_tokens``: list of overflowing tokens if a max length is "
"specified. - ``num_truncated_tokens``: number of overflowing tokens a "
"``max_seq_len`` is specified - ``special_tokens_mask``: if adding special"
" tokens, this is a list of [0, 1], with 0 specifying special added   "
"tokens and 1 specifying sequence tokens. - ``offset_mapping``: list of "
"(index of start char in text,index of end char in text) of token. (0,0) "
"if token is a sqecial token - ``overflow_to_sample``: index of example "
"from which this feature is generated"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:44
msgid "A List of dictionary of shape::"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:68
msgid ""
"``special_tokens_mask``: if adding special tokens, this is a list of [0, "
"1], with 0 specifying special added tokens and 1 specifying sequence "
"tokens."
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:70
msgid ""
"``offset_mapping``: list of (index of start char in text,index of end "
"char in text) of token. (0,0) if token is a sqecial token"
msgstr ""

#: of
#: paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer.batch_encode:71
msgid ""
"``overflow_to_sample``: index of example from which this feature is "
"generated"
msgstr ""

