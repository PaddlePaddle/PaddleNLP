# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-18 21:31+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../source/paddlenlp.transformers.optimization.rst:2
msgid "optimization"
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:1
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup:1
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:1
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:1
msgid "基类：:class:`paddle.optimizer.lr.LambdaDecay`"
msgstr ""

#: of paddlenlp.transformers.optimization.LinearDecayWithWarmup:1
msgid ""
"Creates a learning rate scheduler, which increases learning rate linearly"
" from 0 to given `learning_rate`, after this warmup period learning rate "
"would be decreased linearly from the base learning rate to 0."
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:5
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup:7
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:5
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:6
msgid "The base learning rate. It is a python float number."
msgstr ""

#: of paddlenlp.transformers.optimization.CosineDecayWithWarmup:9
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:7
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:8
msgid "The number of training steps."
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:7
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup:11
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:9
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:10
msgid ""
"If int, it means the number of steps for warmup. If float, it means the "
"proportion of warmup in total training steps."
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:14
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup:23
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:12
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:19
msgid ""
"The index of last epoch. It can be set to restart training. If None, it "
"means initial learning rate. Defaults to -1."
msgstr ""

#: of paddlenlp.transformers.optimization.LinearDecayWithWarmup:16
msgid "If True, prints a message to stdout for each update. Defaults to False."
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:20
#: paddlenlp.transformers.optimization.CosineDecayWithWarmup:29
#: paddlenlp.transformers.optimization.LinearDecayWithWarmup:21
#: paddlenlp.transformers.optimization.PolyDecayWithWarmup:25
msgid "实际案例"
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:1
msgid ""
"Creates a learning rate scheduler, which increases learning rate linearly"
" from 0 to given `learning_rate` during warmup periods and keeps learning"
" rate a constant after that."
msgstr ""

#: of paddlenlp.transformers.optimization.ConstScheduleWithWarmup:10
msgid ""
"The number of training steps. If `warmup` is a float number, "
"`total_steps` must be provided. Defaults to None."
msgstr ""

#: of paddlenlp.transformers.optimization.CosineDecayWithWarmup:1
msgid ""
"Creates a learning rate scheduler, which increases learning rate linearly"
" from 0 to given `learning_rate`, after this warmup period learning rate "
"would be decreased following the values of the cosine function. If "
"`with_hard_restarts` is True, the cosine function could have serveral "
"hard restarts."
msgstr ""

#: of paddlenlp.transformers.optimization.CosineDecayWithWarmup:14
msgid "Whether cosine function has several hard restarts. Defaults to False."
msgstr ""

#: of paddlenlp.transformers.optimization.CosineDecayWithWarmup:17
msgid ""
"If `with_hard_restarts` is False, it means the number of waves in cosine "
"scheduler and should be an integer number and defaults to 1. If "
"`with_hard_restarts` is True, it means the number of hard restarts to use"
" and should be a float number and defaults to be 0.5. Defaults to None."
msgstr ""

#: of paddlenlp.transformers.optimization.PolyDecayWithWarmup:1
msgid ""
"Creates a learning rate scheduler, which increases learning rate linearly"
" from 0 to given `lr_init`, after this warmup period learning rate would "
"be decreased as a polynomial decay from the base learning rate to the end"
" learning rate `lr_end`."
msgstr ""

#: of paddlenlp.transformers.optimization.PolyDecayWithWarmup:13
msgid "The end learning rate. Defaults to 1e-7."
msgstr ""

#: of paddlenlp.transformers.optimization.PolyDecayWithWarmup:16
msgid "Power factor. Defaults to 1.0."
msgstr ""

#: of paddlenlp.transformers.optimization.CosineAnnealingWithWarmupDecay:1
msgid "基类：:class:`paddle.optimizer.lr.LRScheduler`"
msgstr ""

#: of
#: paddlenlp.transformers.optimization.CosineAnnealingWithWarmupDecay.get_lr:1
msgid ""
"For those subclass who overload ``LRScheduler`` (Base Class), User should"
" have a custom implementation of ``get_lr()`` ."
msgstr ""

#: of
#: paddlenlp.transformers.optimization.CosineAnnealingWithWarmupDecay.get_lr:3
msgid "Otherwise, an ``NotImplementedError`` exception will be thrown."
msgstr ""

