# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, PaddleNLP
# This file is distributed under the same license as the PaddleNLP package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PaddleNLP \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-07 11:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../source/paddlenlp.transformers.bert.tokenizer.rst:2
msgid "tokenizer"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer:1
#: paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer:1
msgid "基类：:class:`object`"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer:1
msgid ""
"Runs basic tokenization (punctuation splitting, lower casing, etc.). "
":param do_lower_case: Whether the text strips accents and convert to"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer:3
msgid ""
"lower case. If you use the BERT Pretrained model, lower is set to Flase "
"when using the cased model, otherwise it is set to True. Default: True."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer.tokenize:1
msgid ""
"Tokenizes a piece of text using basic tokenizer. :param text: A piece of "
"text. :type text: str"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer.tokenize
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.convert_tokens_to_string
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.tokenize
#: paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize
msgid "返回"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer.tokenize:5
msgid "A list of tokens."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BasicTokenizer.tokenize
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.convert_tokens_to_string
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.tokenize
#: paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize
msgid "返回类型"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:1
msgid "基类：:class:`paddlenlp.transformers.tokenizer_utils.PretrainedTokenizer`"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:1
msgid ""
"Constructs a BERT tokenizer. It uses a basic tokenizer to do punctuation "
"splitting, lower casing and so on, and follows a WordPiece tokenizer to "
"tokenize as subwords. :param vocab_file: file path of the vocabulary "
":type vocab_file: str :param do_lower_case: Whether the text strips "
"accents and convert to"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:7
msgid ""
"lower case. If you use the BERT pretrained model, lower is set to Flase "
"when using the cased model, otherwise it is set to True. Default: True."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add
msgid "参数"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:11
msgid "The special token for unkown words. Default: \"[UNK]\"."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:13
msgid "The special token for separator token . Default: \"[SEP]\"."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:15
msgid "The special token for padding. Default: \"[PAD]\"."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:17
msgid "The special token for cls. Default: \"[CLS]\"."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:19
msgid "The special token for mask. Default: \"[MASK]\"."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer:23
msgid "实际案例"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer.vocab_size:1
msgid ""
"return the size of vocabulary. :returns: the size of vocabulary. :rtype: "
"int"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer.tokenize:1
msgid ""
"End-to-end tokenization for BERT models. :param text: The text to be "
"tokenized. :type text: str"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.BertTokenizer.tokenize:5
msgid "A list of string representing converted tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.convert_tokens_to_string:1
msgid ""
"Converts a sequence of tokens (list of string) in a single string. Since "
"the usage of WordPiece introducing `##` to concat subwords, also remove "
"`##` when converting. :param tokens: A list of string representing tokens"
" to be converted. :type tokens: list"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.convert_tokens_to_string:7
msgid "Converted string from tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add:1
msgid ""
"Returns the number of added tokens when encoding a sequence with special "
"tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add:5
msgid ""
"This encodes inputs and checks the number of added tokens, and is "
"therefore not efficient. Do not put this inside your training loop."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add:8
msgid ""
"Returns the number of added tokens in the case of a sequence pair if set "
"to True, returns the number of added tokens in the case of a single "
"sequence if set to False."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.num_special_tokens_to_add:11
msgid "Number of tokens added to sequences"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:1
msgid ""
"Build model inputs from a sequence or a pair of sequence for sequence "
"classification tasks by concatenating and adding special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:4
msgid "A BERT sequence has the following format: ::"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:9
msgid "List of IDs to which the special tokens will be added."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:11
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:13
msgid "Optional second list of IDs for sequence pairs."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:14
msgid "List of input_id with the appropriate special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_inputs_with_special_tokens:15
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:17
msgid ":obj:`List[int]`"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:1
msgid ""
"Build offset map from a pair of offset map by concatenating and adding "
"offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:3
msgid "A BERT offset_mapping has the following format: ::"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:8
msgid "List of char offsets to which the special tokens will be added."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:10
msgid "Optional second list of char offsets for offset mapping pairs."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:13
msgid "List of char offsets with the appropriate offsets of special tokens."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.build_offset_mapping_with_special_tokens:14
msgid ":obj:`List[tuple]`"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:1
msgid ""
"Create a mask from the two sequences passed to be used in a sequence-pair"
" classification task."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:3
msgid "A BERT sequence pair mask has the following format: ::"
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:9
msgid ""
"If :obj:`token_ids_1` is :obj:`None`, this method only returns the first "
"portion of the mask (0s)."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:11
msgid "List of IDs."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.create_token_type_ids_from_sequences:16
msgid "List of token_type_id according to the given sequence(s)."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask:1
msgid ""
"Retrieves sequence ids from a token list that has no special tokens "
"added. This method is called when adding special tokens using the "
"tokenizer ``encode`` methods."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask:4
msgid "List of ids of the first sequence."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask:6
msgid "List of ids of the second sequence."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask:8
msgid ""
"Whether or not the token list is already formatted with special tokens "
"for the model. Defaults to None."
msgstr ""

#: of
#: paddlenlp.transformers.bert.tokenizer.BertTokenizer.get_special_tokens_mask:12
msgid ""
"The list of integers in the range [0, 1]: 1 for a special token, 0 for a "
"sequence token."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer:1
msgid ""
"Runs WordPiece tokenization. :param vocab: Vocab of the word piece "
"tokenizer. :type vocab: Vocab|dict :param unk_token: A specific token to "
"replace all unkown tokens. :type unk_token: str :param "
"max_input_chars_per_word: If a word's length is more than"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer:7
msgid "max_input_chars_per_word, it will be dealt as unknown word. Default: 100."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize:1
msgid ""
"Tokenizes a piece of text into its word pieces. This uses a greedy "
"longest-match-first algorithm to perform tokenization using the given "
"vocabulary. :param text: A single token or whitespace separated tokens. "
"This should have"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize:5
msgid "already been passed through `BasicTokenizer`."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize:7
msgid "A list of wordpiece tokens."
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize:11
msgid "示例"
msgstr ""

#: of paddlenlp.transformers.bert.tokenizer.WordpieceTokenizer.tokenize:12
msgid "input = \"unaffable\" output = [\"un\", \"##aff\", \"##able\"]"
msgstr ""

