============
模型压缩简介
============


近些年，基于Transformer的语言模型在机器翻译、阅读理解、文本匹配、自然语言推理等自然语言处理任务上取得了实质\
进展。然而，海量的参数和计算资源的大量耗费，使BERT及其变体在部署中困难重重。模型压缩的发展，使得这些问题得到\
了缓解。

模型压缩简介
------------

模型压缩在保证一定精度的情况下，能够降低模型的存储，加速模型的推理时间。常见的模型压缩方法主要包括模型裁剪、量化和蒸馏。\
下面分别对这几种方法进行简要的介绍。

模型裁剪
^^^^^^^^^^^^
模型裁剪是通过对已经训练好的模型中不重要的网络连接进行裁剪，减少模型的冗余和计算量，从而减少网络存储、大幅度进行加速的模型压缩方法。

量化
^^^^^^^^^^^^
一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示。实际上，有时不需要保留那么高的精度，可以通过量化的方法减少\
模型的存储空间，通常用INT8代替Float32存储。比如，SGD（Stochastic Gradient Descent）所需要的精度仅为6~8bit，\
因此合理的量化网络也可保证精度的情况下减小模型的存储体积，并且能够大幅度加速，使得神经网络在CPU上的运行成为可能。\
通常，量化包含多种方法，例如：二值神经网络、三元权重网络以及XNOR网络。


蒸馏
^^^^^^^^^^^^
蒸馏本质是student模型（参数量较少的模型）对teacher模型（参数量较多）的拟合，student模型从teacher中学到知识，比自己单独学习效果更好，。比较常见的方法通常是由Bert base蒸馏到\
Bi-LSTM或者是Transformer层数更少的BERT小模型。例如DistilBERT，它保留了BERT-base 97%的精度，\
减少了40%的参数，推理速度快了60%。


模型压缩示例
------------

下面将会对基于飞桨实现的常见的模型压缩示例进行介绍，其中《由BERT到Bi-LSTM的知识蒸馏》可以作为蒸馏实验的"Hello World"示例。\
而《使用DynaBERT中的策略对BERT进行压缩》中使用的DynaBERT则是同时对不同尺寸的子网络进行训练，通过该方法训练后可以在推理阶段直接对模型裁剪。


.. toctree::
   :maxdepth: 1

   distill_lstm.rst
   ofa_bert.rst
