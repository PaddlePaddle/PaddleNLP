# é¢„è®­ç»ƒ

[LLaMA v1/v2](./llama)ã€[GPT-3](./gpt-3) ç›®å½•ä¸­æä¾›äº†æ¨¡å‹é¢„è®­ç»ƒçš„æ•°æ®å‡†å¤‡å’Œè®­ç»ƒç»†èŠ‚ï¼Œåç»­æˆ‘ä»¬å°†æ”¯æŒæ›´å¤šçš„æ¨¡å‹é¢„è®­ç»ƒã€‚


```
# åƒé—®æ¨¡å‹é¢„è®­ç»ƒ
python -u  -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./qwen/pretrain_argument_stage2.json
```


## æ¨¡å‹é¢„è®­ç»ƒæ”¯æŒçš„åˆ†å¸ƒå¼èƒ½åŠ›ä¸€è§ˆ

æ¨¡å‹|èƒ½åŠ›|||||||||||||
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
||Data Parallelism|Tensor Parallelism|Pipeline Parallelism|sequence parallelism|Flash Attention|Sharding Stage1 ||Stage2||Stage3||Selective Recompute|
|||||||recompute|DP|recompute|DP|recompute|DP||
LLaMA-65B   |âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
LLaMA2-70B  |âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
BaiChuan-13B|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
GPT3        |âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
Qwen-7B     |âœ…|âœ…|âœ…|ó € ó € ó € â¬œ|âœ…|â¬œ|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
Qwen-14B    |âœ…|âœ…|âœ…|â¬œ|âœ…|â¬œ|âœ…|âœ…|âœ…|âœ…|âœ…|âœ…|
OPT 66B     |âœ…|â¬œ|â¬œ|â¬œ|âŒ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|ğŸš§|
Bloom-176B  |âœ…|âœ…|â¬œ|â¬œ|âŒ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|ğŸš§|
ChatGLM-6B  |âœ…|âœ…|â¬œ|â¬œ|âŒ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|ğŸš§|
GLM 130B    |âœ…|âœ…|â¬œ|â¬œ|âŒ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|â¬œ|ğŸš§|

* âœ…: å·²æ”¯æŒï¼ŒSupported
* ğŸš§: éƒ¨åˆ†æ”¯æŒï¼ŒIn Progress
* âŒ: æš‚ä¸æ”¯æŒï¼ŒNot Supported


## æ¨¡å‹æƒé‡æ”¯æŒåˆ—è¡¨
ä¸Šè¡¨ä¸­å±•ç¤ºçš„æ˜¯éƒ¨åˆ†æ¨¡å‹æƒé‡ï¼Œæ”¯æŒçš„æ‰€æœ‰æ¨¡å‹å¦‚ä¸‹ï¼š

```
* LLaMAç³»åˆ—
  - facebook/llama-7b [è‹±æ–‡]
  - facebook/llama-13b [è‹±æ–‡]
  - facebook/llama-65b [è‹±æ–‡]
  - meta-llama/Llama-2-7b [è‹±æ–‡]
  - meta-llama/Llama-2-7b-chat [è‹±æ–‡]
  - meta-llama/Llama-2-13b [è‹±æ–‡]
  - meta-llama/Llama-2-13b-chat [è‹±æ–‡]
  - meta-llama/Llama-2-70b [è‹±æ–‡]
  - baichuan-inc/Baichuan-7B [ä¸­æ–‡]
  - baichuan-inc/Baichuan-13B-Base [ä¸­æ–‡]
  - baichuan-inc/Baichuan-13B-Chat [ä¸­æ–‡]
  - baichuan-inc/Baichuan2-7B-Base [ä¸­æ–‡]
  - baichuan-inc/Baichuan2-7B-Chat [ä¸­æ–‡]
  - baichuan-inc/Baichuan2-13B-Base [ä¸­æ–‡]
  - baichuan-inc/Baichuan2-13B-Chat [ä¸­æ–‡]
  - FlagAlpha/Llama2-Chinese-7b-Chat [ä¸­æ–‡]
  - FlagAlpha/Llama2-Chinese-13b-Chat [ä¸­æ–‡]
  - idea-ccnl/ziya-llama-13b-v1 [ä¸­æ–‡]
  - linly-ai/chinese-llama-2-7b [ä¸­æ–‡]
  - linly-ai/chinese-llama-2-13b [ä¸­æ–‡]
* ChatGLMç³»åˆ—
  - THUDM/chatglm-6b-v1.1 [ä¸­æ–‡]
  - THUDM/chatglm2-6b [ä¸­æ–‡]
* BLOOMç³»åˆ—
  - bigscience/bloom-7b1 [è‹±æ–‡]
  - bigscience/bloomz-7b1 [å¤šè¯­è¨€]
  - bigscience/bloomz-7b1-mt [å¤šè¯­è¨€]
* Qwenç³»åˆ—
  - qwen/qwen-7b [ä¸­æ–‡]
  - qwen/qwen-7b-chat [ä¸­æ–‡]
  - qwen/qwen-14b [ä¸­æ–‡]
  - qwen/qwen-14b-chat [ä¸­æ–‡]
```


## é¢„è®­ç»ƒæ€§èƒ½
ä»¥ä¸‹æµ‹è¯•ç»“æœåŸºäº

æœºå™¨ç¯å¢ƒï¼š A100 80G * 8, CUDA 11.8, NCCL 2.15

è®­ç»ƒé…ç½®ï¼šå•ä¸ªstep, global batch sizeä¸º 256K token.
```
paddle commit id              : 9bf5a86f13aa85b38715c015693f973290c8f9da
paddlenlp commit id           : 0731766c01ae85891b5bbe770eb397eec2287cbb
```
| æ¨¡å‹ | åºåˆ—é•¿åº¦ | åˆ†å¸ƒå¼ç­–ç•¥ | é€Ÿåº¦(`tokens/card/sec`) | æ˜¾å­˜å ç”¨(`MB^1`) | é…ç½®æ–‡ä»¶
| :-: | :-: | :-: | :-: | :-: |  :-: |
| `baichuan-inc/Baichuan2-13B-Base` |4096 | sd8 |    1366  | 74767MB | `./llama/pretrain_baichuan2-13b-sd8-stage2.json`
| `baichuan-inc/Baichuan2-7B-Base` |4096 | tp2sd4 |     3570  |58363MB | `./llama/pretrain_baichuan2-7b-tp2sd4-stage2.json`
| `facebook/llama-13b` |4096 | tp2sd4 |     1980  | 64402MB | `./llama/pretrain_llama-13b-tp2sd4-stage2.json`
| `facebook/llama-7b` |4096 | tp2sd4 |     3740  | 52092MB | `./llama/pretrain_llama-7b-tp2sd4-stage2.json`
| `FlagAlpha/Llama2-Chinese-13b-Chat` |4096 | tp2sd4 |     1978 | 64185MB | `./llama/pretrain_flagalpha-llama2-13b-tp2sd4-stage2.json`
| `FlagAlpha/Llama2-Chinese-7b-Chat` |4096 | tp2sd4 |     3760 | 52092MB | `./llama/pretrain_flagalpha-llama2-7b-tp2sd4-stage2.json`
| `idea-ccnl/ziya-llama-13b-v1` |4096 | tp2sd4 |     1973 | 64233MB | `./llama/pretrain_ziya-llama-13b-tp2sd4-stage2.json`
| `linly-ai/chinese-llama-2-7b` |4096 | tp2sd4 |     3742 | 51751MB | `./llama/pretrain_linly-llama2-7b-tp2sd4-stage2.json`
| `meta-llama/Llama-2-13b` |4096 | tp2sd4 |     1980 | 64199MB | `./llama/pretrain_llama2-13b-tp2sd4-stage2.json`
| `meta-llama/Llama-2-7b` |4096 | tp2sd4 |     3756 | 52092MB | `./llama/pretrain_llama2-7b-tp2sd4-stage2.json`
| `qwen/qwen-7b` |4096 | tp2sd4 |     3602 | 65448MB | `./qwen/pretrain_qwen-7b-tp2sd4-stage2.json`

æ³¨ï¼š
1. æ˜¾å­˜å ç”¨(MB)ä½¿ç”¨çš„æ˜¯ `max_memory_allocated`, å®é™…ç‰©ç†æ˜¾å­˜ä¼šå ç”¨æ›´å¤šï¼Œå¤§çº¦å¤š2-3GB.
2. é€Ÿåº¦ä¼šæœ‰å°å¹…æ³¢åŠ¨ï¼Œä¾‹å¦‚ `facebook/llama-7b` å’Œ `meta-llama/Llama-2-7b` æ˜¯ç›¸åŒè®­ç»ƒé…ç½®ã€‚
