{
    "task": "commonsense",
    "data_dir": "/home/ldn/baidu/pyreft/paddle-version/loreft/datasets",
    "train_dataset": null,
    "eval_dataset": null,
    "model": "/home/ldn/.paddlenlp/models/meta-llama/Llama-2-7b",
    "seed": 42,
    "layers": "31",
    "rank": 8,
    "position": "f7+l7",
    "epochs": 6,
    "is_wandb": false,
    "wandb_name": "reft",
    "save_model": true,
    "max_n_train_example": null,
    "max_n_eval_example": null,
    "intervention_type": "TinyIntervention",
    "gradient_accumulation_steps": 1,
    "batch_size": 8,
    "eval_batch_size": 4,
    "output_dir": "./official_results_paddle",
    "lr": 0.0003,
    "schedule": "linear",
    "warmup_ratio": 0.01,
    "weight_decay": 0.0,
    "dropout": 0.0,
    "act_fn": null,
    "add_bias": false,
    "test_split": "test",
    "train_on_inputs": false,
    "max_length": 512,
    "use_normalized_template": true,
    "allow_cls_grad": false,
    "metric_for_best_model": "accuracy",
    "dtype": "bfloat16",
    "logging_steps": 1,
    "wandb_dir": "wandb",
    "wandb_proj": "MyReFT",
    "share_weights": true,
    "greedy_decoding": true,
    "temperature": null,
    "top_p": null,
    "top_k": null,
    "n_params": 69640
}