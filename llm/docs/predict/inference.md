# å¤§æ¨¡å‹æ¨ç†æ•™ç¨‹

PaddleNLPä»¥ä¸€ç«™å¼ä½“éªŒã€æè‡´æ€§èƒ½ä¸ºè®¾è®¡ç†å¿µï¼Œå®ç°å¤§æ¨¡å‹çš„å¿«é€Ÿæ¨ç†ã€‚

PaddleNLPå¤§æ¨¡å‹æ¨ç†æ„å»ºäº†é«˜æ€§èƒ½æ¨ç†æ–¹æ¡ˆï¼š

- å†…ç½®åŠ¨æ€æ’å…¥å’Œå…¨ç¯èŠ‚ç®—å­èåˆç­–ç•¥

- æ”¯æŒPageAttentionã€FlashDecodingä¼˜åŒ–

- æ”¯æŒWeight Only INT8åŠINT4æ¨ç†ï¼Œæ”¯æŒæƒé‡ã€æ¿€æ´»ã€Cache KVè¿›è¡ŒINT8ã€FP8é‡åŒ–çš„æ¨ç†

- æä¾›åŠ¨æ€å›¾æ¨ç†å’Œé™æ€å›¾æ¨ç†ä¸¤ç§æ–¹å¼


PaddleNLPå¤§æ¨¡å‹æ¨ç†æä¾›å‹ç¼©ã€æ¨ç†ã€æœåŠ¡å…¨æµç¨‹ä½“éªŒ ï¼š

- æä¾›å¤šç§PTQæŠ€æœ¯ï¼Œæä¾›WACï¼ˆæƒé‡/æ¿€æ´»/ç¼“å­˜ï¼‰çµæ´»å¯é…çš„é‡åŒ–èƒ½åŠ›ï¼Œæ”¯æŒINT8ã€FP8ã€4Bité‡åŒ–èƒ½åŠ›

- æ”¯æŒå¤šç¡¬ä»¶å¤§æ¨¡å‹æ¨ç†ï¼ŒåŒ…æ‹¬[æ˜†ä»‘XPU](../../xpu/llama/README.md)ã€[æ˜‡è…¾NPU](../../npu/llama/README.md)ã€[æµ·å…‰K100](../dcu_install.md)ã€[ç‡§åŸGCU](../../gcu/llama/README.md)ã€[X86 CPU](../cpu_install.md)ç­‰

- æä¾›é¢å‘æœåŠ¡å™¨åœºæ™¯çš„éƒ¨ç½²æœåŠ¡ï¼Œæ”¯æŒè¿ç»­æ‰¹å¤„ç†(continuous batching)ã€æµå¼è¾“å‡ºç­‰åŠŸèƒ½ï¼Œæ”¯æŒgRPCã€HTTPåè®®çš„æœåŠ¡æ¥å£


## 1. æ¨¡å‹æ”¯æŒ

PaddleNLP ä¸­å·²ç»æ·»åŠ é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ç›¸å…³å®ç°ï¼Œå·²éªŒè¯è¿‡çš„æ¨¡å‹å¦‚ä¸‹ï¼š
| Models | Example Models |
|--------|----------------|
|Llama 3.1, Llama 3, Llama 2|`meta-llama/Meta-Llama-3.1-8B`, `meta-llama/Meta-Llama-3.1-8B-Instruct`, `meta-llama/Meta-Llama-3.1-405B`, `meta-llama/Meta-Llama-3.1-405B-Instruct`,`meta-llama/Meta-Llama-3-8B`, `meta-llama/Meta-Llama-3-8B-Instruct`, `meta-llama/Meta-Llama-3-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-Guard-3-8B`, `Llama-2-7b, meta-llama/Llama-2-7b-chat`, `meta-llama/Llama-2-13b`, `meta-llama/Llama-2-13b-chat`, `meta-llama/Llama-2-70b`, `meta-llama/Llama-2-70b-chat`|
|Qwen 2| `Qwen/Qwen2-0.5B`, `Qwen/Qwen2-0.5B-Instruct`, `Qwen/Qwen2-1.5B`, `Qwen/Qwen2-1.5B-Instruct`, `Qwen/Qwen2-7B`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-72B`, `Qwen/Qwen2-72B-Instruct`, `Qwen/Qwen2-57B-A14B`, `Qwen/Qwen2-57B-A14B-Instruct`|
|Qwen-Moe| `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, `Qwen/Qwen2-57B-A14B`, `Qwen/Qwen2-57B-A14B-Instruct`|
|Mixtral| `mistralai/Mixtral-8x7B-Instruct-v0.1`|
|ChatGLM 3, ChatGLM 2| `THUDM/chatglm3-6b`, `THUDM/chatglm2-6b`|
|Baichuan 2, Baichuan|`baichuan-inc/Baichuan2-7B-Base`, `baichuan-inc/Baichuan2-7B-Chat`, `baichuan-inc/Baichuan2-13B-Base`, `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, `baichuan-inc/Baichuan-13B-Base`, `baichuan-inc/Baichuan-13B-Chat`|


## 2. ç¡¬ä»¶&ç²¾åº¦æ”¯æŒ

PaddleNLP æä¾›äº†å¤šç§ç¡¬ä»¶å¹³å°å’Œç²¾åº¦æ”¯æŒï¼ŒåŒ…æ‹¬ï¼š

| Precision      | Hopper| Ada | Ampere | Turing | Volta | æ˜†ä»‘XPU | æ˜‡è…¾NPU | æµ·å…‰K100 | ç‡§åŸGCU | x86 CPU |
|:--------------:|:-----:|:---:|:------:|:------:|:-----:|:------:|:-------:|:-------:|:------:|:-------:|
| FP32           |  âœ…   |  âœ… | âœ…     | âœ…      | âœ…    | âœ…      |  âœ…     | âœ…      | âœ…      |   âœ…    |
| FP16           |  âœ…   |  âœ… | âœ…     | âœ…      | âœ…    | âœ…      |  âœ…     | âœ…      | âœ…      |   âœ…    |
| BF16           |  âœ…   |  âœ… | âœ…     | âŒ      | âŒ    | âŒ      |  âŒ     | âŒ      | âŒ      |   âœ…    |
| INT8           |  âœ…   |  âœ… | âœ…     | âœ…      | âœ…    | âœ…      |  âœ…     | âœ…      | âŒ      |   âœ…    |
| FP8            |  ğŸš§   |  âœ… | âŒ     | âŒ      | âŒ    | âŒ      |  âŒ     | âŒ      | âŒ      |   âŒ    |


## 3. æ¨ç†å‚æ•°

PaddleNLP æä¾›äº†å¤šç§å‚æ•°ï¼Œç”¨äºé…ç½®æ¨ç†æ¨¡å‹å’Œä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚

### 3.1 å¸¸è§„å‚æ•°

- `model_name_or_path`: å¿…éœ€ï¼Œé¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è€…æœ¬åœ°çš„æ¨¡å‹è·¯å¾„ï¼Œç”¨äºçƒ­å¯æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé»˜è®¤ä¸ºNoneã€‚

- `dtype`: å¿…éœ€ï¼Œæ¨¡å‹å‚æ•°dtypeï¼Œé»˜è®¤ä¸ºNoneã€‚å¦‚æœæ²¡æœ‰ä¼ å…¥`lora_path`æˆ–`prefix_path`åˆ™å¿…é¡»ä¼ å…¥`dtype`å‚æ•°ã€‚

- `lora_path`: LoRAå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹LoRAå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚

- `prefix_path`: Prefix Tuningå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹Prefix Tuningå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚

- `batch_size`: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1ã€‚è¯¥å‚æ•°è¶Šå¤§ï¼Œå ç”¨æ˜¾å­˜è¶Šé«˜ï¼›è¯¥å‚æ•°è¶Šå°ï¼Œå ç”¨æ˜¾å­˜è¶Šä½ã€‚

- `data_file`: å¾…æ¨ç†jsonæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºNoneã€‚æ ·ä¾‹æ•°æ®ï¼š

    ```json
    {"tgt":"", "src": "å†™ä¸€ä¸ª300å­—çš„å°è¯´å¤§çº²ï¼Œå†…å®¹æ˜¯æç™½ç©¿è¶Šåˆ°ç°ä»£ï¼Œæœ€åæˆä¸ºå…¬å¸æ–‡èŒäººå‘˜çš„æ•…äº‹"}
    {"tgt":"", "src": "æˆ‘è¦é‡‡è®¿ä¸€ä½ç§‘å¹»ä½œå®¶ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«5ä¸ªé—®é¢˜çš„åˆ—è¡¨"}
    ```

- `output_file`: ä¿å­˜æ¨ç†ç»“æœæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºoutput.jsonã€‚

- `device`: è¿è¡Œç¯å¢ƒï¼Œé»˜è®¤ä¸ºgpuï¼Œå¯é€‰çš„æ•°å€¼æœ‰gpuã€[cpu](../cpu_install.md)ã€[xpu](../../xpu/llama/README.md)ã€[npu](../../npu/llama/README.md)ã€[gcu](../../gcu/llama/README.md)ç­‰ï¼ˆ[dcu](../dcu_install.md)ä¸gpuæ¨ç†å‘½ä»¤ä¸€è‡´ï¼‰ã€‚

- `model_type`: åˆå§‹åŒ–ä¸åŒç±»å‹æ¨¡å‹ï¼Œgpt-3: GPTForCausalLM; ernie-3.5-se: Ernie35ForCausalLM; é»˜è®¤ä¸º Noneã€‚

- `mode`: ä½¿ç”¨åŠ¨æ€å›¾æˆ–è€…é™æ€å›¾æ¨ç†ï¼Œå¯é€‰å€¼æœ‰`dynamic`ã€ `static`ï¼Œé»˜è®¤ä¸º`dynamic`ã€‚

- `avx_model`: å½“ä½¿ç”¨CPUæ¨ç†æ—¶ï¼Œæ˜¯å¦ä½¿ç”¨AvxModelï¼Œé»˜è®¤ä¸ºFalseã€‚å‚è€ƒ[CPUæ¨ç†æ•™ç¨‹](../cpu_install.md)ã€‚

- `avx_type`: avxè®¡ç®—ç±»å‹ï¼Œé»˜è®¤ä¸ºNoneã€‚å¯é€‰çš„æ•°å€¼æœ‰`fp16`ã€ `bf16`ã€‚

- `src_length`: æ¨¡å‹è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§tokené•¿åº¦ï¼Œé»˜è®¤ä¸º1024ã€‚

- `max_length`:æ¨¡å‹è¾“å…¥ï¼ˆä¸Šä¸‹æ–‡+ç”Ÿæˆå†…å®¹ï¼‰çš„æœ€å¤§tokené•¿åº¦, é»˜è®¤ä¸º2048ã€‚


### 3.2 æ€§èƒ½ä¼˜åŒ–å‚æ•°

- `inference_model`: æ˜¯å¦ä½¿ç”¨ Inference Model æ¨ç†ï¼Œé»˜è®¤å€¼ä¸º Falseã€‚Inference Model å†…ç½®åŠ¨æ€æ’å…¥å’Œå…¨ç¯èŠ‚ç®—å­èåˆç­–ç•¥ï¼Œå¼€å¯åæ€§èƒ½æ›´ä¼˜ã€‚

- `block_attn`: æ˜¯å¦ä½¿ç”¨ Block Attention æ¨ç†ï¼Œ é»˜è®¤å€¼ä¸ºFalseã€‚Block Attention æ˜¯åŸºäº PageAttention çš„æ€æƒ³è®¾è®¡å¹¶å®ç°çš„ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½æ¨ç†å’ŒåŠ¨æ€æ’å…¥çš„åŸºç¡€ä¸Šå¯ä»¥åŠ¨æ€åœ°ä¸º cachekv åˆ†é…å­˜å‚¨ç©ºé—´ï¼Œæå¤§åœ°èŠ‚çœæ˜¾å­˜å¹¶æå‡æ¨ç†çš„ååã€‚

- `block_size`: å¦‚æœä½¿ç”¨ Block Attention æ¨ç†ï¼ŒæŒ‡å®šä¸€ä¸ª Block å¯ä»¥å­˜å‚¨çš„ token æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º64ã€‚


### 3.3 é‡åŒ–å‚æ•°

PaddleNLP æä¾›äº†å¤šç§é‡åŒ–ç­–ç•¥ï¼Œæ”¯æŒWeight Only INT8åŠINT4æ¨ç†ï¼Œæ”¯æŒWACï¼ˆæƒé‡ã€æ¿€æ´»ã€Cache KVï¼‰è¿›è¡ŒINT8ã€FP8é‡åŒ–çš„æ¨ç†

- `quant_type`: æ˜¯å¦ä½¿ç”¨é‡åŒ–æ¨ç†ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚å¯é€‰çš„æ•°å€¼æœ‰`weight_only_int8`ã€`weight_only_int4`ã€`a8w8`å’Œ`a8w8_fp8`ã€‚`a8w8`ä¸`a8w8_fp8`éœ€è¦é¢å¤–çš„actå’Œweightçš„scaleæ ¡å‡†è¡¨ï¼Œæ¨ç†ä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹å¯¼å‡ºå‚è€ƒ[å¤§æ¨¡å‹é‡åŒ–æ•™ç¨‹](../quantization.md)ã€‚

- `cachekv_int8_type`: æ˜¯å¦ä½¿ç”¨cachekv int8é‡åŒ–ï¼Œé»˜è®¤å€¼ä¸ºNoneã€‚å¯é€‰`dynamic`ï¼ˆå·²ä¸å†ç»´æŠ¤ï¼Œä¸å»ºè®®ä½¿ç”¨ï¼‰å’Œ`static`ä¸¤ç§ï¼Œ`static`éœ€è¦é¢å¤–çš„cache kvçš„scaleæ ¡å‡†è¡¨ï¼Œä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹å¯¼å‡ºå‚è€ƒ[å¤§æ¨¡å‹é‡åŒ–æ•™ç¨‹](../quantization.md)ã€‚


### 3.4 è§£ç ç­–ç•¥å‚æ•°

- `decode_strategy`: æ¨ç†è§£ç ç­–ç•¥ï¼Œé»˜è®¤å€¼ä¸º`sampling`ï¼Œå¯é€‰çš„æ•°å€¼æœ‰`greedy_search`ã€`beam_search`å’Œ`sampling`ã€‚

- `top_k`: â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¸º top-k è¿‡æ»¤ä¿ç•™çš„æœ€é«˜æ¦‚ç‡æ ‡è®°çš„æ•°é‡ã€‚é»˜è®¤å€¼ä¸º1ï¼Œç­‰ä»·äºè´ªå¿ƒç­–ç•¥ã€‚

- `top_p`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ top-p è¿‡æ»¤çš„ç´¯ç§¯æ¦‚ç‡ã€‚é»˜è®¤å€¼ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚

- `temperature`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¼šå¯¹è¾“å‡ºlogité™¤ä»¥temperatureã€‚é»˜è®¤å€¼ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚

### 3.4 æ€§èƒ½åˆ†æå‚æ•°

- `benchmark`: æ˜¯å¦å¼€å¯æ€§èƒ½åˆ†æï¼Œé»˜è®¤å€¼ä¸ºFalseã€‚å¦‚æœè®¾ä¸ºtrueï¼Œä¼šå°†æ¨¡å‹è¾“å…¥å¡«å……ä¸ºsrc_lengthå¹¶å¼ºåˆ¶è§£ç åˆ°max_lengthï¼Œå¹¶è®¡ç®—æ¨¡å‹æ¨ç†ååé‡ã€è®°å½•æ¨ç†æ—¶é—´ã€‚


## 4. å¿«é€Ÿå¼€å§‹

### 4.1 ç¯å¢ƒå‡†å¤‡

å‚è€ƒ[å®‰è£…æ•™ç¨‹](./installation.md)ã€‚

### 4.2 æ¨ç†ç¤ºä¾‹

ä¸‹é¢ç»™å‡ºLlama2-7Bçš„åŠ¨æ€å›¾æ¨ç†ç¤ºä¾‹ï¼š

```shell
# åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn

# XPUè®¾å¤‡åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn --device xpu

# Weight Only Int8 åŠ¨æ€å›¾æ¨ç†å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --quant_type weight_only_int8 --block_attn

# PTQ-A8W8æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --dtype float16 --block_attn --quant_type a8w8

# PTQ-A8W8C8æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --dtype float16 --block_attn --quant_type a8w8  --cachekv_int8_type static

# CacheKV åŠ¨æ€é‡åŒ–æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn --cachekv_int8_type dynamic
```

**Note:**

1. `quant_type`å¯é€‰çš„æ•°å€¼æœ‰`weight_only_int8`ã€`weight_only_int4`ã€`a8w8`å’Œ`a8w8_fp8`ã€‚
2. `a8w8`ä¸`a8w8_fp8`éœ€è¦é¢å¤–çš„actå’Œweightçš„scaleæ ¡å‡†è¡¨ï¼Œæ¨ç†ä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹å¯¼å‡ºå‚è€ƒ[å¤§æ¨¡å‹é‡åŒ–æ•™ç¨‹](../quantization.md)ã€‚
3. `cachekv_int8_type`å¯é€‰`dynamic`ï¼ˆå·²ä¸å†ç»´æŠ¤ï¼Œä¸å»ºè®®ä½¿ç”¨ï¼‰å’Œ`static`ä¸¤ç§ï¼Œ`static`éœ€è¦é¢å¤–çš„cache kvçš„scaleæ ¡å‡†è¡¨ï¼Œä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹å¯¼å‡ºå‚è€ƒ[å¤§æ¨¡å‹é‡åŒ–æ•™ç¨‹](../quantization.md)ã€‚

æ›´å¤šå¤§æ¨¡å‹æ¨ç†æ•™ç¨‹ï¼š

-  [llama](./llama.md)
-  [qwen](./qwen.md)
-  [mixtral](./mixtral.md)

ç¯å¢ƒå‡†å¤‡ï¼Œå‚è€ƒï¼š

- [å®‰è£…æ•™ç¨‹](./installation.md)

è·å–æœ€ä½³æ¨ç†æ€§èƒ½ï¼š

- [æœ€ä½³å®è·µ](./best_practices.md)

æ›´å¤šå‹ç¼©ã€æœåŠ¡åŒ–æ¨ç†ä½“éªŒï¼š

- [å¤§æ¨¡å‹é‡åŒ–æ•™ç¨‹](../quantization.md)
- [æœåŠ¡åŒ–éƒ¨ç½²æ•™ç¨‹](https://github.com/PaddlePaddle/FastDeploy/blob/develop/README_CN.md)

æ›´å¤šç¡¬ä»¶å¤§æ¨¡å‹æ¨ç†æ•™ç¨‹ï¼š

- [æ˜†ä»‘XPU](../../xpu/llama/README.md)
- [æ˜‡è…¾NPU](../../npu/llama/README.md)
- [æµ·å…‰K100](../dcu_install.md)
- [ç‡§åŸGCU](../../gcu/llama/README.md)
- [X86 CPU](../cpu_install.md)

## è‡´è°¢

æˆ‘ä»¬å‚è€ƒ[PageAttention](https://github.com/vllm-project/vllm)çš„pageåˆ†å—çš„æ€æƒ³å®ç°äº†generationé˜¶æ®µçš„block attentionã€‚åŸºäº[Flash Decoding](https://github.com/Dao-AILab/flash-attention)çš„KVåˆ†å—æ€æƒ³å®ç°äº†é•¿sequenceåœºæ™¯ä¸‹çš„æ¨ç†åŠ é€Ÿã€‚åŸºäº[Flash Attention2](https://github.com/Dao-AILab/flash-attention)å®ç°äº†prefillé˜¶æ®µçš„attentionåŠ é€Ÿã€‚FP8 GEMMåŸºäº[CUTLASS](https://github.com/NVIDIA/cutlass)çš„é«˜æ€§èƒ½æ¨¡æ¿åº“å®ç°ã€‚æœ‰éƒ¨åˆ†ç®—å­å¦‚gemm_dequantå‚è€ƒäº†[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)å’Œ[FasterTransformer](https://github.com/NVIDIA/FasterTransformer.git)çš„å®ç°å’Œä¼˜åŒ–æ€è·¯ã€‚

