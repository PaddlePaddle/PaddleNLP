.. _introduction:
===============================
å¤§æ¨¡å‹é¢„è®­ç»ƒä»‹ç»
===============================


PaddleNLPå¤§æ¨¡å‹å¥—ä»¶æ”¯æŒ LLaMA v1/v2ã€GPT-3ã€BaiChuanã€Qwen ç­‰å¤§æ¨¡å‹çš„é¢„è®­ç»ƒæ”¯æŒã€‚

git clone ä»£ç åˆ°æœ¬åœ°ï¼Œå³å¯å¼€å§‹ã€‚

.. code-block:: bash

    git clone https://github.com/PaddlePaddle/PaddleNLP.git
    # pip install ./PaddleNLP ä½¿ç”¨developç‰ˆæœ¬
    cd PaddleNLP/llm
    # åˆ°è¾¾è¿è¡Œç›®å½•



.. _create-dataset:
æ•°æ®åˆ¶ä½œ
-----------------------------

è¯¦ç»†åˆ¶ä½œæµç¨‹å¯å‚è€ƒä¸‹åˆ—æ–‡æ¡£ï¼š

.. toctree::
   :maxdepth: 1

   å†…ç½®é¢„ç»ƒæ•°æ®é›†åŠè‡ªå®šä¹‰æ•°æ®åˆ¶ä½œ <../dataset.md>
   CLUECorpus2020 è¯­æ–™åˆ¶ä½œ <../tools/preprocess/docs/CLUECorpus2020.md>
   CLUECorpusSmall  è¯­æ–™åˆ¶ä½œ <../tools/preprocess/docs/CLUECorpusSmall.md>
   OpenWebText2 è¯­æ–™åˆ¶ä½œ <../tools/preprocess/docs/OpenWebText2.md>
   WuDaoCorpus2.0 Base è¯­æ–™ <../tools/preprocess/docs/WuDaoCorpusBase.md>



.. _start_training:
å¼€å§‹è®­ç»ƒ
-------------------------


ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·è¿è¡Œæµ‹è¯•æœ¬æ¨¡å‹ï¼Œæœ¬é¡¹ç›®æä¾›äº†å¤„ç†å¥½çš„100kæ¡docçš„è®­ç»ƒæ ·æœ¬ï¼š

.. code-block:: bash

    # llama æ¨¡å‹æ•°æ®ä¸‹è½½
    wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k.bin
    wget https://bj.bcebos.com/paddlenlp/models/transformers/llama/data/llama_openwebtext_100k.idx

    # gpt æ¨¡å‹æ•°æ®ä¸‹è½½
    # wget https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_ids.npy
    # wget https://bj.bcebos.com/paddlenlp/models/transformers/gpt/data/gpt_en_dataset_300m_idx.npz


å°†æ‰€æœ‰é¢„å¤„ç†å¾—åˆ°çš„æ–‡ä»¶ç»Ÿä¸€æ”¾å…¥ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œä»¥å¤‡è®­ç»ƒä½¿ç”¨ï¼š

.. code-block:: bash

    mkdir data
    mv llama_openwebtext_100k.bin ./data
    mv llama_openwebtext_100k.idx ./data



.. code-block:: bash

    # ç¼–è¯‘è‡ªå®šä¹‰ç®—å­ï¼Œå¯é€‰
    cd ../slm/model_zoo/gpt-3/external_ops/ && python3 setup.py install && cd -

    # llama æ¨¡å‹é¢„è®­ç»ƒ
    python -u  -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./config/llama/pretrain_argument.json

    # Qwen æ¨¡å‹é¢„è®­ç»ƒ
    python -u  -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./config/qwen/pretrain_argument.json


æ³¨æ„ï¼š

1. å»ºè®®ä½¿ç”¨paddle developç‰ˆæœ¬è®­ç»ƒï¼Œéœ€è¦å®‰è£… ``pip install fast_dataindex visualdl==2.5.3`` ç­‰ç›¸å…³ç¼ºå¤±whlåŒ…ã€‚
2. ``use_flash_attention`` éœ€è¦åœ¨A100æœºå™¨å¼€å¯ï¼Œå»ºè®®ä½¿ç”¨cuda11.8ç¯å¢ƒã€‚
3. ``use_fused_rms_norm`` éœ€è¦å®‰è£… `æ­¤ç›®å½• <https://github.com/PaddlePaddle/PaddleNLP/tree/develop/slm/model_zoo/gpt-3/external_ops>`_ ä¸‹çš„è‡ªå®šä¹‰OP, `python setup.py install`ã€‚å¦‚æœå®‰è£…åä»ç„¶æ‰¾ä¸åˆ°ç®—å­ï¼Œéœ€è¦é¢å¤–è®¾ç½® ``PYTHONPATH``ã€‚
4. ``continue_training`` è¡¨ç¤ºä»ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è®­ç»ƒã€‚7bæ¨¡å‹åˆå§‹losså¤§æ¦‚ä¸º2.xx, éšæœºåˆå§‹åŒ–æ¨¡å‹lossä»11.xå·¦å³ä¸‹é™ã€‚
5. å½“å‰è„šæœ¬ä¸ºshardingç‰ˆæœ¬ï¼Œéœ€è¦4Då¹¶è¡Œè®­ç»ƒï¼ˆæ•°æ®ã€shardingã€å¼ é‡ã€æµæ°´çº¿å¹¶è¡Œï¼‰çš„ç”¨æˆ·ï¼Œè¯·å‚è€ƒ ``run_trainer_tp4pp2.sh`` è„šæœ¬ã€‚
6. å¤šæœºè®­ç»ƒæ—¶ï¼Œè‹¥å„æœºå™¨ä½¿ç”¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶ä½ç½®ç›¸åŒï¼ˆä¾‹å¦‚æŒ‚è½½å…±äº«ç¡¬ç›˜æƒ…å†µï¼‰ï¼Œè¯·æŒ‡å®š ``--share_folder true`` ä½¿å…¨å±€0å·å¡åˆ¶ä½œç¼“å­˜æ•°æ®ã€‚å¦åˆ™é»˜è®¤å„å°æœºå™¨çš„0å·å¡ç‹¬ç«‹åˆ¶ä½œç¼“å­˜æ•°æ®ï¼Œ
7. è‹¥æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­å­˜åœ¨é»˜è®¤ç¼“å­˜æ–‡ä»¶å¤¹ ``index-cache/`` ï¼Œåˆ™é¢å¤–æŒ‡å®šçš„ ``--data_cache`` ä¸ç”Ÿæ•ˆï¼Œè®­ç»ƒæ—¶ä¼˜å…ˆåŠ è½½é»˜è®¤ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹ã€‚


é¢„è®­ç»ƒä½¿ç”¨äº†PaddleNLPçš„Traineræ¨¡å—ï¼Œç›¸å…³åˆ†å¸ƒå¼ç­–ç•¥ä½¿ç”¨ï¼Œè¯·å‚è€ƒ `å¤§æ¨¡å‹ Trainer æ··åˆå¹¶è¡Œè®­ç»ƒæ•™ç¨‹ <./llm_trainer.rst>` 


.. _model_capability:
æ¨¡å‹é¢„è®­ç»ƒæ”¯æŒçš„åˆ†å¸ƒå¼èƒ½åŠ›ä¸€è§ˆ
--------------------------------------


.. csv-table:: æ¨¡å‹èƒ½åŠ›æ±‡æ€»
    :header: Model,Data Parallelism,Tensor Parallelism,Pipeline Parallelism,sequence parallelism,Flash Attention,Selective Recompute,Sharding Stage1 + recompute,Sharding Stage1 + DP,Stage2 + recompute,Stage2 + DP,Stage3 + recompute,Stage3 + DP
    :widths: 5 2 2 2 2 2 2 2 2 2 2 2 2

    ``LLaMA-65B``   ,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…
    ``LLaMA2-70B``  ,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…
    ``BaiChuan-13B``,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…
    ``GPT3``        ,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…,âœ…
    ``Qwen-7B``     ,âœ…,âœ…,âœ…,â¬œ,âœ…,âœ…,â¬œ,âœ…,âœ…,âœ…,âœ…,âœ…
    ``Qwen-14B``    ,âœ…,âœ…,âœ…,â¬œ,âœ…,âœ…,â¬œ,âœ…,âœ…,âœ…,âœ…,âœ…
    ``OPT 66B``     ,âœ…,âœ…,â¬œ,â¬œ,âŒ,ğŸš§,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ
    ``Bloom-176B``  ,âœ…,âœ…,â¬œ,â¬œ,âœ…,ğŸš§,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ
    ``ChatGLM-6B``  ,âœ…,âœ…,â¬œ,â¬œ,âœ…,ğŸš§,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ
    ``ChatGLM2``    ,âœ…,âœ…,â¬œ,â¬œ,âŒ,ğŸš§,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ
    ``GLM 130B``    ,âœ…,âœ…,â¬œ,â¬œ,âŒ,ğŸš§,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ,â¬œ


* âœ…: å·²æ”¯æŒï¼ŒSupported
* ğŸš§: éƒ¨åˆ†æ”¯æŒï¼ŒIn Progress
* âŒ: æš‚ä¸æ”¯æŒï¼ŒNot Supported


.. _model_weight:
æ¨¡å‹æƒé‡æ”¯æŒåˆ—è¡¨
-------------------------


ä¸Šè¡¨ä¸­å±•ç¤ºçš„æ˜¯éƒ¨åˆ†æ¨¡å‹æƒé‡ï¼Œæ”¯æŒçš„æ‰€æœ‰æ¨¡å‹å¦‚ä¸‹ï¼š

.. code-block:: text

  * LLaMAç³»åˆ—
    - facebook/llama-7b [è‹±æ–‡]
    - facebook/llama-13b [è‹±æ–‡]
    - facebook/llama-65b [è‹±æ–‡]
    - meta-llama/Llama-2-7b [è‹±æ–‡]
    - meta-llama/Llama-2-7b-chat [è‹±æ–‡]
    - meta-llama/Llama-2-13b [è‹±æ–‡]
    - meta-llama/Llama-2-13b-chat [è‹±æ–‡]
    - meta-llama/Llama-2-70b [è‹±æ–‡]
    - baichuan-inc/Baichuan-7B [ä¸­æ–‡]
    - baichuan-inc/Baichuan-13B-Base [ä¸­æ–‡]
    - baichuan-inc/Baichuan-13B-Chat [ä¸­æ–‡]
    - baichuan-inc/Baichuan2-7B-Base [ä¸­æ–‡]
    - baichuan-inc/Baichuan2-7B-Chat [ä¸­æ–‡]
    - baichuan-inc/Baichuan2-13B-Base [ä¸­æ–‡]
    - baichuan-inc/Baichuan2-13B-Chat [ä¸­æ–‡]
    - FlagAlpha/Llama2-Chinese-7b-Chat [ä¸­æ–‡]
    - FlagAlpha/Llama2-Chinese-13b-Chat [ä¸­æ–‡]
    - idea-ccnl/ziya-llama-13b-v1 [ä¸­æ–‡]
    - linly-ai/chinese-llama-2-7b [ä¸­æ–‡]
    - linly-ai/chinese-llama-2-13b [ä¸­æ–‡]
  * ChatGLMç³»åˆ—
    - THUDM/chatglm-6b-v1.1 [ä¸­æ–‡]
    - THUDM/chatglm2-6b [ä¸­æ–‡]
  * BLOOMç³»åˆ—
    - bigscience/bloom-7b1 [è‹±æ–‡]
    - bigscience/bloomz-7b1 [å¤šè¯­è¨€]
    - bigscience/bloomz-7b1-mt [å¤šè¯­è¨€]
  * Qwenç³»åˆ—
    - qwen/qwen-7b [ä¸­æ–‡]
    - qwen/qwen-7b-chat [ä¸­æ–‡]
    - qwen/qwen-14b [ä¸­æ–‡]
    - qwen/qwen-14b-chat [ä¸­æ–‡]


.. _model_performance:
æ¨¡å‹é¢„è®­ç»ƒæ€§èƒ½
------------------

ä»¥ä¸‹æµ‹è¯•ç»“æœåŸºäº

æœºå™¨ç¯å¢ƒï¼š

- GPU: A100 80G * 8, CUDA 11.8, NCCL 2.15
- CPU: Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz
- å†…å­˜ï¼š1 TB

.. code-block:: text

    paddle commit id              : 9b36e53f24ac5f471b20de99e0cc3980f38b44ab
    paddlenlp commit id           : 0b246a609a3062e3c3256d87193b70277b5b07e0
  

.. csv-table:: æ¨¡å‹æ€§èƒ½æµ‹è¯•æ±‡æ€»
    :header: æ¨¡å‹,åºåˆ—é•¿åº¦,åˆ†å¸ƒå¼ç­–ç•¥,é€Ÿåº¦ [#]_ [#]_,æ˜¾å­˜å ç”¨ [#]_,é…ç½®æ–‡ä»¶,æµ‹è¯•æ—¶é—´
    :widths: 10 2 4 2 2 15 5
   
    ``FlagAlpha/Llama2-Chinese-13b-Chat``,4096,``tp2sd4_stage2``,1980.22,64323MB,``./llama/pretrain-flagalpha_llama2_13b-tp2sd4_stage2.json``,2023-11-27 21:42:38
    ``FlagAlpha/Llama2-Chinese-7b-Chat`` ,4096,``tp2sd4_stage2``,3744.62,52092MB,``./llama/pretrain-flagalpha_llama2_7b-tp2sd4_stage2.json``,2023-11-27 21:44:57
    ``baichuan-inc/Baichuan2-13B-Base``  ,4096,``sd8_stage2``,1354.99,74767MB,``./baichuan/pretrain-baichuan2_13b-sd8_stage2.json``,2023-11-27 21:51:26
    ``baichuan-inc/Baichuan2-7B-Base``   ,4096,``tp2sd4_stage2``,3542.45,58363MB,``./baichuan/pretrain-baichuan2_7b-tp2sd4_stage2.json``,2023-11-27 21:53:58
    ``facebook/llama-13b``               ,4096,``tp2sd4_stage2``,1969.64,64278MB,``./llama/pretrain-llama_13b-tp2sd4_stage2.json``,2023-11-27 21:58:03
    ``facebook/llama-7b``                ,4096,``tp2sd4_stage2``,3754.73,52092MB,``./llama/pretrain-llama_7b-tp2sd4_stage2.json``,2023-11-27 22:00:30
    ``idea-ccnl/ziya-llama-13b-v1``      ,4096,``tp2sd4_stage2``,1968.34,63983MB,``./llama/pretrain-ziya_llama_13b-tp2sd4_stage2.json``,2023-11-27 22:04:35
    ``linly-ai/chinese-llama-2-7b``      ,4096,``tp2sd4_stage2``,3732.9,51751MB,``./llama/pretrain-linly_llama2_7b-tp2sd4_stage2.json``,2023-11-27 22:06:58
    ``meta-llama/Llama-2-13b``           ,4096,``tp2sd4_stage2``,1975.63,64294MB,``./llama/pretrain-llama2_13b-tp2sd4_stage2.json``,2023-11-27 22:11:04
    ``meta-llama/Llama-2-7b``            ,4096,``tp2sd4_stage2``,3755.21,52092MB,``./llama/pretrain-llama2_7b-tp2sd4_stage2.json``,2023-11-27 22:13:34
    ``qwen/qwen-7b``                     ,4096,``tp2sd4_stage2``,3607.28,65448MB,``./qwen/pretrain-qwen_7b-tp2sd4_stage2.json``,2023-11-27 22:16:04


..  [#] é€Ÿåº¦çš„å•ä½æ˜¯``tokens/card/sec``ï¼Œæ¯å¼ å¡æ¯ç§’éœ€è®­ç»ƒçš„tokenæ•°ã€‚
..  [#] é€Ÿåº¦ä¼šæœ‰å°å¹…æ³¢åŠ¨ï¼Œä¾‹å¦‚ ``facebook/llama-7b`` å’Œ ``meta-llama/Llama-2-7b`` æ˜¯ç›¸åŒè®­ç»ƒé…ç½®ã€‚
..  [#] æ˜¾å­˜å ç”¨å•ä½æ˜¯MBï¼Œä½¿ç”¨çš„æ˜¯ ``max_memory_allocated`` è·å–æ˜¾å­˜, å®é™…ç‰©ç†æ˜¾å­˜ä¼šå ç”¨æ›´å¤šï¼Œå¤§çº¦å¤š2-3GB.
