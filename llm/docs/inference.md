# å¤§æ¨¡å‹æ¨ç†æ•™ç¨‹

PaddleNLPé™¤äº†æä¾›å¸¸ç”¨æ¨¡å‹æ¨ç†å¤–ï¼Œè¿˜æä¾›äº†é«˜æ€§èƒ½æ¨ç†ï¼Œå†…ç½®åŠ¨æ€æ’å…¥å’Œå…¨ç¯èŠ‚ç®—å­èåˆç­–ç•¥ï¼Œæå¤§åŠ å¿«å¹¶è¡Œæ¨ç†çš„é€Ÿåº¦ã€‚

git clone ä»£ç åˆ°æœ¬åœ°ï¼Œå³å¯å¼€å§‹ã€‚

```bash
    git clone https://github.com/PaddlePaddle/PaddleNLP.git
    # pip install ./PaddleNLP ä½¿ç”¨developç‰ˆæœ¬
    cd PaddleNLP/llm
    # åˆ°è¾¾è¿è¡Œç›®å½•
```

## 1. å¸¸ç”¨æ¨¡å‹æ¨ç†
PaddleNLP æä¾›äº†åŠ¨æ€å›¾æ¨ç†å’Œé™æ€å›¾æ¨ç†ä¸¤ç§æ–¹å¼ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€ŸéªŒè¯æ¨¡å‹æ¨ç†æ•ˆæœï¼ˆåŒ…å«LoRAã€PrefixTuningï¼‰

### 1.1 åŠ¨æ€å›¾æ¨ç†
```shell
# åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --data_file ./data/dev.json --dtype float16
```
å¯¹äºLoRAã€PrefixTuning æ¨¡å‹åªéœ€é¢å¤–ä¼ å…¥ç›¸åº”çš„lora_pathæˆ–prefix_pathå³å¯ï¼Œå¦‚ï¼š`--lora_path ./checkpoints/llama_lora_ckpts`æˆ–`--prefix_path ./checkpoints/llama_prefix_ckpts`ï¼Œè¯¦è§æ¨ç†å‚æ•°å‡å°‘ã€‚

### 1.2 é™æ€å›¾æ¨ç†

```shell
# é™æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒï¼Œ LoRAéœ€è¦å…ˆåˆå¹¶å‚æ•°ï¼ŒPrefix Tuningæš‚ä¸æ”¯æŒ
# step1 : é™æ€å›¾å¯¼å‡º
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --output_path ./inference --dtype float16
# step2: é™æ€å›¾æ¨ç†
python ./predict/predictor.py --model_name_or_path ./inference --data_file ./data/dev.json --dtype float16 --mode static
```

## 2. é«˜æ€§èƒ½æ¨¡å‹æ¨ç†

é«˜æ€§èƒ½æ¨ç†å†…ç½®åŠ¨æ€æ’å…¥å’Œå…¨ç¯èŠ‚ç®—å­èåˆç­–ç•¥ï¼Œéšè—äº†åº•å±‚å®ç°çš„ç»†èŠ‚ï¼Œå®ç°äº†å¼€ç®±å³ç”¨é«˜æ€§èƒ½å¹¶è¡Œæ¨ç†èƒ½åŠ›ã€‚
<div align="center">
    <img width="800" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/63761690/42174b47-f765-48d6-9fef-907b69bf6706">
</div>
<div align="center">
    <font size ="1">
    é£æ¡¨é«˜æ€§èƒ½æ¨ç†ç®—å­èåˆç¤ºæ„å›¾
     </font>
</div>

<div align="center">
    <img width="800" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/63761690/616b3fc5-b9b2-4b10-a5c8-2f892a65ae6b">
</div>
<div align="center">
    <font size ="1">
    åŠ¨æ€æ’å…¥å›¾è§£ & é£æ¡¨é«˜æ€§èƒ½æ¨¡å‹æ¨ç†æ€§èƒ½å›¾
     </font>
</div>

PaddleNLP ä¸­å·²ç»æ·»åŠ é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ç›¸å…³å®ç°ï¼Œæ”¯æŒï¼š

| Model                       | Inference Model | PTuning | WINT8 | PTQ-A8W8 |
|-----------------------------|-----------------|---------|-------|-----|
| [LLaMA1/2](../llama)         | âœ…               | âœ…       | âœ…     | âœ…   |
| [ChatGLM](../chatglm)        | âœ…               | âœ…       | âœ…     | âŒ   |
| [ChatGLM2](../chatglm2)      | âœ…               | âŒ       | âŒ     | âŒ   |
| [Bloom](../bloom)            | âœ…               | âœ…       | âœ…     | âŒ   |
| [GPT-3](../gpt-3)            | âœ…               | âŒ       | âŒ     | âŒ   |
| [Qwen](../qwen)              | âœ…               | âŒ       | âŒ     | âŒ   |
| [BaiChuan-7B](../llama)     | âœ…               | âœ…       | âœ…     | ğŸš§   |
| [BaiChuan2-7B](../llama)     | âœ…               | âœ…       | âœ…     | ğŸš§   |
| [BaiChuan2-13B](../llama) | ğŸš§               | ğŸš§       | ğŸš§     | ğŸš§   |

* âœ…: Supported
* ğŸš§: In Progress
* âŒ: Not Supported
* WINT8:æŒ‡Weight-Only Quantization INT8ï¼Œå³å¯¹æƒé‡è¿›è¡ŒINT8é‡åŒ–çš„æ¨¡å‹ã€‚
* PTQ-A8W8:æŒ‡ä½¿ç”¨PTQå¯¹çº¿æ€§å±‚çš„æ¿€æ´»å’Œæƒé‡éƒ½é‡åŒ–ä¸ºINT8çš„æ¨¡å‹ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨ç†çš„ååï¼Œæˆ‘ä»¬åŸºäºPageAttentionçš„æ€æƒ³è®¾è®¡å¹¶å®ç°äº†BlockAttentionï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½æ¨ç†å’ŒåŠ¨æ€æ’å…¥çš„åŸºç¡€ä¸Šå¯ä»¥åŠ¨æ€åœ°ä¸ºcachekvåˆ†é…å­˜å‚¨ç©ºé—´ï¼Œæå¤§åœ°èŠ‚çœæ˜¾å­˜ï¼Œä»è€Œåœ¨åŒä¸€æ—¶åˆ»å¤„ç†æ›´å¤šçš„queryä»¥è·å¾—ååçš„æå‡ã€‚ä¸‹é¢åˆ†åˆ«ç»™å‡ºå…³é—­BlockAttentionå’Œæ‰“å¼€BlockAttentionè¿›è¡Œé«˜æ€§èƒ½æ¨ç†çš„å‘½ä»¤å‚è€ƒã€‚

### 2.2 ç¯å¢ƒå‡†å¤‡

- PaddleNLP develop
- PaddlePaddle develop

PaddleNLP é’ˆå¯¹äºTransformer ç³»åˆ—ç¼–å†™äº†é«˜æ€§èƒ½è‡ªå®šä¹‰ç®—å­ï¼Œæå‡æ¨¡å‹åœ¨æ¨ç†å’Œè§£ç è¿‡ç¨‹ä¸­çš„æ€§èƒ½ï¼Œä½¿ç”¨ä¹‹å‰éœ€è¦é¢„å…ˆå®‰è£…è‡ªå®šä¹‰ç®—å­åº“ï¼š

```shell
git clone https://github.com/PaddlePaddle/PaddleNLP
#GPUè®¾å¤‡å®‰è£…è‡ªå®šä¹‰ç®—å­
cd ./paddlenlp/csrc && python setup_cuda.py install
#XPUè®¾å¤‡å®‰è£…è‡ªå®šä¹‰ç®—å­
cd ./paddlenlp/csrc/xpu/src && sh cmake_build.sh
```

### 2.3 å…³é—­BlockAttentionçš„é«˜æ€§èƒ½æ¨ç†

#### 2.3.1 åŠ¨æ€å›¾æ¨ç†

```shell
# åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16

# PrefixTuningåŠ¨æ€å›¾æ¨ç†å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --export_precache true --prefix_path ./checkpoints/llama_prefix_ckpts

# Weight Only Int8 åŠ¨æ€å›¾æ¨ç†å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --quant_type weight_only_int8

# PTQ-A8W8æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --dtype float16
```
**Note**ï¼š
1. LoRA æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰æ˜¯éœ€è¦åˆå¹¶å‚æ•°ï¼Œè¯¦ç»†å¯è§ï¼š[åˆå¹¶ LoRA å‚æ•°](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/merge_lora_params.py)ã€‚
2. PrefixTuningæ¨ç†éœ€è¦ä¼ å…¥ç›¸åº”çš„pre_cacheï¼Œéœ€è¦é¢å¤–è®¾ç½®`export_precache`ä¸º`true`ï¼Œå¹¶ä¸”ä¼ å…¥å¯¹åº”çš„PrefixTuningå‚æ•°ä¿å­˜è·¯å¾„`prefix_path`ã€‚
3. ä½¿ç”¨Weight Only Int8 æ¨ç†éœ€è¦é¢å¤–ä¼ å…¥ `quant_type`ã€‚

#### 2.3.2 é™æ€å›¾æ¨ç†
**step1ï¼šåŠ¨è½¬é™**
```shell
# åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16

# PrefixTuningåŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --export_precache true

# Weight Only Int8 åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --quant_type weight_only_int8

# PTQ-A8W8åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --output_path ./inference --dtype float16
```
**Note**ï¼š
1. LoRA æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰æ˜¯éœ€è¦åˆå¹¶å‚æ•°ï¼Œè¯¦ç»†å¯è§ï¼š[åˆå¹¶ LoRA å‚æ•°](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/merge_lora_params.py)ã€‚
2. PrefixTuningæ¨ç†éœ€è¦ä¼ å…¥ç›¸åº”çš„pre_cacheï¼Œéœ€è¦é¢å¤–è®¾ç½®`export_precache`ä¸º`true`ã€‚
3. ä½¿ç”¨Weight Only Int8 æ¨ç†éœ€è¦é¢å¤–ä¼ å…¥ `quant_type`ã€‚
4. A8W8æ¨ç†ä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚

**step2ï¼šé™æ€å›¾æ¨ç†**
```shell
# é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --quant_type weight_only_int8 --dtype "float16" --mode "static"

# PrefixTuningé™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --quant_type weight_only_int8 --dtype "float16" --mode "static" --export_precache true --prefix_path ./checkpoints/llama_prefix_ckpts

# Weight Only Int8 é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --quant_type weight_only_int8 --dtype "float16" --mode "static" --quant_type weight_only_int8

# PTQ-A8W8é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
# ä»¥ä¸‹ç¯å¢ƒå˜é‡ç”¨äºå¼€å¯int8çŸ©é˜µä¹˜çš„ç®—æ³•é€‰æ‹©ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæ‰“å¼€ä¹‹åç¬¬ä¸€æ¬¡æ‰§è¡Œä¼šæ‰§è¡Œç®—æ³•é€‰æ‹©ä»è€Œå¯¼è‡´é€Ÿåº¦è¾ƒæ…¢ã€‚
export FLAGS_use_autotune=1
export FLAGS_cublaslt_exhaustive_search_times=10
export FLAGS_cache_inference_while_scope=1

python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --quant_type weight_only_int8 --dtype "float16" --mode "static"
```
**Note**ï¼š
1. LoRA æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰æ˜¯éœ€è¦åˆå¹¶å‚æ•°ï¼Œè¯¦ç»†å¯è§ï¼š[åˆå¹¶ LoRA å‚æ•°](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/llm/merge_lora_params.py)ã€‚
2. PrefixTuningæ¨ç†éœ€è¦ä¼ å…¥ç›¸åº”çš„pre_cacheï¼Œéœ€è¦é¢å¤–è®¾ç½®`export_precache`ä¸º`true`ï¼Œå¹¶ä¸”ä¼ å…¥å¯¹åº”çš„PrefixTuningå‚æ•°ä¿å­˜è·¯å¾„`prefix_path`ã€‚
3. ä½¿ç”¨Weight Only Int8 æ¨ç†éœ€è¦é¢å¤–ä¼ å…¥ `quant_type`ã€‚
4. A8W8æ¨ç†ä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚


### 2.4 æ‰“å¼€BlockAttentionçš„é«˜æ€§èƒ½æ¨ç†

#### 2.4.1 åŠ¨æ€å›¾æ¨ç†

```shell
# åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn

# XPUè®¾å¤‡åŠ¨æ€å›¾æ¨¡å‹æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn --device xpu

# Weight Only Int8 åŠ¨æ€å›¾æ¨ç†å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --quant_type weight_only_int8 --block_attn

# PTQ-A8W8æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --dtype float16 --block_attn

# CacheKV åŠ¨æ€é‡åŒ–æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --dtype float16 --block_attn --cachekv_int8
```

#### 2.4.2 é™æ€å›¾æ¨ç†
**step1ï¼šåŠ¨è½¬é™**
```shell
# åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --block_attn

# XPUè®¾å¤‡åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --block_attn --device xpu

# Weight Only Int8 åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --quant_type weight_only_int8 --block_attn

# PTQ-A8W8åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py --model_name_or_path checkpoints/llama_ptq_ckpts --inference_model --output_path ./inference --dtype float16 --block_attn

# CacheKV åŠ¨æ€é‡åŒ–åŠ¨è½¬é™å‘½ä»¤å‚è€ƒ
python ./predict/export_model.py  --model_name_or_path meta-llama/Llama-2-7b-chat --inference_model --output_path ./inference --dtype float16 --block_attn --cachekv_int8
```

**step2ï¼šé™æ€å›¾æ¨ç†**
```shell
# é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --dtype "float16" --mode "static" --block_attn

# XPUè®¾å¤‡é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --dtype "float16" --mode "static" --block_attn --device xpu

# Weight Only Int8 é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --dtype "float16" --mode "static" --quant_type weight_only_int8 --block_attn

# PTQ-A8W8é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
# ä»¥ä¸‹ç¯å¢ƒå˜é‡ç”¨äºå¼€å¯int8çŸ©é˜µä¹˜çš„ç®—æ³•é€‰æ‹©ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæ‰“å¼€ä¹‹åç¬¬ä¸€æ¬¡æ‰§è¡Œä¼šæ‰§è¡Œç®—æ³•é€‰æ‹©ä»è€Œå¯¼è‡´é€Ÿåº¦è¾ƒæ…¢ã€‚
export FLAGS_use_autotune=1
export FLAGS_cublaslt_exhaustive_search_times=10
export FLAGS_cache_inference_while_scope=1

python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --dtype "float16" --mode "static" --block_attn

# CacheKV åŠ¨æ€é‡åŒ–8é™æ€å›¾æ¨ç†å‘½ä»¤å‚è€ƒ
python ./predict/predictor.py  --model_name_or_path ./inference --inference_model --dtype "float16" --mode "static" --cachekv_int8 --block_attn
```
**Note**ï¼š
1. ä½¿ç”¨Weight Only Int8 æ¨ç†éœ€è¦é¢å¤–ä¼ å…¥ `quant_type`ã€‚
2. A8W8æ¨ç†ä¼ å…¥çš„ `model_name_or_path` ä¸ºPTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ã€‚


## 3. æ¨ç†å‚æ•°ä»‹ç»

- `model_name_or_path`: å¿…é¡»ï¼Œé¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è€…æœ¬åœ°çš„æ¨¡å‹è·¯å¾„ï¼Œç”¨äºçƒ­å¯æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `batch_size`: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º8ã€‚è¯¥å‚æ•°è¶Šå¤§ï¼Œå ç”¨æ˜¾å­˜è¶Šé«˜ï¼›è¯¥å‚æ•°è¶Šå°ï¼Œå ç”¨æ˜¾å­˜è¶Šä½ã€‚
- `src_length`: æ¨¡å‹è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§tokené•¿åº¦ï¼Œé»˜è®¤ä¸º1024ã€‚
- `max_length`:æ¨¡å‹è¾“å…¥ï¼ˆä¸Šä¸‹æ–‡+ç”Ÿæˆå†…å®¹ï¼‰çš„æœ€å¤§tokené•¿åº¦, é»˜è®¤ä¸º2048ã€‚
- `lora_path`: LoRAå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹LoRAå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `prefix_path`: Prefix Tuningå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹Prefix Tuningå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `top_k`: â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¸º top-k è¿‡æ»¤ä¿ç•™çš„æœ€é«˜æ¦‚ç‡æ ‡è®°çš„æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œç­‰ä»·äºè´ªå¿ƒç­–ç•¥ã€‚
- `top_p`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ top-p è¿‡æ»¤çš„ç´¯ç§¯æ¦‚ç‡ã€‚é»˜è®¤ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚
- `temperature`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¼šå¯¹è¾“å‡ºlogité™¤ä»¥temperatureã€‚é»˜è®¤ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚
- `data_file`:å¿…é¡»ï¼Œå¾…æ¨ç†jsonæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `output_file`:ä¿å­˜æ¨ç†ç»“æœæ–‡ä»¶åï¼Œé»˜è®¤ä¸ºoutput.jsonã€‚
- `device`: è¿è¡Œç¯å¢ƒï¼Œé»˜è®¤ä¸ºgpuã€‚
- `dtype`: æ¨¡å‹å‚æ•°dtypeï¼Œé»˜è®¤ä¸ºNoneã€‚å¦‚æœæ²¡æœ‰ä¼ å…¥`lora_path`ã€`prefix_path`åˆ™å¿…é¡»ä¼ å…¥
- `model_type`: åˆå§‹åŒ–ä¸åŒç±»å‹æ¨¡å‹ï¼Œgpt-3: GPTForCausalLM; ernie-3.5-se: Ernie35ForCausalLM; é»˜è®¤ä¸º Noneã€‚
- `mode`: ä½¿ç”¨åŠ¨æ€å›¾æˆ–è€…é™æ€å›¾æ¨ç†ï¼Œå€¼ä¸ºï¼š[dynamic, static]ï¼Œé»˜è®¤ä¸º dynamicã€‚
- `inference_model`: æ˜¯å¦ä½¿ç”¨Inference Model æ¨ç†ï¼Œé»˜è®¤å€¼ä¸º Falseã€‚
- `block_attn`: æ˜¯å¦ä½¿ç”¨Block Attention æ¨ç†ï¼Œ é»˜è®¤å€¼ä¸ºFalseã€‚
- `block_size`: å¦‚æœä½¿ç”¨Block Attention æ¨ç†ï¼ŒæŒ‡å®šä¸€ä¸ªBlockå¯ä»¥å­˜å‚¨çš„tokenæ•°é‡ï¼Œé»˜è®¤å€¼ä¸º64ã€‚
- `cachekv_int8`: æ˜¯å¦ä½¿ç”¨cachekv int8é‡åŒ–ç”¨äºèŠ‚çœæ˜¾å­˜ï¼Œé»˜è®¤å€¼ä¸ºFalseã€‚
