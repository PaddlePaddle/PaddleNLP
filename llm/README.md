#  é£æ¡¨å¤§æ¨¡å‹å…¨æµç¨‹å·¥å…·é“¾

é£æ¡¨å¤§æ¨¡å‹å¥—ä»¶ç§‰æ‰¿äº†ä¸€ç«™å¼ä½“éªŒã€æ€§èƒ½æè‡´ã€ç”Ÿæ€å…¼å®¹çš„è®¾è®¡ç†å¿µï¼Œæ—¨åœ¨æä¾›ä¸šç•Œä¸»æµå¤§æ¨¡å‹é¢„è®­ç»ƒã€ç²¾è°ƒï¼ˆå«SFTã€PEFTï¼‰ã€é‡åŒ–ã€æ¨ç†ç­‰å…¨æµç¨‹ç»Ÿä¸€å·¥å…·é“¾ï¼Œ å¸®åŠ©å¼€å‘è€…ä½æˆæœ¬ã€ä½é—¨æ§›ã€å¿«é€Ÿå®ç°å¤§è¯­è¨€æ¨¡å‹å®šåˆ¶åŒ–ã€‚

<div align="center">
    <img width="800" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/63761690/17710a9a-d972-4772-9bf4-19ff938b5fe9">
</div>


##  ğŸš£â€â™‚ï¸ é£æ¡¨å¤§æ¨¡å‹å·¥å…·é“¾ç‰¹æ€§ ğŸš£â€â™‚ï¸

-  **é£æ¡¨4Då¹¶è¡Œåˆ†å¸ƒå¼ç­–ç•¥**ã€‚ PaddleNLP Trainer å°è£…æ”¯æŒäº†é£æ¡¨4Då¹¶è¡Œé…ç½®ï¼ˆæ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€Sharding å¹¶è¡Œï¼‰ï¼Œå±è”½å¤šç¡¬ä»¶ç¼–ç¨‹å¤æ‚æ€§ï¼Œç”¨æˆ·å¯ä»¥ä¿®æ”¹Traineré…ç½®ç»„åˆå¤šç§é¢„è®­ç»ƒæˆ–ç²¾è°ƒè¿‡ç¨‹çš„åˆ†å¸ƒå¼ç­–ç•¥ï¼Œè·å¾—æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è®­ç»ƒä½“éªŒã€‚

-  **é«˜æ•ˆç²¾è°ƒç­–ç•¥**ã€‚é£æ¡¨å¤§æ¨¡å‹å¥—ä»¶æä¾›SFTã€PEFTç­‰å¤šç§ç²¾è°ƒç­–ç•¥ï¼Œæ­è½½è‡ªç ”Intokensç­–ç•¥æœ‰æ•ˆå‡å°‘äº†pad tokençš„å æ¯”ï¼Œæé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚ç‹¬åˆ›PEFTç»“åˆä½æ¯”ç‰¹å’Œåˆ†å¸ƒå¼å¹¶è¡Œç­–ç•¥ï¼Œå¤§å¹…é™ä½å¤§æ¨¡å‹ç²¾è°ƒç¡¬ä»¶é—¨æ§›ã€‚


- **å¤§æ¨¡å‹æ— æŸé‡åŒ–**ã€‚å·¥å…·é“¾å†…ç½®äº†PaddleSlim å›¢é˜Ÿè‡ªç ”çš„è‡ªé€‚åº”Shift-SmoothQuantçš„A8W8é‡åŒ–ç®—æ³•å’Œä¸šç•Œä¸»æµGPTQçš„W4é‡åŒ–ç®—æ³•ï¼Œå®ç°äº†ä¸»æµå¤§æ¨¡å‹çš„æ— æŸé‡åŒ–ï¼Œæœ‰æ•ˆåŠ é€Ÿæ¨¡å‹æ¨ç†ã€‚


- **é«˜æ€§èƒ½æ¨ç†**ã€‚å·¥å…·é“¾é«˜æ€§èƒ½æ¨ç†æ¨¡å—å†…ç½®åŠ¨æ€æ’å…¥å’Œå…¨ç¯èŠ‚ç®—å­èåˆç­–ç•¥ï¼Œæå¤§åŠ å¿«å¹¶è¡Œæ¨ç†çš„é€Ÿåº¦ã€‚åŒæ—¶éšè—äº†åº•å±‚å®ç°çš„ç»†èŠ‚ï¼Œå®ç°é«˜æ€§èƒ½æ¨ç†å¼€ç®±å³ç”¨ã€‚


##  ğŸ§¨ æ”¯æŒæ¨¡å‹åˆ—è¡¨ ğŸ§¨

| Model | Pretrain | SFT | LoRA | Prefix Tuning |  Quantization | weight convert |
| --- | --- | --- | --- | --- | --- |  --- |
| [LLaMA/LLaMA2](./llama) | âœ…  | âœ… | âœ… | âœ… | âœ…  | âœ…  |
| [Baichuan/Baichuan2](./llama) | âœ…  | âœ… | âœ… | âœ… | âœ…  | âœ…  |
| [ChatGLM-6B](./chatglm) |  âŒ  |  âœ…  |    âœ…  |  âœ…  |  âœ…  | âŒ  |
| [ChatGLM2/ChatGLM3](./chatglm2) |  âŒ  |    âœ…  |  âœ…  |  âœ…  |  âœ…  | âœ…  |
| [Qwen](./qwen) | âœ… | âœ… | âœ… | âœ… |  ğŸš§ | âœ…  |j
| [Bloom](./bloom) | âŒ  | âœ… | âœ… |  âœ… | âœ… | âœ…  |
| [GPT-3](./gpt-3) |   âœ…  |  âœ…  |    ğŸš§  | ğŸš§  | ğŸš§ | âœ…  |
| [OPT](./opt) | ğŸš§ | âœ… | âœ… | ğŸš§ |  ğŸš§ | âœ…  |
| [GLM](./glm) | âŒ  | âœ… | âœ… | ğŸš§ |   ğŸš§ | âœ…  |

* âœ…: Supported
* ğŸš§: In Progress
* âŒ: Not Supported


##  ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€



### 1. é¢„è®­ç»ƒ
PaddleNLPå°†é£æ¡¨4Då¹¶è¡Œç­–ç•¥åŠ å…¥åˆ°Trainer APIä¸­ï¼Œ ç”¨æˆ·åªéœ€ä¿®æ”¹Traineré…ç½®å³å¯ä½¿ç”¨ä¸åŒçš„åˆ†å¸ƒå¼ç­–ç•¥ã€‚ç›®å‰å·¥å…·é“¾æä¾›[LLaMA/LLaMA2](./llama)ã€[GPT-3](./gpt-3)ã€[Qwen](./qwen) ç­‰æ¨¡å‹é¢„è®­ç»ƒåŠŸèƒ½ï¼Œæ›´å¤šæ¨¡å‹æ”¯æŒæŒç»­æ›´æ–°ä¸­ã€‚

<div align="center">
    <img width="500" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/37530985/a2f0261d-7f76-4faf-ae01-cc9d37d5fcc0">
</div>
<div align="center">
    <font size ="1">
    é£æ¡¨ä¸ Megatron é¢„è®­ç»ƒæ€§èƒ½æ¯”å¯¹
     </font>
</div>

å•æœº8å¡qwené¢„è®­ç»ƒå¯åŠ¨å‘½ä»¤å‚è€ƒï¼š
```
python -u  -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" run_pretrain.py ./qwen/pretrain_argument_stage2.json
```
æ›´å¤šæ¨¡å‹é¢„è®­ç»ƒæ•°æ®å‡†å¤‡ã€ç¯å¢ƒå‡†å¤‡ã€å‚æ•°é…ç½®ç»†èŠ‚è¯·å‚è€ƒ[é¢„è®­ç»ƒæ–‡æ¡£](./docs/pretrain.md)ã€‚


### 2. ç²¾è°ƒ
ç›®å‰ç²¾è°ƒç»Ÿä¸€è„šæœ¬åªå·²æ”¯æŒå¤§éƒ¨åˆ†ä¸»æµæ¨¡å‹ï¼Œè¯¦è§å¯¹åº”æ¨¡å‹ç›®å½•ã€‚æ›´å¤šLoRAã€Prefix Tuningè¯·å‚è§[ç²¾è°ƒæ–‡æ¡£](./docs/finetune.md)ã€‚é™¤æ­¤ä»¥å¤–è¿˜æ”¯æŒäº†é«˜æ•ˆå¤šè½®å¯¹è¯æ¨¡å¼ç²¾è°ƒï¼Œå…·ä½“çš„é…ç½®å¯çœ‹[å¤šè½®å¯¹è¯æ–‡æ¡£](./docs/chat_template.md)

<div align="center">
    <img width="500" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/63761690/b2b4db4f-0cf3-4d28-989c-e3c00d24f397">
</div>
<div align="center">
    <font size ="1">
    é£æ¡¨ä¸ Huggingface Transformers å¾®è°ƒæ€§èƒ½æ¯”å¯¹
     </font>
</div>

#### 2.1. ç¯å¢ƒå‡†å¤‡
- paddlepaddle-gpu >= 2.5.1
- paddlenlp >= 2.6.1
- tiktoken (ä»… Qwen éœ€è¦)

#### 2.2. ç²¾è°ƒè®­ç»ƒæ•°æ®æ ¼å¼

ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·æµ‹è¯•ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›ç¤ºä¾‹æ•°æ®é›†[å¹¿å‘Šç”Ÿæˆæ•°æ®é›†](https://bj.bcebos.com/paddlenlp/datasets/examples/AdvertiseGen.tar.gz)ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥ä»¿ç…§æ•°æ®é›†çš„æ ¼å¼åˆ¶ä½œè‡ªå·±çš„æ•°æ®é›†è¿›è¡Œç²¾è°ƒã€‚æˆ‘ä»¬æ”¯æŒçš„æ•°æ®æ ¼å¼æ˜¯æ¯è¡ŒåŒ…å«ä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

- `src` : `str, List(str)`, æ¨¡å‹çš„è¾“å…¥æŒ‡ä»¤ï¼ˆinstructionï¼‰ã€æç¤ºï¼ˆpromptï¼‰ï¼Œæ¨¡å‹åº”è¯¥æ‰§è¡Œçš„ä»»åŠ¡ã€‚
- `tgt` : `str, List(str)`, æ¨¡å‹çš„è¾“å‡ºã€‚

æ ·ä¾‹æ•°æ®ï¼š
```
{"src": "ç±»å‹#è£™*é¢œè‰²#è“è‰²*é£æ ¼#æ¸…æ–°*å›¾æ¡ˆ#è´è¶ç»“", "tgt": "è£™èº«å¤„é‡‡ç”¨ç«‹ä½“è´è¶ç»“è£…é¥°è¾…ä»¥è“è‰²æ¡å¸¦ç‚¹ç¼€ï¼Œä»¤è¡£èº«é€ å‹é¥±æ»¡å¯Œæœ‰å±‚æ¬¡çš„åŒæ—¶ä¸ºå…¶æ³¨å…¥ä¸€ä¸ç”œç¾æ°”æ¯ã€‚å°†å¥³å­©æ¸…æ–°å¨‡ä¿çš„ä¸€é¢è¡¬æ‰˜è€Œå‡ºã€‚"}
...
```



#### 2.3. SFT
```bash
# å¼ é‡å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¸¸ç”¨ï¼‰
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" finetune_generation.py ./llama/sft_argument.json

# ChatGLM2ã€OPTä¸æ”¯æŒå¼ é‡å¹¶è¡Œï¼Œé»˜è®¤ä½¿ç”¨Shardingç­–ç•¥ï¼ˆPaddle 2.5.1æ”¯æŒSharding Stage2ï¼ŒSharding Stage3éœ€è¦ä½¿ç”¨Paddle developç‰ˆæœ¬ï¼‰
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" finetune_generation.py ./chatglm2/sft_argument.json

# å¼ é‡å¹¶è¡Œ&æµæ°´çº¿å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
python -u  -m paddle.distributed.launch --gpus "0,1,2,3" finetune_generation.py ./llama/sft_pp_argument.json
```

#### 2.4. LoRA
```bash
# å•å¡LoRAè®­ç»ƒ
python  finetune_generation.py ./llama/lora_argument.json

# å¼ é‡å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
# åªéœ€å°†lora_argument.jsonä¸­tensor_parallel_degreeä¿®æ”¹ä¸º2
# å¹¶ç”¨ -m paddle.distributed.launch --gpus "0,1"æŒ‡å®šä¸€ä¸‹å¡æ•°
python  -u  -m paddle.distributed.launch --gpus "0,1"  finetune_generation.py ./llama/lora_argument.json
```

#### 2.5. Prefix Tuning
```bash
# å•å¡è®­ç»ƒ
python  finetune_generation.py ./llama/pt_argument.json

# å¼ é‡å¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
# åªéœ€å°†pt_argument.jsonä¸­tensor_parallel_degreeä¿®æ”¹ä¸º2
# å¹¶ç”¨ -m paddle.distributed.launch --gpus "0,1"æŒ‡å®šä¸€ä¸‹å¡æ•°
python  -u  -m paddle.distributed.launch --gpus "0,1"  finetune_generation.py ./llama/pt_argument.json
```

### 3. é‡åŒ–
å¤§æ¨¡å‹é‡åŒ–å°†16ä½ã€32ä½æµ®ç‚¹æ•°çš„æ¨¡å‹å‚æ•°æˆ–æ¿€æ´»é‡åŒ–ä¸º4ä½æˆ–8ä½æ•´æ•°èƒ½å¤Ÿæœ‰æ•ˆé™ä½æ¨¡å‹å­˜å‚¨ç©ºé—´å’Œè®¡ç®—èµ„æºéœ€æ±‚ï¼ŒåŒæ—¶åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚å·¥å…·é“¾é‡åŒ–ç®—æ³•åŒ…å«ï¼š
- **PTQ**ã€‚PaddleSlim å›¢é˜Ÿè‡ªç ”çš„è‡ªé€‚åº”Shift-SmoothQuanté‡åŒ–ç®—æ³•ï¼Œåœ¨[SmoothQuant](https://arxiv.org/abs/2211.10438)å’Œ[Outlier Suppression+](https://arxiv.org/abs/2304.09145)åŸºç¡€ä¸Š
æ–°å¢PieceWiseSearchå‚æ•°æœç´¢ç®—æ³•ï¼Œå¯¹æ¨¡å‹æƒé‡å’Œæ¿€æ´»åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œå‡å°‘åç»­A8W8 PTQé‡åŒ–æŸå¤±ã€‚


- **GPTQ**ã€‚[GPTQ](https://arxiv.org/abs/2210.17323)æ˜¯ä¸šç•Œä¸»æµçš„æƒé‡é‡åŒ–ç®—æ³•ï¼Œå¯ä»¥å°†å¤§æ¨¡å‹æƒé‡è¿›è¡Œ4ä½æ•´æ•°æ— æŸé‡åŒ–ï¼Œæé«˜æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚

<div align="center">
    <img width="500" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/37530985/969b62db-9692-4d50-b91a-85cff305d153">
</div>
<div align="center">
    <font size ="1">
    é£æ¡¨é‡åŒ–ç®—æ³•æ•ˆæœå±•ç¤º
     </font>
</div>


```
# PTQ é‡åŒ–å¯åŠ¨å‘½ä»¤å‚è€ƒ
python  finetune_generation.py ./llama/ptq_argument.json

# GPTQ é‡åŒ–å¯åŠ¨å‘½ä»¤å‚è€ƒ
python  finetune_generation.py ./llama/ptq_argument.json
```

æ›´å¤šæŠ€æœ¯ç»†èŠ‚å’Œæ¨¡å‹é‡åŒ–ä½¿ç”¨è¯¦è§[é‡åŒ–æ–‡æ¡£](./docs/quantization.md)ã€‚


### 4. æ¨¡å‹æ¨ç†

æ­¤å¤– PaddleNLP è¿˜æä¾›äº†é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ï¼Œä»è€ŒåŠ é€Ÿ LLM æ¨¡å‹çš„éƒ¨ç½²è½åœ°ï¼Œè¯¦ç»†æ–‡æ¡£è¯·çœ‹ï¼š[Inference Model](./docs/inference.md)

#### 4.1 åŠ¨æ€å›¾æ¨ç†

```shell
# é¢„è®­ç»ƒ&SFTåŠ¨æ€å›¾æ¨¡å‹æ¨ç†
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --data_file ./data/dev.json \
    --dtype float16

# LoRAåŠ¨æ€å›¾æ¨¡å‹æ¨ç†
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --lora_path ./checkpoints/llama_lora_ckpts

# Prefix TuningåŠ¨æ€å›¾æ¨¡å‹æ¨ç†
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --data_file ./data/dev.json \
    --prefix_path ./checkpoints/llama_pt_ckpts
```

#### 4.2 é™æ€å›¾æ¨ç†

```shell
# é¦–å…ˆéœ€è¦è¿è¡Œä¸€ä¸‹å‘½ä»¤å°†åŠ¨æ€å›¾å¯¼å‡ºä¸ºé™æ€å›¾
# LoRAéœ€è¦å…ˆåˆå¹¶å‚æ•°ï¼Œè¯¦è§3.7LoRAå‚æ•°åˆå¹¶
# Prefix Tuningæš‚ä¸æ”¯æŒ
python export_model.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --output_path ./inference \
    --dtype float16


# é™æ€å›¾æ¨¡å‹æ¨ç†
python predictor.py \
    --model_name_or_path inference \
    --data_file ./data/dev.json \
    --dtype float16 \
    --mode static
```

#### 4.3 Inference Model é«˜æ€§èƒ½æ¨ç†

<div align="center">
    <img width="500" alt="llm" src="https://github.com/PaddlePaddle/PaddleNLP/assets/63761690/fb248224-0ad1-4d6a-a1ca-3a8dd765c41d">
</div>
<div align="center">
    <font size ="1">
    æ¨ç†éƒ¨ç½²æ€§èƒ½ä¸šç•Œé¢†å…ˆ
     </font>
</div>


æ­¤å¤– PaddleNLP è¿˜æä¾›äº†é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ï¼Œä»è€ŒåŠ é€Ÿ LLM æ¨¡å‹çš„éƒ¨ç½²è½åœ°

æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨å¦‚ä¸‹æ‰€ç¤ºï¼š

| Model                       | Inference Model | PTuning | Wint8 | PTQ |
|-----------------------------|-----------------|---------|-------|-----|
| [LLaMA1/2](./llama)         | âœ…               | âœ…       | âœ…     | âœ…   |
| [ChatGLM](./chatglm)        | âœ…               | âœ…       | âœ…     | âŒ   |
| [ChatGLM2](./chatglm2)      | âœ…               | âŒ       | âŒ     | âŒ   |
| [BaiChuan1](./baichuan)     | âœ…               | âœ…       | âœ…     | âœ…   |
| [BaiChuan2-7B](./baichuan)  | âŒ               | âŒ       | âŒ     | âŒ   |
| [BaiChuan2-13B](./baichuan) | âœ…               | âœ…       | âœ…     | âœ…   |
| [Bloom](./bloom)            | âœ…               | âœ…       | âœ…     | âŒ   |
| [GPT-3](./gpt-3)            | âœ…               | âŒ       | âŒ     | âŒ   |
| [Qwen](./qwen)              | âŒ               | âŒ       | âŒ     | âŒ   |



### 5. æœåŠ¡éƒ¨ç½²

#### 5.1 ç¯å¢ƒå‡†å¤‡

- python >= 3.8
- gradio
- flask

#### 5.2 Flask & Gradio UIæœåŠ¡åŒ–éƒ¨ç½²

æˆ‘ä»¬æä¾›äº†ä¸€å¥—ç®€å•æ˜“ç”¨çš„UIæœåŠ¡åŒ–éƒ¨ç½²è„šæœ¬:


```
python -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" flask_server.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --port 8010 \
    --flask_port 8011 \
    --src_length 1024 \
    --dtype "float16"
```

<details><summary>&emsp; è„šæœ¬å‚æ•°ä»‹ç»</summary><div>

- `port`: Gradio UI æœåŠ¡ç«¯å£å·ï¼Œé»˜è®¤8011ã€‚
- `flask_port`: FlaskæœåŠ¡ç«¯å£å·ï¼Œé»˜è®¤8010ã€‚
- å…¶ä»–å‚æ•°è¯·å‚è§åŠ¨æ€å›¾æ¨ç†ä¸­å‚æ•°ã€‚

</div></details>

### 6. æƒé‡è‡ªåŠ¨è½¬æ¢
PaddleNLP æä¾›äº†å¯è‡ªåŠ¨å°† PyTorch ç›¸å…³çš„æƒé‡è½¬åŒ–ä¸º Paddle æƒé‡çš„æ¥å£ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from paddlenlp.transformers import AutoModelForCausalLM

AutoModelForCausalLM.from_pretrained("/path/to/pytorch/model", convert_from_torch=True, dtype="float16")
```

> dtype ä¸ºè½¬åŒ–æƒé‡çš„çœŸå® dtype æ•°æ®ç±»å‹ï¼Œé€šå¸¸ä¸ºï¼šfloat16, bloat16 å’Œ float32ã€‚

ä»¥ä¸Šä»£ç å¯è‡ªåŠ¨åŠ è½½ pytorch æƒé‡å¹¶è½¬åŒ–ä¸ºå¯¹åº” paddle æƒé‡ä¿å­˜åœ¨ `/path/to/pytorch/model` ç›®å½•ä¸‹ã€‚
è½¬æ¢ torch åˆ†ç‰‡æƒé‡ç­‰æ–¹æ³•å…·ä½“å‚è€ƒ[æ–‡æ¡£](./docs/torch2paddle.md)
