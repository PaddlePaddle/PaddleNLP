/root/paddlejob/workspace/zhengxiong/pd_zx/bin/python: can't open file './llm/run_pretrain.py': [Errno 2] No such file or directory
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:18:02,621] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:18:02,622] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:18:02,622] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:18:02.623777 17820 tcp_utils.cc:107] Retry to connect to 10.174.138.221:57868 while the server is not yet listening.
I0102 17:18:05.623924 17820 tcp_utils.cc:130] Successfully connected to 10.174.138.221:57868
W0102 17:18:07.641075 17820 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:18:07.668573 17820 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 17:18:08,753] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 17:18:09,184] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:18:09,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:18:11,430] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:18:14,976] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:18:14,976] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:18:14,977] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - [0m
[33m[2024-01-02 17:18:14,980] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:18:14,991] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:18:14,992] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:18:15,025] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,026] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,052] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,053] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:18:15,078] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,078] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,112] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:18:15,113] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:18:15,114] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,115] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,115] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,115] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,115] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11888.62it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.28s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.30s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.23s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.10s/it]
[32m[2024-01-02 17:18:30,264] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:18:30,265] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:18:30,298] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:18:30,299] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1327, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1397, in _get_resized_embeddings
    raise TypeError(
TypeError: Old embeddings are of type <class 'paddle.distributed.fleet.layers.mpu.mp_layers.VocabParallelEmbedding'>, which is not an instance of <class 'paddle.nn.layer.common.Embedding'>. You should either use a different resize function or make sure that old_embeddings are an instance of <class 'paddle.nn.layer.common.Embedding'>.
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:32:16,652] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:32:16,652] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:32:16,652] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:32:16.654135 26340 tcp_utils.cc:130] Successfully connected to 10.174.138.221:48964
W0102 17:32:18.789242 26340 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:32:18.816327 26340 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 17:32:19,812] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 17:32:20,280] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:32:20,281] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:32:20,818] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:32:21,773] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:32:21,774] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:32:21,775] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - [0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - [0m
[33m[2024-01-02 17:32:21,779] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:32:21,780] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,780] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:32:21,790] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:32:21,791] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:32:21,825] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,826] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,853] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,854] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:32:21,895] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,896] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,929] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:32:21,929] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:32:21,931] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,931] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,931] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,931] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,931] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11761.93it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.20s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.15s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.11s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.99s/it]
[32m[2024-01-02 17:32:36,331] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:32:36,331] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:32:36,378] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:32:36,379] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1328, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1407, in _get_resized_embeddings
    len(new_tokenizer),
NameError: name 'new_tokenizer' is not defined
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:33:50,974] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:33:50,974] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:33:50,975] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:33:50.976552 27319 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35364 while the server is not yet listening.
I0102 17:33:53.976750 27319 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35364
W0102 17:33:55.921015 27319 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:33:55.950650 27319 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 17:33:57,077] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 17:33:57,421] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:33:57,422] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:33:57,726] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:33:58,731] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:33:58,733] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:33:58,733] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:33:58,734] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 17:33:58,734] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,734] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:33:58,735] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - [0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - [0m
[33m[2024-01-02 17:33:58,737] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:33:58,748] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:33:58,748] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:33:58,781] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,783] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,807] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,808] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:33:58,833] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,834] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,866] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:33:58,866] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:33:58,868] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,868] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,868] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,869] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,869] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12122.27it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.18s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.14s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.10s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.99s/it]
[32m[2024-01-02 17:34:13,666] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:34:13,666] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:34:13,705] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:34:13,706] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
> /root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py(1406)_get_resized_embeddings()
-> if self.config.tensor_parallel_degree > 1:
(Pdb) *** NameError: name 'elf' is not defined
(Pdb) *** NameError: name 'elf' is not defined
(Pdb) *** NameError: name 'elf' is not defined
(Pdb) *** NameError: name 'elf' is not defined
(Pdb) *** NameError: name 'elf' is not defined
(Pdb) /root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:07:21,365] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:07:21,365] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:07:21,365] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:07:21.367274 50209 tcp_utils.cc:107] Retry to connect to 10.174.138.221:51334 while the server is not yet listening.
I0102 18:07:24.367436 50209 tcp_utils.cc:130] Successfully connected to 10.174.138.221:51334
W0102 18:07:26.269480 50209 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:07:26.295517 50209 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 18:07:29,410] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 18:07:29,861] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:07:29,862] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:07:30,179] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:07:31,050] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:07:31,051] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:07:31,051] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - [0m
[33m[2024-01-02 18:07:31,055] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:07:31,065] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:07:31,066] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:07:31,116] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,117] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,144] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,144] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:07:31,171] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,171] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,209] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:07:31,209] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:07:31,211] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,211] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,211] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,211] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,212] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11625.01it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.26s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.28s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.29s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.14s/it]
[32m[2024-01-02 18:07:46,662] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:07:46,662] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:07:46,712] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:07:46,713] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:07:46,735] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:07:46,756] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:07:46,757] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:07:46,757] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:07:46,999] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:07:47,061] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:07:53,221] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:07:53,229] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:07:53,231] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:53,299] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:07:53,299] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - current_device                : gpu:3[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-07-21_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - optimizer_name_suffix         : tp01_shard01[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:07:53,328] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:07:53,329] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:07:53,330] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-02 18:07:53,330] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:07:53,330] [    INFO][0m - [0m
[32m[2024-01-02 18:07:53,330] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:07:53,336] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,374] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,459] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,549] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,602] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:07:53,653] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,654] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:08:18,332] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:08:18,332] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:08:18,332] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:08:18.334443 51286 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41212
W0102 18:08:23.320052 51286 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:08:23.346457 51286 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 18:08:24,645] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 18:08:25,179] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:08:25,180] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:08:25,734] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:08:27,116] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:08:27,119] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:08:27,120] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,122] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:08:27,123] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - [0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - [0m
[33m[2024-01-02 18:08:27,125] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:08:27,138] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:08:27,138] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:08:27,230] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,232] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,259] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,260] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:08:27,297] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,298] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,333] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:08:27,333] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:08:27,337] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,337] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,337] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,337] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,337] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11238.76it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.08s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.11s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.06s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.95s/it]
[32m[2024-01-02 18:08:41,818] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:08:41,818] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:08:41,857] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:08:41,858] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:08:41,877] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:08:41,898] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:08:41,898] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:08:41,898] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:08:42,143] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:08:42,209] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:08:43,736] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:08:43,741] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:08:43,741] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:08:43,746] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:08:43,746] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:43,815] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:08:43,815] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:08:43,838] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:43,838] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:08:43,838] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - current_device                : gpu:3[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:08:43,839] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:08:43,840] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-08-18_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:08:43,841] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - optimizer_name_suffix         : tp01_shard01[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - [0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:08:43,855] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:43,894] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:43,985] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-02 18:08:44,038] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,038] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:44,076] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:44,129] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:08:44,180] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,181] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:08:44,181] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:08:44,181] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:08:44,185] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:08:44,194] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:09:09,710] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:09:13,917] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:09:53,419] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:09:53,419] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:09:53,419] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:09:53.421258 52878 tcp_utils.cc:107] Retry to connect to 10.174.138.221:61359 while the server is not yet listening.
I0102 18:09:56.421422 52878 tcp_utils.cc:130] Successfully connected to 10.174.138.221:61359
W0102 18:10:00.123006 52878 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:00.149163 52878 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 18:10:01,064] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 18:10:01,475] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:01,476] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:01,921] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:02,972] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:02,973] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:02,974] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:02,975] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:02,975] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,975] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,975] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,975] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - [0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - [0m
[33m[2024-01-02 18:10:02,978] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:02,989] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:02,989] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:03,020] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,021] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,047] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,048] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:03,077] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,078] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,111] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:03,112] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:03,113] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,114] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,114] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,114] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,114] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11755.34it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.14s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.11s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.10s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.99s/it]
[32m[2024-01-02 18:10:17,793] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:17,794] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:17,829] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:17,829] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:17,849] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:17,875] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:17,875] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:17,875] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:18,120] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:18,182] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:19,222] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:19,231] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:10:19,232] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:19,300] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:19,300] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - current_device                : gpu:3[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-09-53_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - optimizer_name_suffix         : tp01_shard01[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - [0m
[32m[2024-01-02 18:10:19,336] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:10:19,343] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,381] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,500] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-02 18:10:19,552] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,553] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,591] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,644] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:19,696] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,697] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:10:35,050] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:10:35,050] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:10:35,051] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0102 18:10:35.052629 53319 tcp_utils.cc:107] Retry to connect to 10.174.138.221:47036 while the server is not yet listening.
I0102 18:10:38.052809 53319 tcp_utils.cc:130] Successfully connected to 10.174.138.221:47036
W0102 18:10:40.361042 53319 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:40.384653 53319 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 18:10:41,373] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 18:10:41,841] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:41,843] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:42,357] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:43,224] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:43,226] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:43,227] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:43,232] [    INFO][0m - [0m
[33m[2024-01-02 18:10:43,232] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:43,287] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,289] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,314] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,314] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:43,345] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,346] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,383] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:43,383] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:43,387] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,387] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,387] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,387] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,387] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11963.22it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.11s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.17s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.11s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.02s/it]
[32m[2024-01-02 18:10:58,100] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:58,100] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:58,142] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:58,143] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:58,166] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:58,189] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:58,190] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:58,190] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:58,437] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:58,499] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:59,298] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:59,303] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:10:59,303] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:59,307] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:10:59,308] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:59,377] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:59,377] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:59,400] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:59,400] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:59,400] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:59,400] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - current_device                : gpu:3[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:59,401] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-10-35_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - optimizer_name_suffix         : tp01_shard01[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - [0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:10:59,415] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,454] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,537] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-02 18:10:59,588] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,588] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,626] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,679] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:59,730] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,731] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:10:59,731] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:10:59,732] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:10:59,736] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:10:59,756] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:11:24,770] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:11:24,775] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:11:27,588] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:26:52,726] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:26:52,726] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:26:52,727] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0102 18:26:52.728382 62340 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55969 while the server is not yet listening.
I0102 18:26:55.728550 62340 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55969
W0102 18:26:57.757036 62340 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:26:57.782974 62340 gpu_resources.cc:149] device: 3, cuDNN Version: 8.6.
[2024-01-02 18:26:58,810] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:26:59,796] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:27:00,704] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:27:00,705] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:27:00,706] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:27:00,707] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - [0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - [0m
[33m[2024-01-02 18:27:00,709] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:27:00,709] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:27:00,721] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:27:00,721] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:27:00,763] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,764] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,787] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,788] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:27:00,813] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,814] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,850] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:27:00,850] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:27:00,852] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,852] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,852] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,852] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,852] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11748.75it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.30s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.14s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.20s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.01s/it]
[32m[2024-01-02 18:27:15,444] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:27:15,444] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:27:15,490] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:27:15,491] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:27:15,512] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 529, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1351, in resize_token_embeddings
    self.model.lm_head.weight = new_lm_head_weight
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1474, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'LlamaForCausalLM' object has no attribute 'model'
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:03:44,891] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:03:44,891] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:03:44,892] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
=======================================================================
I0104 21:03:44.893286  7620 tcp_utils.cc:130] Successfully connected to 10.174.138.221:54784
W0104 21:03:49.813167  7620 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:03:49.840458  7620 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:03:53,258] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:03:54,423] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:03:56,337] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:03:56,339] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:03:56,343] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:03:56,344] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - [0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,345] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:03:56,346] [    INFO][0m - [0m
[33m[2024-01-04 21:03:56,346] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:03:56,347] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,348] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:03:56,366] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:03:56,366] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:03:56,420] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,421] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,451] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,452] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:03:56,480] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,481] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,526] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:03:56,526] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:03:56,530] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,530] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,530] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,530] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,530] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11366.68it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.19s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.15s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.25s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.14s/it]
[32m[2024-01-04 21:04:04,390] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:04:04,390] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:04:04,430] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:04:04,430] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:04:04,453] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:04:04,464] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:04:04,465] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:04:04,471] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:04:04,472] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:04:04,472] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:04:04,745] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:04:05,062] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:04:05,072] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:04:05,717] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:04:05,789] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:04:05,789] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:04:05,817] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:04:05,818] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:04:05,819] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-03-44_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:04:05,820] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:04:05,821] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:04:05,822] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:04:05,823] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:04:05,824] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:04:05,824] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:04:05,824] [    INFO][0m - [0m
[32m[2024-01-04 21:04:05,824] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:04:06,054] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:04:06,093] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:04:06,180] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:04:06,180] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:04:06,181] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:04:06,185] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:04:06,187] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:04:24,577] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:06:05,762] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:06:05,762] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:06:05,762] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
=======================================================================
I0104 21:06:05.763913  9356 tcp_utils.cc:130] Successfully connected to 10.174.138.221:59733
W0104 21:06:07.976080  9356 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:06:08.001581  9356 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:06:10,140] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:06:13,291] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:06:16,486] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:06:16,488] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:06:16,489] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - [0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - [0m
[33m[2024-01-04 21:06:16,494] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:06:16,495] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,496] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:06:16,508] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:06:16,508] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:06:16,552] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,554] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,577] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,577] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:06:16,603] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,604] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,642] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:06:16,642] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:06:16,646] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,646] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,646] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,646] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,646] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11391.37it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.07s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.08s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.06s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02s/it]
[32m[2024-01-04 21:06:23,852] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:06:23,852] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:06:23,892] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:06:23,892] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:06:23,914] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:06:23,931] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:06:23,932] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:06:23,940] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:06:23,941] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:06:23,941] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:06:24,209] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:06:28,144] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:06:28,149] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:06:28,150] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:06:28,154] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:06:29,014] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:29,085] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:06:29,085] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:06:29,109] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:06:29,110] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:06:29,111] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-06-05_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:06:29,112] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:06:29,113] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:06:29,114] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:06:29,115] [    INFO][0m - [0m
[32m[2024-01-04 21:06:29,116] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:06:29,168] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:06:29,207] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:06:29,277] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:06:29,277] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:06:29,277] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:06:29,277] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:06:29,281] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:06:29,281] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:07:38,134] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:22:33,232] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:22:33,232] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:22:33,233] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0104 21:22:33.234450 19918 tcp_utils.cc:107] Retry to connect to 10.174.138.221:41794 while the server is not yet listening.
I0104 21:22:36.234647 19918 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41794
W0104 21:22:38.324084 19918 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:22:38.349933 19918 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:22:39,531] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:22:40,325] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:22:42,140] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:22:42,141] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:22:42,141] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - [0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - [0m
[33m[2024-01-04 21:22:42,145] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:22:42,146] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,146] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:22:42,156] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:22:42,156] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:22:42,193] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,194] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,224] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,224] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:22:42,250] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,251] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,293] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:22:42,293] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:22:42,296] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,296] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,296] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,296] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,296] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12214.05it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.11s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.08s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.07s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.04s/it]
[32m[2024-01-04 21:22:49,274] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:22:49,274] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:22:49,318] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:22:49,319] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:22:49,340] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:22:49,352] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:22:49,360] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:22:49,360] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:22:49,360] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:22:49,633] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:22:49,991] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:22:50,000] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:22:50,891] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:50,960] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:22:50,960] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:22:50,989] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,990] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:22:50,991] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:22:50,992] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-22-33_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:22:50,993] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:22:50,994] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:22:50,995] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:22:50,996] [    INFO][0m - [0m
[32m[2024-01-04 21:22:50,997] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:22:51,009] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:22:51,047] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:22:51,143] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:22:51,143] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:22:51,144] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:22:51,144] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:22:51,148] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:22:51,152] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:23:59,329] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:27:55,386] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:27:55,386] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:27:55,386] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
=======================================================================
I0104 21:27:55.388177 23880 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55680 while the server is not yet listening.
I0104 21:27:58.388378 23880 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55680
W0104 21:28:00.857625 23880 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:28:00.881196 23880 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:28:02,045] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:28:03,067] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:28:04,340] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:28:04,340] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:28:04,341] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - [0m
[33m[2024-01-04 21:28:04,345] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:28:04,356] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:28:04,356] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:28:04,395] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,396] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,426] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,426] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:28:04,453] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,454] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,497] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:28:04,497] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:28:04,499] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,499] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,499] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,499] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,499] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12087.33it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.08s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.02s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.01s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.13it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.06it/s]
[32m[2024-01-04 21:28:11,399] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:28:11,399] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:28:11,437] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:28:11,438] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:28:11,455] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:28:11,477] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:28:11,480] [    INFO][0m - -------------------------------------new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:28:11,487] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:28:11,487] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:28:11,487] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:28:11,757] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:28:12,566] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:28:12,575] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:28:13,050] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:13,122] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:28:13,123] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:28:14,411] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:14,411] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:28:14,411] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:14,411] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:28:14,412] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:28:14,413] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-27-55_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:28:14,414] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:28:14,415] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:28:14,416] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:28:14,417] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - [0m
[32m[2024-01-04 21:28:14,418] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:28:14,433] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:28:14,472] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:28:14,622] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:28:14,622] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:28:14,622] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:28:14,623] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:28:14,627] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:28:14,629] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:29:23,341] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:34:45,704] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:34:45,704] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:34:45,704] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0104 21:34:45.705967 28843 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63421 while the server is not yet listening.
I0104 21:34:48.706161 28843 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63421
W0104 21:34:50.896378 28843 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:34:50.926860 28843 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:34:52,154] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:34:53,142] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:34:55,344] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:34:55,346] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:34:55,347] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - [0m
[33m[2024-01-04 21:34:55,352] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:34:55,353] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,354] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:34:55,366] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:34:55,366] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:34:55,410] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,412] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,441] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,442] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:34:55,472] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,473] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,511] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:34:55,512] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:34:55,514] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,514] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,514] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,514] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,514] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11567.30it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.05s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.10s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.07s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.03it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02s/it]
[32m[2024-01-04 21:35:02,878] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:35:02,878] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:35:02,914] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:35:02,915] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:35:02,935] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:35:02,961] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:35:02,961] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:35:02,961] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:35:03,233] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:35:03,499] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:35:03,504] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:35:03,504] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:35:03,508] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:35:04,216] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:35:04,285] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:35:04,285] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:35:04,308] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:35:04,309] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,310] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-34-45_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:35:04,311] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:35:04,312] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:35:04,313] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:35:04,314] [    INFO][0m - [0m
[32m[2024-01-04 21:35:04,315] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:35:04,458] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:35:04,496] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:35:04,589] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:35:04,589] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:35:04,589] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:35:04,593] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:35:04,594] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:36:10,939] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:36:10,939] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:36:10,939] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0104 21:36:10.940927 30163 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35737
W0104 21:36:16.806079 30163 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:36:16.832887 30163 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:36:17,882] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:36:18,914] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:36:18,915] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:36:18,916] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:36:20,314] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:36:20,315] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:36:20,315] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:36:20,316] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:20,316] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - [0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:36:20,318] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - [0m
[33m[2024-01-04 21:36:20,319] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:36:20,330] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:36:20,330] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:36:20,364] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,365] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,397] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,397] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:36:20,428] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,429] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,474] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:36:20,474] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:36:20,476] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,476] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,476] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,476] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,476] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12206.94it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.08s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:01<00:02,  1.03it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.02s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.03it/s]
[32m[2024-01-04 21:36:27,374] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:36:27,374] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:36:27,409] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:36:27,410] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:36:27,428] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:36:27,455] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:36:27,456] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:36:27,456] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:36:27,725] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:36:34,497] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:36:34,507] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:36:35,053] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:35,123] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:36:35,123] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:36:35,160] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:35,160] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:36:35,161] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:36:35,162] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:36:35,163] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-36-10_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-04 21:36:35,164] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:36:35,165] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:36:35,166] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - [0m
[32m[2024-01-04 21:36:35,167] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:36:35,550] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:36:35,589] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-04 21:36:35,753] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:36:35,753] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:36:35,754] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 128[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total num train samples = 256,000[0m
[32m[2024-01-04 21:36:35,758] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:36:35,758] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:36:46,684] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-04 21:36:48,557] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-04 21:36:50,140] [    INFO][0m - ret is None[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:37:48,012] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:37:48,013] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:37:48,013] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:37:48.014636 31401 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63978 while the server is not yet listening.
I0104 21:37:51.014848 31401 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63978
W0104 21:37:53.370421 31401 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:37:53.398043 31401 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:37:55,343] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
[2024-01-04 21:37:57,769] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:37:59,330] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [3], pp_group: [3], dp_group: [1, 3], check/clip group: [2, 3]
[32m[2024-01-04 21:37:59,332] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:37:59,333] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - [0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - [0m
[33m[2024-01-04 21:37:59,337] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,339] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:37:59,351] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:37:59,351] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:37:59,394] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,396] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,422] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,423] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:37:59,454] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,455] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,490] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:37:59,490] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:37:59,493] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,493] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,493] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,493] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,493] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11541.84it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:02<00:09,  2.33s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:04<00:06,  2.28s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:06<00:04,  2.25s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:09<00:02,  2.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.19s/it]
[32m[2024-01-04 21:38:15,616] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:38:15,616] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:38:15,653] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:38:15,654] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:38:15,675] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:38:15,710] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:38:15,710] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:38:15,710] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:38:15,983] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:38:19,280] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:38:19,284] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:38:19,284] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:38:19,289] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-04 21:38:20,999] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:38:21,074] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:38:21,074] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:38:21,103] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:38:21,104] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:38:21,105] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-37-48_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:38:21,106] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:38:21,107] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:21,108] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:38:21,109] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:38:21,110] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:38:21,110] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-04 21:38:21,110] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:38:21,110] [    INFO][0m - [0m
[32m[2024-01-04 21:38:21,110] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:38:21,116] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:38:21,156] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:38:21,275] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:38:21,275] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:38:21,275] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:38:21,276] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:38:21,280] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:38:21,313] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:38:34,593] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-04 21:38:58,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:39:40,803] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:39:40,804] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:39:40,804] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
=======================================================================
I0104 21:39:40.805567 32987 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35415 while the server is not yet listening.
I0104 21:39:43.805776 32987 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35415
W0104 21:39:45.808192 32987 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:39:45.834967 32987 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-04 21:39:48,386] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
[2024-01-04 21:39:49,020] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:39:50,932] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [3], pp_group: [3], dp_group: [1, 3], check/clip group: [2, 3]
[32m[2024-01-04 21:39:50,933] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:39:50,933] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - [0m
[33m[2024-01-04 21:39:50,937] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:39:50,992] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:50,993] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,020] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,021] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:39:51,049] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,050] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,084] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:39:51,084] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:39:51,086] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,086] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,086] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,086] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,086] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12094.30it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:02<00:09,  2.39s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:04<00:06,  2.19s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:06<00:04,  2.18s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:08<00:02,  2.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.11s/it]
[32m[2024-01-04 21:40:07,057] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:40:07,057] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:40:07,099] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:40:07,100] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:40:07,119] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:40:07,148] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:40:07,148] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:40:07,148] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:40:07,420] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:40:07,584] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:40:07,588] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:40:07,589] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:40:07,593] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-04 21:40:08,831] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-04 21:40:08,899] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:40:08,900] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:40:08,928] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:40:08,929] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:40:08,930] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-39-40_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:40:08,931] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - optimizer_name_suffix         : tp01[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:08,932] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:40:08,933] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:40:08,934] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - [0m
[32m[2024-01-04 21:40:08,935] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:40:11,080] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:40:11,119] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:40:11,403] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:40:11,403] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:40:11,404] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:40:11,404] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:40:11,409] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:40:11,440] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:40:26,818] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:02:57,832] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:02:57,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:02:57,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:02:57.834734 110468 tcp_utils.cc:107] Retry to connect to 10.174.138.221:46099 while the server is not yet listening.
I0105 13:03:00.834939 110468 tcp_utils.cc:130] Successfully connected to 10.174.138.221:46099
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:14:25,033] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:14:25,033] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:14:25,033] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:14:25.034742 49447 tcp_utils.cc:130] Successfully connected to 10.174.138.221:43579
W0105 13:14:29.714565 49447 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:14:29.741523 49447 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:14:30,743] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:14:31,379] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:14:32,890] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:14:32,892] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:14:32,893] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:14:32,894] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - [0m
[33m[2024-01-05 13:14:32,897] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:14:32,898] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,923] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,923] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:14:32,926] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,927] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:14:49,329] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:14:57,883] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:24<00:24, 24.35s/it][32m[2024-01-05 13:15:03,129] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:15:05,441] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:30<00:00, 13.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:30<00:00, 15.45s/it]
[32m[2024-01-05 13:15:05,843] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[33m[2024-01-05 13:15:05,843] [ WARNING][0m - Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path and are newly initialized: ['lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2024-01-05 13:15:05,844] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-05 13:15:05,845] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:15:05,845] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:15:05,845] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:15:06,117] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:15:06,317] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:15:06,322] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:15:06,323] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:15:06,327] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:15:07,261] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:15:07,371] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:15:07,372] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:15:07,413] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:15:07,413] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:15:07,413] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:15:07,413] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:15:07,414] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:15:07,416] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:15:07,417] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:15:07,418] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-14-25_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:15:07,419] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:15:07,420] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:15:07,421] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:15:07,422] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:15:07,423] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:15:07,424] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - [0m
[32m[2024-01-05 13:15:07,425] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:15:07,503] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:15:07,544] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-05 13:15:07,656] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:15:07,656] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:15:07,656] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:15:07,661] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:15:07,661] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:16:25,624] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:16:25,630] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:16:27,850] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:17:36,955] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:17:36,955] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:17:36,955] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:17:36.957211 52411 tcp_utils.cc:107] Retry to connect to 10.174.138.221:38381 while the server is not yet listening.
I0105 13:17:39.957405 52411 tcp_utils.cc:130] Successfully connected to 10.174.138.221:38381
W0105 13:17:44.979211 52411 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:17:45.010377 52411 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:17:46,928] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:17:48,083] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:17:48,083] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:17:48,084] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:17:48,085] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:17:48,086] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - [0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - [0m
[33m[2024-01-05 13:17:48,087] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,109] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,110] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:17:48,111] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,112] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:38:43,734] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:38:43,735] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:38:43,735] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:38:43.736403 71804 tcp_utils.cc:130] Successfully connected to 10.174.138.221:53539
W0105 13:38:48.649689 71804 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:38:48.677879 71804 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:38:50,050] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:38:51,165] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:38:52,587] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:38:52,588] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:38:52,588] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - [0m
[33m[2024-01-05 13:38:52,592] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,614] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,615] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:38:52,616] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,617] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:39:06,403] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:14,215] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:20<00:20, 20.36s/it][32m[2024-01-05 13:39:19,999] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:23,008] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 13.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.21s/it]
[32m[2024-01-05 13:39:23,490] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:39:23,490] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:39:23,491] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:39:23,491] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:39:23,491] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:39:23,492] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:39:23,758] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:39:24,093] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:39:24,103] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:39:25,038] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:39:25,108] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:39:25,109] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:39:25,146] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:39:25,146] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:39:25,146] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:39:25,147] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:39:25,148] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:39:25,149] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-38-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-05 13:39:25,150] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:39:25,151] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:39:25,152] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - [0m
[32m[2024-01-05 13:39:25,153] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:39:25,258] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:39:25,297] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-05 13:39:25,376] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:39:25,376] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:39:25,376] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:39:25,377] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:39:25,380] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:39:25,381] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:40:33,582] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:40:35,464] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:41:37,924] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:41:37,924] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:41:37,924] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:41:37.925913 73847 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63585 while the server is not yet listening.
I0105 13:41:40.926118 73847 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63585
W0105 13:41:42.851975 73847 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:41:42.880671 73847 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:41:43,858] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:41:44,656] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:41:45,959] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:41:45,960] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:41:45,961] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - [0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:41:45,966] [    INFO][0m - [0m
[33m[2024-01-05 13:41:45,966] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:41:45,968] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,993] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,993] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 3,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:41:45,996] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,997] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:42:01,884] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:11,532] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:24<00:24, 24.49s/it][32m[2024-01-05 13:42:17,463] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:20,262] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:32<00:00, 14.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:32<00:00, 16.13s/it]
[32m[2024-01-05 13:42:20,756] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:42:20,756] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:42:20,757] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:42:20,758] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:42:20,758] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:42:20,758] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:42:21,027] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:42:21,524] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:42:21,529] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:42:21,530] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:42:21,534] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:42:22,511] [    INFO][0m - The global seed is set to 42, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:42:22,580] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:42:22,580] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:42:22,610] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:42:22,611] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:42:22,612] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-41-37_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:42:22,613] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - optimizer_name_suffix         : tp03[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,614] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:42:22,615] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - tensor_parallel_rank          : 3[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:42:22,616] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - weight_name_suffix            : tp03[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - [0m
[32m[2024-01-05 13:42:22,617] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:42:22,673] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:42:22,712] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-05 13:42:22,794] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:42:22,794] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:42:22,794] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:42:22,795] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:42:22,795] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:42:22,795] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:42:22,799] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:42:22,799] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:43:32,931] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:43:32,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:43:34,820] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:43:57,096] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:43:57,096] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:43:57,096] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:43:57.098006 75818 tcp_utils.cc:130] Successfully connected to 10.174.138.221:62346
W0105 13:43:59.259984 75818 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:43:59.290366 75818 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:44:00,453] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:44:01,228] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:44:01,824] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [3],  sharding_group: [0, 1, 2, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:44:01,824] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:44:01,825] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:44:01,826] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - [0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - [0m
[33m[2024-01-05 13:44:01,828] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,853] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,853] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:44:01,854] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,855] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:17<00:17, 17.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 11.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 12.33s/it]
[32m[2024-01-05 13:44:37,128] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:44:37,128] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:44:37,130] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:44:37,130] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:44:37,130] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:44:37,130] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:44:37,370] [    INFO][0m - Frozen parameters: 6.48e+09 || Trainable parameters:5.10e+08 || Total parameters:6.99e+09|| Trainable:7.30%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:44:46,116] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:44:46,130] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:44:46,132] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:44:46,146] [    INFO][0m -  ï¼ˆå…‰å®³ï¼‰ä¹Ÿè¶Šæ¥è¶Šå¤šã€‚
è¿™äº›äººé€ çš„ç¯å…‰ç…§äº®å¤œæ™šçš„å¤©ç©ºï¼ŒåŠ ä¸Šäººç±»æ´»åŠ¨é€ æˆçš„å„ç§ç©ºæ°”æ±¡æŸ“å¼•èµ·çš„ç°å°˜æ•£å°„ç¯å…‰çš„åŒé‡æ•ˆåº”ï¼Œä½¿å¾—èƒŒæ™¯å¤©ç©ºå˜å¾—ç°æš—ï¼Œå¯¹å…‰åº¦å¾®å¼±çš„å¤©ä½“è§‚æµ‹è¶ŠåŠ å›°éš¾ï¼Œå¿…é¡»è¦ç‰¹åˆ«çš„æ»¤å…‰é•œæ¥éš”ç»èƒŒæ™¯å…‰çš„å¹²æ‰°ã€‚
åœ¨æœ‰äº›åœ°åŒºï¼Œå¦‚ç¾Žå›½äºšåˆ©æ¡‘é‚£å·žã€è‹±å›½å’Œæ—¥æœ¬ä¸Žé¦™æ¸¯ï¼Œéƒ½æœ‰æ°‘é—´å›¢ä½“åœ¨ç ”ç©¶ä¸Žå‘èµ·ä»¥å‡å°‘å…‰æ±¡æŸ“ã€‚
é¼“åŠ±ä¸ºè¡—ç¯è£…ä¸Šåå°„ç½©ï¼Œä¸ä»…èƒ½ä½¿ç…§å‘åœ°é¢çš„ç¯å…‰å¢žåŠ ï¼Œä¹Ÿä½¿ç›´æŽ¥å°„è¿›å¤©ç©ºçš„å…‰é‡é™ä½Žã€‚
ä½†å› ä¸ŽåŸŽå¸‚çš„ç»æµŽå‘å±•ç›¸è¿èƒŒï¼Œå¯¹äºŽæŽ¨å¹¿æ„è¯†çš„æ·¡è–„ä¸Žå¯¹ç§‘å­¦çš„ä¸é‡è§†ä¹‹ä¸‹ï¼Œå°¤å…¶åœ¨å‘å±•ä¸­å›½å®¶ï¼Œè¿™æ ·çš„çŠ¶å†µèš•é£Ÿä¸€äº›å¤è€çš„å¤©æ–‡å°è‡³ä¸å†èƒ½ä½œè§‚æµ‹ã€‚
å¦‚åŒ—äº¬å¤è§‚è±¡å°ä¸Žå—äº¬ç´«é‡‘å±±å¤©æ–‡å°ç­‰ã€‚</s> æœ‰é‚£äº›è¿‡èª‰æˆ–ä¸å®žç”¨çš„æ–‡å…·ï¼Ÿ
å°±ç®—å†å¤šäººæŽ¨èæˆ‘ä¹Ÿè¦è¯´.. ç™¾ä¹V5ä¸­æ€§ç¬”ï¼
ä¹‹å‰çœ‹åˆ°çš„æ‰€æœ‰è¯„ä»·éƒ½è¯´è¿™æ˜¯æœ€å¥½å†™çš„ä¸­æ€§ç¬”ä»€ä¹ˆçš„. ä½†æ˜¯å¯èƒ½æ˜¯æ°´æ€§ç¬”çš„å…³ç³»ï¼Œè¿™ç¬”ç”¨åœ¨ç¨è–„ä¸€ç‚¹çš„çº¸ä¸Šéƒ½ä¼šé€åˆ°èƒŒé¢ï¼Œä¸€èˆ¬70gä»¥ä¸‹çš„çº¸å†™åœ¨æ­£é¢ï¼ŒèƒŒé¢ä¼šé€å‡ºå®Œå…¨é•œåƒçš„å­—ï¼Œè‡ªå·±åšç¬”è®°ç”¨å¥½ä¸€ç‚¹çš„çº¸V5å†™èµ·æ¥å¾ˆä¸é”™ï¼Œä½†æ˜¯è¯¾æœ¬è¯•å·ä»€ä¹ˆçš„çœŸçš„é€å¾—å¾ˆä¸¥é‡. è¿˜æœ‰ä¸€ä¸ªå°±æ˜¯daycraftçš„æœ¬å­ï¼Œè®¾è®¡æ–¹é¢è§ä»è§æ™ºï¼Œä½†æ˜¯ç”¨é’¢ç¬”å®¹æ˜“æ´‡ï¼Œè¯•è¿‡4001çš„å¢¨å’Œç™¾ä¹çš„å¢¨ï¼Œå†™åœ¨ä¸Šé¢éƒ½ä¼šå‘å››å‘¨è¾å°„ä¸€æ ·æ•£å¼€æ¥ï¼ŒFå°–çš„éƒ½</s>ITå¤§ç‰›æ˜¯æ€Žæ ·ç‚¼æˆçš„ï¼Ÿ
å…¬å¸æ¥äº†ä¸ªITå¤§ç‰›ï¼Œ92å¹´å‡ºç”Ÿçš„äººï¼ŒçŽ°åœ¨æ˜¯æˆ‘ä»¬çš„team leaderã€‚
æ˜¯æˆ‘è§è¿‡çš„ç¨‹åºå‘˜é‡Œæœ€åŽ‰å®³çš„ä¸€ä¸ªï¼Œä¸çŸ¥é“ä»–çš„èƒ½åŠ›æ˜¯æ€Žä¹ˆç‚¼æˆçš„ã€‚
è¯´è¯´ä»–çš„äº‹è¿¹å§ã€‚
ä»–é«˜ä¸­çš„æ—¶å€™ï¼Œå°±æŽ¥è§¦ç¼–ç¨‹ï¼ˆVB6ï¼‰ï¼Œä»–ä¸€ç›´åœ¨æƒ³ï¼Œè®¾è®¡ä¸€ä¸ªç®—æ³•å’Œè‡ªå·±å¯¹å¼ˆï¼ˆè±¡æ£‹ï¼‰ï¼Œçœ‹çœ‹æ˜¯è‡ªå·±è®¾è®¡çš„ç®—æ³•åŽ‰å®³ï¼Œè¿˜æ˜¯è‡ªå·±åŽ‰å®³ã€‚
ä»–è‡ªå·±ç‹¬ç«‹åšï¼Œæ²¡æœ‰å‚è€ƒä»»ä½•èµ„æ–™ï¼ŒèŠ±äº†åŠä¸ªå­¦æœŸï¼Œç«Ÿç„¶å†™å‡ºæ¥äº†ã€‚
ä»–è‡ªå·±åŠ¨æ‰‹åšäº†ä¸€å°æ–¯ç‰¹æž—å‘åŠ¨æœºï¼Œè®¾è®¡äº†ä¸€å°é…’ç²¾ç¯å¸¦åŠ¨çš„ç¼çº«æœºåŽŸåž‹ã€‚
ä»–å®¶æ˜¯å†œæ‘çš„ï¼Œä»–çˆ¸çˆ¸å¼€è´§è½¦ï¼Œè½¦æŒ‚è¢«å·äº†ã€‚
ä»–å°±ç”¨ç»§ç”µå™¨åšäº†ä¸€ä¸ªæŠ¥è­¦å™¨ï¼Œå¦‚æžœå·è½¦è´¼å·èµ°è½¦æŒ‚ï¼Œå°±ä¼šå¼„æ–­é“œçº¿ï¼Œè§¦å‘ç»§ç”µå™¨æŠ¥è­¦ã€‚
å†œæ‘çš„å»ºç­‘å·¥äºº[0m
[32m[2024-01-05 13:44:49,565] [    INFO][0m - The global seed is set to 45, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:49,634] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:44:49,634] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:44:49,690] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - dataset_rank                  : 3[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:44:49,691] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:44:49,692] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-43-57_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:44:49,693] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - optimizer_name_suffix         : shard03[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:44:49,694] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:44:49,695] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - sharding_parallel_rank        : 3[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - tensor_parallel_degree        : 1[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:44:49,696] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - weight_name_suffix            : [0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - [0m
[32m[2024-01-05 13:44:49,697] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:44:49,713] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-01-05 13:44:49,897] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-01-05 13:44:49,898] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:44:49,898] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:44:49,898] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:44:49,898] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:44:49,899] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:44:49,899] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:44:49,899] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-05 13:44:49,899] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:44:49,899] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Number of trainable parameters = 509,804,544 (per device)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:45:22,945] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Num examples = 920176[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total Batch size = 32[0m
[32m[2024-01-05 13:45:27,158] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:45:43,571] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:45:43,572] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:45:43,572] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='7', default_value='')
=======================================================================
I0105 13:45:43.573783 77266 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55899 while the server is not yet listening.
I0105 13:45:46.573940 77266 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55899
W0105 13:45:48.742522 77266 gpu_resources.cc:119] Please NOTE: device: 7, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:45:48.768682 77266 gpu_resources.cc:149] device: 7, cuDNN Version: 8.6.
[2024-01-05 13:45:50,278] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:45:50,818] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-05 13:45:50,819] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:45:51,208] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-05 13:45:52,261] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 3, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [1, 3], pp_group: [3], dp_group: [3], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:45:52,263] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:45:52,264] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - [0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:45:52,269] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:45:52,269] [    INFO][0m - [0m
[33m[2024-01-05 13:45:52,269] [ WARNING][0m - Process rank: 3, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:45:52,270] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,295] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,295] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 1,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:45:52,298] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,299] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:46:08,308] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:16,735] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:22<00:22, 22.80s/it][32m[2024-01-05 13:46:24,607] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:27,941] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:31<00:00, 14.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:31<00:00, 15.99s/it]
[32m[2024-01-05 13:46:29,394] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:46:29,394] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:46:29,396] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:46:29,396] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:46:29,396] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:46:29,396] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:46:29,670] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:46:30,440] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:46:30,450] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-05 13:46:31,952] [    INFO][0m - The global seed is set to 44, local seed is set to 49 and random seed is set to 42.[0m
[32m[2024-01-05 13:46:32,024] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:46:32,024] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:46:32,046] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:46:32,046] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:46:32,046] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:46:32,046] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:46:32,046] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - current_device                : gpu:7[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:46:32,047] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:46:32,048] [    INFO][0m - gradient_accumulation_steps   : 128[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - local_process_index           : 3[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - local_rank                    : 3[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-45-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,049] [    INFO][0m - logical_process_index         : 3[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - optimizer_name_suffix         : tp01_shard01[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:46:32,050] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - process_index                 : 3[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:46:32,051] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - tensor_parallel_rank          : 1[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:46:32,052] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - weight_name_suffix            : tp01[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - [0m
[32m[2024-01-05 13:46:32,053] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:46:32,112] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,153] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,242] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 3 is not in group _default_pg13
  warnings.warn(
[2024-01-05 13:46:32,296] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,296] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,334] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,387] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-05 13:46:32,439] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,440] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:46:32,440] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Gradient Accumulation steps = 128[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:46:32,444] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-05 13:46:32,450] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:47:20,138] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-05 13:47:22,865] [    INFO][0m - Saving optimizer files.[0m
