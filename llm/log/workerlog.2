/root/paddlejob/workspace/zhengxiong/pd_zx/bin/python: can't open file './llm/run_pretrain.py': [Errno 2] No such file or directory
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:18:02,614] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:18:02,615] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:18:02,615] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:18:02.616866 17818 tcp_utils.cc:107] Retry to connect to 10.174.138.221:57868 while the server is not yet listening.
I0102 17:18:05.617058 17818 tcp_utils.cc:130] Successfully connected to 10.174.138.221:57868
W0102 17:18:07.641006 17818 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:18:07.668571 17818 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:18:08,753] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:18:09,185] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:18:09,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:18:11,430] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:18:14,976] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:18:14,976] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:18:14,977] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - [0m
[33m[2024-01-02 17:18:14,981] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:18:14,991] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:18:14,992] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:18:15,076] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,077] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,103] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,104] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:18:15,134] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,135] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,182] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:18:15,182] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 9827.33it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:09,  2.43s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.30s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.23s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.19s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.08s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.16s/it]
[32m[2024-01-02 17:18:30,856] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:18:30,856] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:18:30,896] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:18:30,896] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1327, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1397, in _get_resized_embeddings
    raise TypeError(
TypeError: Old embeddings are of type <class 'paddle.distributed.fleet.layers.mpu.mp_layers.VocabParallelEmbedding'>, which is not an instance of <class 'paddle.nn.layer.common.Embedding'>. You should either use a different resize function or make sure that old_embeddings are an instance of <class 'paddle.nn.layer.common.Embedding'>.
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:32:16,646] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:32:16,646] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:32:16,646] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:32:16.648509 26338 tcp_utils.cc:130] Successfully connected to 10.174.138.221:48964
W0102 17:32:18.789083 26338 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:32:18.816248 26338 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:32:19,813] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:32:20,280] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:32:20,281] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:32:20,818] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:32:21,773] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:32:21,774] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:32:21,775] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - [0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - [0m
[33m[2024-01-02 17:32:21,778] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:32:21,789] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:32:21,789] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:32:21,820] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,821] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,845] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,845] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:32:21,874] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,875] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,910] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:32:21,910] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12101.28it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:08,  2.09s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.08s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.06s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.04s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.87s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.96s/it]
[32m[2024-01-02 17:32:36,244] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:32:36,244] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:32:36,283] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:32:36,284] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1328, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1407, in _get_resized_embeddings
    len(new_tokenizer),
NameError: name 'new_tokenizer' is not defined
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:33:51,081] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:33:51,081] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:33:51,081] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:33:51.083433 27317 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35364 while the server is not yet listening.
I0102 17:33:54.083640 27317 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35364
W0102 17:33:55.920305 27317 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:33:55.950609 27317 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:33:57,078] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:33:57,421] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:33:57,422] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:33:57,726] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:33:58,731] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:33:58,733] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:33:58,734] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - [0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - [0m
[33m[2024-01-02 17:33:58,739] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:33:58,740] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,740] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:33:58,752] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:33:58,752] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:33:58,791] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,793] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,823] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,823] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:33:58,855] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,856] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,893] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:33:58,893] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:33:58,896] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11293.23it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:08,  2.20s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.17s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.13s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.18s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.98s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.06s/it]
[32m[2024-01-02 17:34:14,022] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:34:14,022] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:34:14,068] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:34:14,069] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
> /root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py(1406)_get_resized_embeddings()
-> if self.config.tensor_parallel_degree > 1:
(Pdb) (Pdb) 2
(Pdb) 2
(Pdb) 2
(Pdb) Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1328, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1406, in _get_resized_embeddings
    if self.config.tensor_parallel_degree > 1:
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1406, in _get_resized_embeddings
    if self.config.tensor_parallel_degree > 1:
  File "/usr/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:07:21,382] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:07:21,382] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:07:21,382] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:07:21.384343 50207 tcp_utils.cc:130] Successfully connected to 10.174.138.221:51334
W0102 18:07:26.272878 50207 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:07:26.297065 50207 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:07:29,410] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:07:29,861] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:07:29,862] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:07:30,179] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:07:31,050] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:07:31,050] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:07:31,051] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - [0m
[33m[2024-01-02 18:07:31,055] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:07:31,066] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:07:31,066] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:07:31,113] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,115] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,143] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,143] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:07:31,221] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,222] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,273] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:07:31,274] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11922.41it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:08,  2.21s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.26s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.17s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.13s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.97s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.07s/it]
[32m[2024-01-02 18:07:46,263] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:07:46,263] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:07:46,305] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:07:46,305] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:07:46,327] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:07:46,351] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:07:46,351] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:07:46,352] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:07:46,597] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:07:46,660] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:07:53,221] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:07:53,229] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-02 18:07:53,230] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:53,297] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:07:53,297] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-07-21_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - [0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:07:53,336] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,374] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,460] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,550] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,603] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:07:53,654] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,655] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:08:18,311] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:08:18,311] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:08:18,311] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:08:18.313167 51284 tcp_utils.cc:107] Retry to connect to 10.174.138.221:41212 while the server is not yet listening.
I0102 18:08:21.313421 51284 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41212
W0102 18:08:23.316722 51284 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:08:23.342082 51284 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:08:24,645] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:08:25,178] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:08:25,180] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:08:25,735] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:08:27,117] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:08:27,119] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:08:27,121] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - [0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:08:27,129] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:08:27,129] [    INFO][0m - [0m
[33m[2024-01-02 18:08:27,129] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:08:27,130] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,131] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:08:27,150] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:08:27,150] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:08:27,247] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,249] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,277] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,278] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:08:27,303] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,304] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,346] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:08:27,347] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 8890.00it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:08,  2.17s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.17s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.13s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.12s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.94s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.03s/it]
[32m[2024-01-02 18:08:42,339] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:08:42,339] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:08:42,374] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:08:42,375] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:08:42,396] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:08:42,418] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:08:42,419] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:08:42,419] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:08:42,662] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:08:42,725] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:08:43,736] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:08:43,742] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-02 18:08:43,742] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:08:43,748] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-02 18:08:43,748] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:43,817] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:08:43,818] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-08-18_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - [0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:08:43,855] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:43,894] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:43,979] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:08:44,031] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,032] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:44,070] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:44,123] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:08:44,175] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,176] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:08:44,176] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:08:44,180] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:08:44,194] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:09:09,712] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:09:13,917] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:09:53,490] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:09:53,490] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:09:53,490] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0102 18:09:53.492327 52876 tcp_utils.cc:107] Retry to connect to 10.174.138.221:61359 while the server is not yet listening.
I0102 18:09:56.492533 52876 tcp_utils.cc:130] Successfully connected to 10.174.138.221:61359
W0102 18:10:00.126271 52876 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:00.150480 52876 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:10:01,064] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:10:01,475] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:01,476] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:01,921] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:02,972] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:02,973] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:02,974] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - [0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - [0m
[33m[2024-01-02 18:10:02,978] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:10:02,979] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:02,980] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:02,991] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:02,991] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:03,028] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,029] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,055] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,055] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:03,082] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,084] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,124] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:03,124] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:03,127] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11348.23it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:09,  2.28s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:07,  2.34s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.28s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:09<00:02,  2.28s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.08s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.18s/it]
[32m[2024-01-02 18:10:18,550] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:18,551] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:18,593] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:18,594] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:18,615] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:18,634] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:18,635] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:18,635] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:18,876] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:18,937] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:19,222] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:19,231] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-02 18:10:19,231] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:19,299] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:19,299] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-09-53_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - [0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:10:19,343] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,381] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,501] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:10:19,553] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,553] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,590] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,643] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:19,695] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,696] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:10:34,847] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:10:34,848] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:10:34,848] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:10:34.849979 53317 tcp_utils.cc:107] Retry to connect to 10.174.138.221:47036 while the server is not yet listening.
I0102 18:10:37.850119 53317 tcp_utils.cc:130] Successfully connected to 10.174.138.221:47036
W0102 18:10:40.361227 53317 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:40.386322 53317 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:10:41,374] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:10:41,841] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:41,843] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:42,357] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:43,224] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:43,226] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:43,227] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - [0m
[33m[2024-01-02 18:10:43,231] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:43,289] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,291] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,317] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,318] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:43,396] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,397] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,446] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:43,447] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10195.20it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:08,  2.19s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.22s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.19s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.16s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.97s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.07s/it]
[32m[2024-01-02 18:10:58,302] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:58,302] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:58,343] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:58,343] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:58,365] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:58,640] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:58,702] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:59,298] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:59,302] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-02 18:10:59,303] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:59,310] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-02 18:10:59,310] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:59,378] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:59,378] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-10-34_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - [0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:10:59,415] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,455] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,540] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:10:59,591] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,591] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,628] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,681] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:59,732] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,733] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:10:59,733] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:10:59,737] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:10:59,756] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:11:24,771] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:11:24,775] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:11:24,775] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:11:27,588] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:26:52,723] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:26:52,724] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:26:52,724] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:26:52.725773 62338 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55969 while the server is not yet listening.
I0102 18:26:55.725978 62338 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55969
W0102 18:26:57.756916 62338 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:26:57.782856 62338 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:26:58,810] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:26:59,796] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:27:00,705] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:27:00,707] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - [0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - [0m
[33m[2024-01-02 18:27:00,713] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:27:00,714] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,714] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:27:00,726] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:27:00,726] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:27:00,776] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,778] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,804] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,805] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:27:00,834] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,834] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,870] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:27:00,870] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11096.04it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████▊                                                                                       | 1/5 [00:02<00:09,  2.33s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████▌                                                                 | 2/5 [00:04<00:06,  2.25s/it]Loading checkpoint shards:  60%|█████████████████████████████████████████████████████████████████▍                                           | 3/5 [00:06<00:04,  2.20s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████▏                     | 4/5 [00:08<00:02,  2.22s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.99s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.10s/it]
[32m[2024-01-02 18:27:16,157] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:27:16,157] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:27:16,200] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:27:16,201] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:27:16,220] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 529, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1351, in resize_token_embeddings
    self.model.lm_head.weight = new_lm_head_weight
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1474, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'LlamaForCausalLM' object has no attribute 'model'
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:03:44,876] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:03:44,877] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:03:44,877] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:03:44.878561  7618 tcp_utils.cc:130] Successfully connected to 10.174.138.221:54784
W0104 21:03:49.816864  7618 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:03:49.843730  7618 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:03:53,258] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:03:54,423] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:03:56,336] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:03:56,337] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:03:56,338] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - [0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - [0m
[33m[2024-01-04 21:03:56,342] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:03:56,354] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:03:56,354] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:03:56,423] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,425] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,452] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,453] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:03:56,495] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,496] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,535] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:03:56,535] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11491.24it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.05s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:02<00:03,  1.02s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:03<00:02,  1.01s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:04<00:01,  1.00s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.01it/s]
[32m[2024-01-04 21:04:03,607] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:04:03,607] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:04:03,643] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:04:03,644] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:04:03,666] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:04:03,695] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:04:03,699] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:04:03,979] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:04:05,062] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:04:05,071] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:04:05,702] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:04:05,770] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:04:05,770] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-03-44_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - [0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:04:06,053] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:04:06,092] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:04:06,180] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:04:06,180] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:04:06,180] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:04:06,180] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:04:06,180] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:04:06,185] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:04:06,187] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:04:24,577] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:06:05,759] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:06:05,760] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:06:05,760] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:06:05.761461  9354 tcp_utils.cc:130] Successfully connected to 10.174.138.221:59733
W0104 21:06:07.978724  9354 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:06:08.002013  9354 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:06:10,140] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:06:13,291] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:06:16,486] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:06:16,487] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:06:16,488] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:06:16,490] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - [0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - [0m
[33m[2024-01-04 21:06:16,493] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,495] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:06:16,507] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:06:16,507] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:06:16,547] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,549] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,578] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,579] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:06:16,607] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,608] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,644] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:06:16,644] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,648] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,648] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12199.84it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.10s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:01<00:02,  1.06it/s]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:02<00:01,  1.10it/s]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.17it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.13it/s]
[32m[2024-01-04 21:06:23,311] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:06:23,312] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:06:23,359] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:06:23,359] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:06:23,377] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:06:23,390] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:06:23,391] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:06:23,673] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:06:28,144] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:06:28,149] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:06:28,149] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:06:28,153] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:06:28,654] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:28,726] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:06:28,726] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:06:28,752] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:28,752] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-06-05_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - [0m
[32m[2024-01-04 21:06:28,760] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:06:29,168] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:06:29,206] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:06:29,275] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:06:29,275] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:06:29,275] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:06:29,275] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:06:29,280] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:06:29,281] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:07:38,130] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:22:33,169] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:22:33,169] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:22:33,169] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:22:33.170857 19916 tcp_utils.cc:107] Retry to connect to 10.174.138.221:41794 while the server is not yet listening.
I0104 21:22:36.171025 19916 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41794
W0104 21:22:38.323936 19916 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:22:38.349954 19916 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:22:39,531] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:22:40,325] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:22:42,140] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:22:42,141] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:22:42,141] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:22:42,142] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - [0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - [0m
[33m[2024-01-04 21:22:42,145] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,146] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:22:42,155] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:22:42,156] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:22:42,191] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,192] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,217] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,217] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:22:42,244] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,245] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,279] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:22:42,279] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:22:42,282] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 8778.37it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:02<00:03,  1.05s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:03<00:01,  1.00it/s]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:04<00:01,  1.05s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.01it/s]
[32m[2024-01-04 21:22:49,200] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:22:49,200] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:22:49,239] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:22:49,240] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:22:49,258] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:22:49,270] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:22:49,551] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:22:49,991] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:22:50,000] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:22:50,577] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:50,650] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:22:50,650] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:22:50,732] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-22-33_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - [0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:22:51,011] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:22:51,050] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:22:51,146] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:22:51,147] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:22:51,147] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:22:51,151] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:22:51,152] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:23:59,332] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:27:55,443] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:27:55,443] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:27:55,444] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:27:55.445196 23878 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55680 while the server is not yet listening.
I0104 21:27:58.445365 23878 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55680
W0104 21:28:00.856276 23878 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:28:00.881598 23878 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:28:02,045] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:28:03,067] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:28:04,340] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:28:04,341] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:28:04,341] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - [0m
[33m[2024-01-04 21:28:04,345] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:28:04,355] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:28:04,355] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:28:04,399] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,400] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,427] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,427] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:28:04,454] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,455] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,500] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:28:04,500] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12314.46it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:02<00:03,  1.07s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:03<00:02,  1.15s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:04<00:01,  1.16s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.02s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.06s/it]
[32m[2024-01-04 21:28:11,921] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:28:11,921] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:28:11,959] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:28:11,960] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:28:11,978] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:28:12,001] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:28:12,004] [    INFO][0m - -------------------------------------new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:28:12,291] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:28:12,566] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:28:12,575] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:28:13,369] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:13,441] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:28:13,441] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:28:14,419] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-27-55_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - [0m
[32m[2024-01-04 21:28:14,427] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:28:14,433] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:28:14,473] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:28:14,621] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:28:14,621] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:28:14,622] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:28:14,626] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:28:14,629] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:29:23,341] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:34:45,664] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:34:45,664] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:34:45,664] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:34:45.666298 28841 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63421 while the server is not yet listening.
I0104 21:34:48.666545 28841 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63421
W0104 21:34:50.896246 28841 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:34:50.922645 28841 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:34:52,154] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:34:53,142] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:34:55,344] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:34:55,346] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:34:55,347] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - [0m
[33m[2024-01-04 21:34:55,352] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:34:55,353] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,354] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:34:55,367] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:34:55,367] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:34:55,457] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,460] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,486] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,486] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:34:55,517] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,517] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,557] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:34:55,558] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10749.11it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.10s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:01<00:02,  1.05it/s]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:02<00:01,  1.12it/s]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.27it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.18it/s]
[32m[2024-01-04 21:35:01,943] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:35:01,943] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:35:01,989] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:35:01,990] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:35:02,007] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:35:02,315] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:35:03,499] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:35:03,504] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:35:03,505] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:35:03,509] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:35:04,031] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:35:04,102] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:35:04,102] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-34-45_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - [0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:35:04,458] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:35:04,496] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:35:04,589] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:35:04,589] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:35:04,589] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:35:04,593] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:35:04,594] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:36:10,868] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:36:10,868] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:36:10,868] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:36:10.869936 30161 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35737 while the server is not yet listening.
I0104 21:36:13.870103 30161 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35737
W0104 21:36:16.802392 30161 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:36:16.828914 30161 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:36:17,882] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:36:18,914] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:36:18,915] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:36:18,916] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:36:20,314] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:36:20,315] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - [0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - [0m
[33m[2024-01-04 21:36:20,321] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:36:20,322] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,323] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:36:20,334] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:36:20,334] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:36:20,373] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,375] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,401] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,401] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:36:20,479] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,480] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,515] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:36:20,516] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10934.06it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:04,  1.09s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:02<00:03,  1.09s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:03<00:02,  1.08s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:04<00:01,  1.04s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.02it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.02s/it]
[32m[2024-01-04 21:36:27,592] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:36:27,592] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:36:27,632] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:36:27,632] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:36:27,654] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:36:27,686] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:36:27,687] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:36:27,687] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:36:27,958] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:36:34,497] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:36:34,506] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-04 21:36:35,398] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:35,470] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:36:35,470] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-36-10_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - [0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:36:35,549] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:36:35,588] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:36:35,753] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:36:35,753] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:36:35,753] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 128[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total num train samples = 256,000[0m
[32m[2024-01-04 21:36:35,757] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:36:35,758] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:36:46,685] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-04 21:36:48,557] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-04 21:36:50,140] [    INFO][0m - ret is None[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:37:48,073] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:37:48,073] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:37:48,073] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0104 21:37:48.074854 31399 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63978 while the server is not yet listening.
I0104 21:37:51.075101 31399 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63978
W0104 21:37:53.370215 31399 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:37:53.397647 31399 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:37:55,342] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-04 21:37:57,769] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:37:59,330] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [2], pp_group: [2], dp_group: [0, 2], check/clip group: [2, 3]
[32m[2024-01-04 21:37:59,332] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:37:59,333] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - [0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - [0m
[33m[2024-01-04 21:37:59,338] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:37:59,339] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,339] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:37:59,352] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:37:59,352] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:37:59,390] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,391] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,423] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,423] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:37:59,450] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,451] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,484] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:37:59,484] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 11841.63it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:01<00:07,  1.97s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:03<00:05,  1.97s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:05<00:03,  1.97s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:07<00:01,  1.95s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.78s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.86s/it]
[32m[2024-01-04 21:38:13,459] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:38:13,459] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:38:13,498] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:38:13,498] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:38:13,517] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:38:14,053] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:38:14,054] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:38:14,054] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:38:14,324] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:38:19,280] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:38:19,285] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:38:19,285] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:38:19,290] [    INFO][0m - 岁 他九十余年的人生里笔耕不辍，留下三千余首脍炙人口的歌词；他生于北京，盛名于台湾，将晚年生活留给了他的第二故乡——重庆；他当过记者、演过话剧、最后却在音乐创作上才华尽展……忘不了，甜蜜蜜，垄上行。
丝丝小雨里，几时再见，小城故事。
今天凌晨6点11分，我国华语歌曲“词坛泰斗”庄奴在重庆与世长辞，享年95岁。
一生写下3000多首歌词《甜蜜蜜》已成绝唱 庄奴老先生这一生当过记者、编辑，演过话剧，可就连他自己也没料到，一次闲暇时光里所写下的《绿岛小夜曲》，却让他以词人的身份一夜成名，从此副业变正行，并为此笔耕不辍了五十余载。
说到庄奴所作的歌词，不得不提到蜚声世界的甜歌皇后邓丽君。
上世纪六七十年代，台湾著名歌星邓丽君唱着《甜蜜蜜》、《小城故事》、《又见炊烟》走遍了世界，这些至今仍在人们口中传唱的歌词，全部出自庄奴之手。
随着邓丽君的歌声传遍世界，写下动人歌词的庄奴也被更多的人所熟知，他所作的每句歌词都充满温情，温暖着每一个人的心灵。
歌词虽暖，可在庄奴自己看来，写词的生活，却是带着几分清苦之意，他曾自己写过一首打油诗，打趣写词生涯——“半杯苦茶半杯酒，半句歌词写半天；半夜三更两点半，半睡半醒半无眠……”从偶一结缘到如今，庄老已创作五十载，作品超三千篇，人们都赞其为“与时间赛跑”，而他将自己一生的创作总结为“行云流水五十年，吟风弄月歌三千”“歌词不能太长、太难，我们是为千千万万普通人写歌，要简单易懂，又要传情达意，要写出他们的心声”，庄奴说，很多人说他的歌写出了想说又说不出来的话，就是这么个意思。
甜蜜蜜甜蜜蜜 你笑得甜蜜蜜好象花儿开在春风里开在春风里在哪里 在哪里见过你你的笑容这样熟悉我一时想不起啊 在[0m
[32m[2024-01-04 21:38:20,881] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:38:20,955] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:38:20,955] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:38:20,989] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:38:20,989] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-37-48_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - optimizer_name_suffix         : tp00[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - [0m
[32m[2024-01-04 21:38:20,997] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:38:21,116] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:38:21,155] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:38:21,273] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:38:21,273] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:38:21,273] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:38:21,278] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:38:21,313] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:38:34,592] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-04 21:38:58,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:39:40,921] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:39:40,922] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:39:40,922] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:39:40.923679 32985 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35415
W0104 21:39:45.811290 32985 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:39:45.835819 32985 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:39:48,386] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-04 21:39:49,020] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:39:50,932] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [2], pp_group: [2], dp_group: [0, 2], check/clip group: [2, 3]
[32m[2024-01-04 21:39:50,933] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:39:50,933] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - [0m
[33m[2024-01-04 21:39:50,937] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:39:50,987] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:50,989] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,017] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,017] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:39:51,043] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,044] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,078] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:39:51,078] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,081] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,081] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 12115.26it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████████████████▊                                                                                               | 1/5 [00:02<00:08,  2.20s/it]Loading checkpoint shards:  40%|███████████████████████████████████████████████▌                                                                       | 2/5 [00:04<00:06,  2.22s/it]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████▍                                               | 3/5 [00:06<00:04,  2.16s/it]Loading checkpoint shards:  80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 4/5 [00:08<00:02,  2.12s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  1.94s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.04s/it]
[32m[2024-01-04 21:40:06,720] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:40:06,720] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:40:06,759] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:40:06,760] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:40:06,780] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:40:07,074] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:40:07,584] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:40:07,588] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-04 21:40:07,589] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:40:07,593] [    INFO][0m - 岁 他九十余年的人生里笔耕不辍，留下三千余首脍炙人口的歌词；他生于北京，盛名于台湾，将晚年生活留给了他的第二故乡——重庆；他当过记者、演过话剧、最后却在音乐创作上才华尽展……忘不了，甜蜜蜜，垄上行。
丝丝小雨里，几时再见，小城故事。
今天凌晨6点11分，我国华语歌曲“词坛泰斗”庄奴在重庆与世长辞，享年95岁。
一生写下3000多首歌词《甜蜜蜜》已成绝唱 庄奴老先生这一生当过记者、编辑，演过话剧，可就连他自己也没料到，一次闲暇时光里所写下的《绿岛小夜曲》，却让他以词人的身份一夜成名，从此副业变正行，并为此笔耕不辍了五十余载。
说到庄奴所作的歌词，不得不提到蜚声世界的甜歌皇后邓丽君。
上世纪六七十年代，台湾著名歌星邓丽君唱着《甜蜜蜜》、《小城故事》、《又见炊烟》走遍了世界，这些至今仍在人们口中传唱的歌词，全部出自庄奴之手。
随着邓丽君的歌声传遍世界，写下动人歌词的庄奴也被更多的人所熟知，他所作的每句歌词都充满温情，温暖着每一个人的心灵。
歌词虽暖，可在庄奴自己看来，写词的生活，却是带着几分清苦之意，他曾自己写过一首打油诗，打趣写词生涯——“半杯苦茶半杯酒，半句歌词写半天；半夜三更两点半，半睡半醒半无眠……”从偶一结缘到如今，庄老已创作五十载，作品超三千篇，人们都赞其为“与时间赛跑”，而他将自己一生的创作总结为“行云流水五十年，吟风弄月歌三千”“歌词不能太长、太难，我们是为千千万万普通人写歌，要简单易懂，又要传情达意，要写出他们的心声”，庄奴说，很多人说他的歌写出了想说又说不出来的话，就是这么个意思。
甜蜜蜜甜蜜蜜 你笑得甜蜜蜜好象花儿开在春风里开在春风里在哪里 在哪里见过你你的笑容这样熟悉我一时想不起啊 在[0m
[32m[2024-01-04 21:40:09,231] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:40:09,301] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:40:09,301] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-39-40_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - optimizer_name_suffix         : tp00[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - [0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:40:11,077] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:40:11,116] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:40:11,401] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:40:11,401] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:40:11,402] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:40:11,407] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:40:11,440] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:40:26,823] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:40:26,830] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:02:57,788] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:02:57,788] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:02:57,788] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:02:57.789602 110466 tcp_utils.cc:107] Retry to connect to 10.174.138.221:46099 while the server is not yet listening.
I0105 13:03:00.789805 110466 tcp_utils.cc:130] Successfully connected to 10.174.138.221:46099
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 395, in main
    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/argparser.py", line 246, in parse_json_file
    obj = dtype(**inputs)
  File "<string>", line 95, in __init__
  File "run_pretrain.py", line 86, in __post_init__
    super().__post_init__()
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py", line 1122, in __post_init__
    fleet.init(is_collective=True, strategy=strategy)
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/fleet/fleet.py", line 305, in init
    paddle.distributed.init_parallel_env()
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::distributed::ProcessGroupNCCL::Barrier(paddle::distributed::BarrierOptions const&)
1   phi::DenseTensor::DenseTensor(phi::Allocator*, phi::DenseTensorMeta const&)
2   paddle::experimental::DefaultAllocator::Allocate(unsigned long)
3   phi::memory_utils::Alloc(phi::Place const&, unsigned long)
4   phi::MemoryUtils::Alloc(phi::Place const&, unsigned long)
5   paddle::memory::Alloc(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 6. Cannot allocate 256.000000B memory on GPU 6, 127.997530TB memory has been allocated and available memory is only 0.000000B.

Please check whether there is any other process using GPU 6.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:14:24,957] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:14:24,957] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:14:24,957] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:14:24.958945 49445 tcp_utils.cc:130] Successfully connected to 10.174.138.221:43579
W0105 13:14:29.718417 49445 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:14:29.742635 49445 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:14:30,743] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:14:31,379] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:14:32,890] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:14:32,892] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:14:32,893] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:14:32,894] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - [0m
[33m[2024-01-05 13:14:32,897] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:14:32,898] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,920] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,921] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:14:32,922] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,923] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:14:47,394] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:14:55,254] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|████████████████████████████████████████████████████▌                                                    | 1/2 [00:21<00:21, 21.47s/it][32m[2024-01-05 13:15:00,684] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:15:03,176] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 13.11s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.36s/it]
[32m[2024-01-05 13:15:03,686] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[33m[2024-01-05 13:15:03,686] [ WARNING][0m - Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path and are newly initialized: ['lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:15:03,960] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:15:06,317] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:15:06,323] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-05 13:15:06,323] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:15:06,328] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-05 13:15:07,344] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-14-24_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - [0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:15:07,503] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:15:07,542] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:15:07,655] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:15:07,655] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:15:07,655] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:15:07,660] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:15:07,661] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:16:25,626] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:16:25,630] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:16:27,850] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:17:36,993] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:17:36,993] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:17:36,994] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:17:36.995771 52409 tcp_utils.cc:107] Retry to connect to 10.174.138.221:38381 while the server is not yet listening.
I0105 13:17:39.996007 52409 tcp_utils.cc:130] Successfully connected to 10.174.138.221:38381
W0105 13:17:44.980296 52409 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:17:45.007508 52409 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:17:46,928] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:17:48,083] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:17:48,083] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:17:48,084] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - [0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - [0m
[33m[2024-01-05 13:17:48,090] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-05 13:17:48,092] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,125] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,125] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:17:48,128] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,129] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:38:43,627] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:38:43,627] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:38:43,628] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:38:43.629176 71802 tcp_utils.cc:107] Retry to connect to 10.174.138.221:53539 while the server is not yet listening.
I0105 13:38:46.629343 71802 tcp_utils.cc:130] Successfully connected to 10.174.138.221:53539
W0105 13:38:48.650488 71802 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:38:48.677554 71802 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:38:50,050] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:38:51,165] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:38:52,587] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:38:52,587] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:38:52,588] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - [0m
[33m[2024-01-05 13:38:52,591] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,614] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,615] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:38:52,616] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,617] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:39:06,106] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:13,762] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|████████████████████████████████████████████████████▌                                                    | 1/2 [00:19<00:19, 19.91s/it][32m[2024-01-05 13:39:19,518] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:22,592] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 13.07s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.10s/it]
[32m[2024-01-05 13:39:23,129] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:39:23,130] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:39:23,131] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:39:23,131] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:39:23,132] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:39:23,132] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:39:23,407] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:39:24,093] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:39:24,102] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-05 13:39:25,076] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:39:25,193] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:39:25,193] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:39:25,232] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-38-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - [0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:39:25,258] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:39:25,296] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:39:25,375] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:39:25,375] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:39:25,375] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:39:25,379] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:39:25,381] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:40:33,582] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:40:35,464] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:41:37,833] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:41:37,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:41:37,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:41:37.835350 73845 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63585 while the server is not yet listening.
I0105 13:41:40.835592 73845 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63585
W0105 13:41:42.851826 73845 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:41:42.880256 73845 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:41:43,858] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:41:44,656] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:41:45,959] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:41:45,960] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:41:45,960] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:41:45,961] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - [0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - [0m
[33m[2024-01-05 13:41:45,964] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,986] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,987] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:41:45,989] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,989] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:41:59,902] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:07,758] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|████████████████████████████████████████████████████▌                                                    | 1/2 [00:20<00:20, 20.98s/it][32m[2024-01-05 13:42:13,555] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:16,364] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 13.29s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.44s/it]
[32m[2024-01-05 13:42:16,883] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:42:16,883] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:42:16,884] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:42:17,160] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:42:21,524] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:42:21,529] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-05 13:42:21,530] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:42:21,534] [    INFO][0m - 每一时刻只含有约15皮克的氡。
溶液所含的镭-226经α衰变，半衰期为1600年。
氡是其中一种衰变产物，其累积速率约为1 mm每天每1克镭。
衰变迅速达致平衡，溶液会不断产生新的氡，其放射性将与镭相同（50 Bq）。
气态Rn（半衰期约为4天）会扩散并逃逸出容器。
UNSCEAR 1993年报告中采用的氡-222的剂量转换因子为9nSv/(Bq/m)；平衡因子（氡与其短寿命子体平衡态的度量）室内取0.4，室外取0.6；居留因子室内取0.8，室外取0.2。
导出氡-222的年有效剂量：
20世纪初，庸医曾利用氡来治疗各种疾病。
病人在密封的小房间内接触氡，以获取「治疗功效」。
很快人们便发现，氡的致电离辐射能够致癌。
虽然氡的放射性可以杀死癌细胞，但它对健康细胞同样有损害。
致电离辐射会导致自由基的形成，进而在细胞及基因上造成更大的伤害，甚至会引发癌症。
曾有人提出用氡的辐射激效来治疗关节炎等自体免疫性疾病。
20世纪末至21世纪初，美国蒙大拿州杰佛逊县的一些「健康矿井」吸引了不少渴望消除关节炎等疾病的人来饮用放射性井水和暴露在氡气之中。
然而因为高剂量辐射会对身体产生负面影响，所以这一疗法并不受到医生的鼓励。
捷克亚希莫夫自1906年起便有使用放射性水浴，而奥地利巴特加斯泰因则在氡被发现之前就已有放射性水浴的使用。
日本鸟取县三朝町也有富含镭的温泉。
德国巴德布兰巴赫则有饮用放射性水的疗法。
奥地利加斯泰纳-海尔施多兰（Gasteiner-Heilstollen）、波兰希维拉杜夫-兹德鲁伊、切尔涅瓦-兹德鲁伊（Czerniawa-Zdrój）、科瓦雷、隆代克-兹德鲁[0m
[32m[2024-01-05 13:42:22,548] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:42:22,627] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:42:22,627] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:42:22,659] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:42:22,659] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-41-37_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:42:22,673] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:42:22,711] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:42:22,793] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:42:22,794] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:42:22,794] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:42:22,798] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:42:22,799] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:43:32,924] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:43:32,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:43:34,820] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:43:57,123] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:43:57,123] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:43:57,124] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:43:57.125466 75816 tcp_utils.cc:130] Successfully connected to 10.174.138.221:62346
W0105 13:43:59.260133 75816 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:43:59.290457 75816 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:44:00,453] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:44:01,228] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:44:01,824] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:44:01,824] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:44:01,825] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - [0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - [0m
[33m[2024-01-05 13:44:01,831] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,867] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,867] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:44:01,870] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,870] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|████████████████████████████████████████████████████▌                                                    | 1/2 [00:21<00:21, 21.19s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 13.40s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.56s/it]
[32m[2024-01-05 13:44:42,671] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:44:42,671] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:44:42,905] [    INFO][0m - Frozen parameters: 6.48e+09 || Trainable parameters:5.10e+08 || Total parameters:6.99e+09|| Trainable:7.30%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:44:46,115] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:44:46,120] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-05 13:44:46,121] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:44:46,126] [    INFO][0m -  （光害）也越来越多。
这些人造的灯光照亮夜晚的天空，加上人类活动造成的各种空气污染引起的灰尘散射灯光的双重效应，使得背景天空变得灰暗，对光度微弱的天体观测越加困难，必须要特别的滤光镜来隔绝背景光的干扰。
在有些地区，如美国亚利桑那州、英国和日本与香港，都有民间团体在研究与发起以减少光污染。
鼓励为街灯装上反射罩，不仅能使照向地面的灯光增加，也使直接射进天空的光量降低。
但因与城市的经济发展相违背，对于推广意识的淡薄与对科学的不重视之下，尤其在发展中国家，这样的状况蚕食一些古老的天文台至不再能作观测。
如北京古观象台与南京紫金山天文台等。</s> 有那些过誉或不实用的文具？
就算再多人推荐我也要说.. 百乐V5中性笔！
之前看到的所有评价都说这是最好写的中性笔什么的. 但是可能是水性笔的关系，这笔用在稍薄一点的纸上都会透到背面，一般70g以下的纸写在正面，背面会透出完全镜像的字，自己做笔记用好一点的纸V5写起来很不错，但是课本试卷什么的真的透得很严重. 还有一个就是daycraft的本子，设计方面见仁见智，但是用钢笔容易洇，试过4001的墨和百乐的墨，写在上面都会向四周辐射一样散开来，F尖的都</s>IT大牛是怎样炼成的？
公司来了个IT大牛，92年出生的人，现在是我们的team leader。
是我见过的程序员里最厉害的一个，不知道他的能力是怎么炼成的。
说说他的事迹吧。
他高中的时候，就接触编程（VB6），他一直在想，设计一个算法和自己对弈（象棋），看看是自己设计的算法厉害，还是自己厉害。
他自己独立做，没有参考任何资料，花了半个学期，竟然写出来了。
他自己动手做了一台斯特林发动机，设计了一台酒精灯带动的缝纫机原型。
他家是农村的，他爸爸开货车，车挂被偷了。
他就用继电器做了一个报警器，如果偷车贼偷走车挂，就会弄断铜线，触发继电器报警。
农村的建筑工人[0m
[32m[2024-01-05 13:44:49,569] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:49,637] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:44:49,637] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataset_rank                  : 2[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-43-57_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - optimizer_name_suffix         : shard02[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_rank        : 2[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_degree        : 1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - weight_name_suffix            : [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:44:49,714] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-01-05 13:44:49,901] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-01-05 13:44:49,902] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:44:49,903] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:44:49,908] [    INFO][0m -   Number of trainable parameters = 509,804,544 (per device)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:45:22,946] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Num examples = 920176[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total Batch size = 32[0m
[32m[2024-01-05 13:45:27,158] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:45:43,661] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:45:43,661] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:45:43,661] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:45:43.663062 77264 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55899 while the server is not yet listening.
I0105 13:45:46.663247 77264 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55899
W0105 13:45:48.742448 77264 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:45:48.769078 77264 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:45:50,278] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-05 13:45:50,818] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-05 13:45:50,819] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:45:51,208] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-05 13:45:52,261] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:45:52,262] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:45:52,263] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - [0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - [0m
[33m[2024-01-05 13:45:52,268] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:45:52,269] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,290] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,291] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:45:52,293] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,294] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:46:08,314] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:15,472] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|████████████████████████████████████████████████████▌                                                    | 1/2 [00:21<00:21, 21.07s/it][32m[2024-01-05 13:46:23,151] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:26,266] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 13.91s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.98s/it]
[32m[2024-01-05 13:46:27,583] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:46:27,583] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:46:27,859] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:46:30,440] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - 颜色搭配技巧之卧室卧室是休息、放松的空间，怎么样来提高睡眠是卧室色彩搭配的关键。
卧室顶部多用白色，白色和光滑的墙面使光的反射率较高，增加明亮程度。
墙壁可用明亮并且宁静的色彩，黄、黄灰色等浅色能够增加房间的开阔感。
室内地面一般采用深色，地面的色彩不要和家具的色彩太接近，否则影响家庭原本良好的风水和设计立体感。
如果选择壁纸来装饰的话，炎热的夏天宜用冷色调、浅褐、浅紫罗兰、浅苹果绿、湖蓝色、蓝色，有宁静、凉爽、恬适之感；而寒冷的冬季宜用红栗、 奶黄、米黄色、咖啡色等暖色系，给人以温暖、开朗、活泼、充实的感觉。
室内风水设计颜色搭配技巧之客厅客厅的颜色搭配主要取决于客厅窗户的朝向，如南向的客厅应以白色作为主色调，而西向客厅应以绿色作为主色调。
这是因为，南方五行属火，按五行生克理论，火克金为财，要保证南向客厅的财气，选用的油漆、墙纸及沙发均宜以白色为首选，因为白色是“金”的代表色，属迎财之色。
西方五行属金，金克木为财，即是说木乃金之财，而绿色是“木”的代表色，并且向西的客厅下午西照的阳光甚为强烈，不但酷热而且刺眼，所以用较清淡而又可护目养眼的绿色，方为适宜。
客厅的家具配置主要包括：沙发、茶几、组合柜和客厅饰物等。
客厅方位朝向的不同，对客厅家具的色彩搭配要求也不一样，如西向客厅，广州玄易老师建议设计师在进行客厅搭配时应以绿色作为主色调来选配沙发、茶几及组合柜的颜色，再决定家具的款式、规格的配置。
室内风水设计颜色搭配技巧之书房书房的色调力求雅致。
书房是人们用于阅读、书写和学习的一种静态工作空间，人们要求头脑冷静、注意力集中、安宁。
室内色调要求以雅致、庄重、和谐为主色调，单从色瓷上来考虑，广州玄易老师建议设计师们宜选用灰色、褐灰色、褐绿色、浅蓝色、浅绿色等，地面[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:46:30,450] [    INFO][0m - 岁 他九十余年的人生里笔耕不辍，留下三千余首脍炙人口的歌词；他生于北京，盛名于台湾，将晚年生活留给了他的第二故乡——重庆；他当过记者、演过话剧、最后却在音乐创作上才华尽展……忘不了，甜蜜蜜，垄上行。
丝丝小雨里，几时再见，小城故事。
今天凌晨6点11分，我国华语歌曲“词坛泰斗”庄奴在重庆与世长辞，享年95岁。
一生写下3000多首歌词《甜蜜蜜》已成绝唱 庄奴老先生这一生当过记者、编辑，演过话剧，可就连他自己也没料到，一次闲暇时光里所写下的《绿岛小夜曲》，却让他以词人的身份一夜成名，从此副业变正行，并为此笔耕不辍了五十余载。
说到庄奴所作的歌词，不得不提到蜚声世界的甜歌皇后邓丽君。
上世纪六七十年代，台湾著名歌星邓丽君唱着《甜蜜蜜》、《小城故事》、《又见炊烟》走遍了世界，这些至今仍在人们口中传唱的歌词，全部出自庄奴之手。
随着邓丽君的歌声传遍世界，写下动人歌词的庄奴也被更多的人所熟知，他所作的每句歌词都充满温情，温暖着每一个人的心灵。
歌词虽暖，可在庄奴自己看来，写词的生活，却是带着几分清苦之意，他曾自己写过一首打油诗，打趣写词生涯——“半杯苦茶半杯酒，半句歌词写半天；半夜三更两点半，半睡半醒半无眠……”从偶一结缘到如今，庄老已创作五十载，作品超三千篇，人们都赞其为“与时间赛跑”，而他将自己一生的创作总结为“行云流水五十年，吟风弄月歌三千”“歌词不能太长、太难，我们是为千千万万普通人写歌，要简单易懂，又要传情达意，要写出他们的心声”，庄奴说，很多人说他的歌写出了想说又说不出来的话，就是这么个意思。
甜蜜蜜甜蜜蜜 你笑得甜蜜蜜好象花儿开在春风里开在春风里在哪里 在哪里见过你你的笑容这样熟悉我一时想不起啊 在[0m
[32m[2024-01-05 13:46:32,008] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:46:32,076] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:46:32,076] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - gradient_accumulation_steps   : 128[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-45-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:46:32,112] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,150] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,233] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:46:32,291] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,291] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,330] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,383] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-05 13:46:32,435] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,436] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:46:32,436] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Gradient Accumulation steps = 128[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-05 13:46:32,450] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:47:20,138] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-05 13:47:22,865] [    INFO][0m - Saving optimizer files.[0m
