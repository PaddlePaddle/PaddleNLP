/root/paddlejob/workspace/zhengxiong/pd_zx/bin/python: can't open file './llm/run_pretrain.py': [Errno 2] No such file or directory
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:18:02,614] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:18:02,615] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:18:02,615] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:18:02.616866 17818 tcp_utils.cc:107] Retry to connect to 10.174.138.221:57868 while the server is not yet listening.
I0102 17:18:05.617058 17818 tcp_utils.cc:130] Successfully connected to 10.174.138.221:57868
W0102 17:18:07.641006 17818 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:18:07.668571 17818 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:18:08,753] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:18:09,185] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:18:09,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:18:11,430] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:18:14,976] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:18:14,976] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:18:14,977] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,978] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:18:14,979] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:18:14,980] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - [0m
[33m[2024-01-02 17:18:14,981] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:14,981] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:18:14,991] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:18:14,992] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:18:15,076] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,077] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,103] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,104] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:18:15,134] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:18:15,135] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:18:15,182] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:18:15,182] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:18:15,185] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 9827.33it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.43s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.30s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.23s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.16s/it]
[32m[2024-01-02 17:18:30,856] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:18:30,856] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:18:30,896] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:18:30,896] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1327, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1397, in _get_resized_embeddings
    raise TypeError(
TypeError: Old embeddings are of type <class 'paddle.distributed.fleet.layers.mpu.mp_layers.VocabParallelEmbedding'>, which is not an instance of <class 'paddle.nn.layer.common.Embedding'>. You should either use a different resize function or make sure that old_embeddings are an instance of <class 'paddle.nn.layer.common.Embedding'>.
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:32:16,646] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:32:16,646] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:32:16,646] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:32:16.648509 26338 tcp_utils.cc:130] Successfully connected to 10.174.138.221:48964
W0102 17:32:18.789083 26338 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:32:18.816248 26338 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:32:19,813] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:32:20,280] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:32:20,281] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:32:20,818] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:32:21,773] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:32:21,774] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:32:21,775] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:32:21,776] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:32:21,777] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - [0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:32:21,778] [    INFO][0m - [0m
[33m[2024-01-02 17:32:21,778] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,779] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:32:21,789] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:32:21,789] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:32:21,820] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,821] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,845] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,845] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:32:21,874] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:32:21,875] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:32:21,910] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:32:21,910] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:32:21,912] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12101.28it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.09s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.08s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.06s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.96s/it]
[32m[2024-01-02 17:32:36,244] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:32:36,244] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:32:36,283] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:32:36,284] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1328, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1407, in _get_resized_embeddings
    len(new_tokenizer),
NameError: name 'new_tokenizer' is not defined
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 17:33:51,081] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 17:33:51,081] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 17:33:51,081] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 17:33:51.083433 27317 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35364 while the server is not yet listening.
I0102 17:33:54.083640 27317 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35364
W0102 17:33:55.920305 27317 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 17:33:55.950609 27317 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 17:33:57,078] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 17:33:57,421] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 17:33:57,422] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 17:33:57,726] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 17:33:58,731] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 17:33:58,733] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 17:33:58,734] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,736] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 17:33:58,737] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - [0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - input_dir                     : /path/to/merged_tokenizer[0m
[32m[2024-01-02 17:33:58,738] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 17:33:58,739] [    INFO][0m - [0m
[33m[2024-01-02 17:33:58,739] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 17:33:58,740] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,740] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 17:33:58,752] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 17:33:58,752] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 17:33:58,791] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,793] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,823] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,823] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 17:33:58,855] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 17:33:58,856] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 17:33:58,893] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 17:33:58,893] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 17:33:58,896] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 17:33:58,897] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11293.23it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.20s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.17s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.13s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.06s/it]
[32m[2024-01-02 17:34:14,022] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 17:34:14,022] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 17:34:14,068] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 17:34:14,069] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
> /root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py(1406)_get_resized_embeddings()
-> if self.config.tensor_parallel_degree > 1:
(Pdb) (Pdb) 2
(Pdb) 2
(Pdb) 2
(Pdb) Traceback (most recent call last):
  File "run_pretrain.py", line 644, in <module>
    main()
  File "run_pretrain.py", line 532, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1328, in resize_token_embeddings
    new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1406, in _get_resized_embeddings
    if self.config.tensor_parallel_degree > 1:
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1406, in _get_resized_embeddings
    if self.config.tensor_parallel_degree > 1:
  File "/usr/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:07:21,382] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:07:21,382] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:07:21,382] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:07:21.384343 50207 tcp_utils.cc:130] Successfully connected to 10.174.138.221:51334
W0102 18:07:26.272878 50207 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:07:26.297065 50207 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:07:29,410] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:07:29,861] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:07:29,862] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:07:30,179] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:07:31,050] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:07:31,050] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:07:31,051] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:31,052] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:07:31,053] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:07:31,054] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - [0m
[33m[2024-01-02 18:07:31,055] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,055] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:07:31,066] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:07:31,066] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:07:31,113] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,115] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,143] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,143] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:07:31,221] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:07:31,222] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:07:31,273] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:07:31,274] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:07:31,276] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11922.41it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.21s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.26s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.17s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.07s/it]
[32m[2024-01-02 18:07:46,263] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:07:46,263] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:07:46,305] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:07:46,305] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:07:46,327] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:07:46,351] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:07:46,351] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:07:46,352] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:07:46,597] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:07:46,660] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:07:53,221] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:07:53,225] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:07:53,229] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:07:53,230] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:07:53,297] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:07:53,297] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:07:53,320] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:07:53,321] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:07:53,322] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-07-21_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:07:53,323] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:07:53,324] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:07:53,325] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:07:53,326] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - [0m
[32m[2024-01-02 18:07:53,327] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:07:53,336] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,374] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,460] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,512] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:07:53,550] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:07:53,603] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:07:53,654] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:07:53,655] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:08:18,311] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:08:18,311] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:08:18,311] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:08:18.313167 51284 tcp_utils.cc:107] Retry to connect to 10.174.138.221:41212 while the server is not yet listening.
I0102 18:08:21.313421 51284 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41212
W0102 18:08:23.316722 51284 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:08:23.342082 51284 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:08:24,645] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:08:25,178] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:08:25,180] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:08:25,735] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:08:27,117] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:08:27,119] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:08:27,121] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,124] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:08:27,125] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:08:27,126] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - [0m
[32m[2024-01-02 18:08:27,127] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:08:27,128] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:08:27,129] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:08:27,129] [    INFO][0m - [0m
[33m[2024-01-02 18:08:27,129] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:08:27,130] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,131] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:08:27,150] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:08:27,150] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:08:27,247] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,249] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,277] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,278] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:08:27,303] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:08:27,304] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:08:27,346] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:08:27,347] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:08:27,352] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 8890.00it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.17s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.17s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.13s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.03s/it]
[32m[2024-01-02 18:08:42,339] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:08:42,339] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:08:42,374] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:08:42,375] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:08:42,396] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:08:42,418] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:08:42,419] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:08:42,419] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:08:42,662] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:08:42,725] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:08:43,736] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:08:43,742] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:08:43,742] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:08:43,748] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:08:43,748] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:08:43,817] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:08:43,818] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:08:43,842] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:08:43,843] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:08:43,844] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-08-18_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,845] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:08:43,846] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:08:43,847] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:08:43,848] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - [0m
[32m[2024-01-02 18:08:43,849] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:08:43,855] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:43,894] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:43,979] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:08:44,031] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,032] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:08:44,070] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:08:44,123] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:08:44,175] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:08:44,176] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:08:44,176] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:08:44,176] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:08:44,177] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:08:44,180] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:08:44,194] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:09:09,712] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:09:09,716] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:09:13,917] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:09:53,490] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:09:53,490] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:09:53,490] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0102 18:09:53.492327 52876 tcp_utils.cc:107] Retry to connect to 10.174.138.221:61359 while the server is not yet listening.
I0102 18:09:56.492533 52876 tcp_utils.cc:130] Successfully connected to 10.174.138.221:61359
W0102 18:10:00.126271 52876 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:00.150480 52876 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:10:01,064] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:10:01,475] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:01,476] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:01,921] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:02,972] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:02,973] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:02,974] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:02,976] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:02,977] [    INFO][0m - [0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - [0m
[33m[2024-01-02 18:10:02,978] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:02,978] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:10:02,979] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:02,980] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:02,991] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:02,991] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:03,028] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,029] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,055] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,055] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:03,082] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:03,084] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:03,124] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:03,124] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:03,127] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:03,128] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11348.23it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.28s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:07,  2.34s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.28s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:09<00:02,  2.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.18s/it]
[32m[2024-01-02 18:10:18,550] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:18,551] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:18,593] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:18,594] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:18,615] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:18,634] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:18,635] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:18,635] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:18,876] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:18,937] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:19,222] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:10:19,227] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:19,231] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:10:19,231] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:19,299] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:19,299] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:19,328] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:19,329] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:19,330] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-09-53_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:19,331] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:19,332] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:19,333] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:19,334] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - [0m
[32m[2024-01-02 18:10:19,335] [    INFO][0m - Starting training from resume_from_checkpoint : ./checkpoints/checkpoint-1[0m
[2024-01-02 18:10:19,343] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,381] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,501] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:10:19,553] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,553] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:19,590] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:19,643] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:19,695] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:19,696] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 620, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 713, in train
    self._load_optimizer_and_scheduler(resume_from_checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2299, in _load_optimizer_and_scheduler
    if not use_unified_checkpoint:
UnboundLocalError: local variable 'use_unified_checkpoint' referenced before assignment
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:10:34,847] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:10:34,848] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:10:34,848] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:10:34.849979 53317 tcp_utils.cc:107] Retry to connect to 10.174.138.221:47036 while the server is not yet listening.
I0102 18:10:37.850119 53317 tcp_utils.cc:130] Successfully connected to 10.174.138.221:47036
W0102 18:10:40.361227 53317 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:10:40.386322 53317 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:10:41,374] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:10:41,841] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:10:41,843] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:10:42,357] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:10:43,224] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:10:43,226] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:10:43,227] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:10:43,229] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:10:43,230] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:10:43,231] [    INFO][0m - [0m
[33m[2024-01-02 18:10:43,231] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,233] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:10:43,245] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:10:43,289] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,291] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,317] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,318] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:10:43,396] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:10:43,397] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:10:43,446] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:10:43,447] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:10:43,451] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 10195.20it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:08,  2.19s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.22s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.19s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.07s/it]
[32m[2024-01-02 18:10:58,302] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:10:58,302] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:10:58,343] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:10:58,343] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:10:58,365] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - Init new lora model[0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-02 18:10:58,400] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-02 18:10:58,640] [    INFO][0m - Frozen parameters: 3.61e+09 || Trainable parameters:1.37e+07 || Total parameters:3.62e+09|| Trainable:0.38%[0m
[32m[2024-01-02 18:10:58,702] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:3.81e+08 || Total parameters:3.62e+09|| Trainable:10.53%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-02 18:10:59,298] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-02 18:10:59,302] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-02 18:10:59,303] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-02 18:10:59,310] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-02 18:10:59,310] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:10:59,378] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-02 18:10:59,378] [    INFO][0m - Using half precision[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-02 18:10:59,402] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - current_device                : gpu:2[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - decay_steps                   : 1[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-02 18:10:59,403] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-02 18:10:59,404] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan02_18-10-34_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-02 18:10:59,405] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - max_steps                     : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-02 18:10:59,406] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-02 18:10:59,407] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-02 18:10:59,408] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - [0m
[32m[2024-01-02 18:10:59,409] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-02 18:10:59,415] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,455] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,540] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-02 18:10:59,591] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,591] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-02 18:10:59,628] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-02 18:10:59,681] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-02 18:10:59,732] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-02 18:10:59,733] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-02 18:10:59,733] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 512[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total optimization steps = 1[0m
[32m[2024-01-02 18:10:59,733] [    INFO][0m -   Total num train samples = 512[0m
[32m[2024-01-02 18:10:59,737] [    INFO][0m -   Number of trainable parameters = 381,026,304 (per device)[0m
[32m[2024-01-02 18:10:59,756] [    INFO][0m -   Number of trainable parameters = 762,052,608 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/llama/modeling.py:1471: UserWarning: enable_parallel_cross_entropy, the vocab_size should be splited: 59792, 59792
  warnings.warn(
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-02 18:11:24,771] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-02 18:11:24,775] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-02 18:11:24,775] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-02 18:11:24,776] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-02 18:11:27,588] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-02 18:26:52,723] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-02 18:26:52,724] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-02 18:26:52,724] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_set_to_1d', current_value=False, default_value=True)
=======================================================================
I0102 18:26:52.725773 62338 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55969 while the server is not yet listening.
I0102 18:26:55.725978 62338 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55969
W0102 18:26:57.756916 62338 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0102 18:26:57.782856 62338 gpu_resources.cc:149] device: 2, cuDNN Version: 8.6.
[2024-01-02 18:26:58,810] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-02 18:26:59,264] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-02 18:26:59,796] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-02 18:27:00,705] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-02 18:27:00,707] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-02 18:27:00,708] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,710] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - modules_to_save               : embed_tokens,lm_head[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-02 18:27:00,711] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - [0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - ============================================================[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-02 18:27:00,712] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - [0m
[33m[2024-01-02 18:27:00,713] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-02 18:27:00,713] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-02 18:27:00,714] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,714] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-02 18:27:00,726] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-02 18:27:00,726] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-02 18:27:00,776] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,778] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,804] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,805] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-02 18:27:00,834] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-02 18:27:00,834] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-02 18:27:00,870] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-02 18:27:00,870] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                            | 0/5 [00:00<?, ?it/s][32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-02 18:27:00,874] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11096.04it/s]
Loading checkpoint shards:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 1/5 [00:02<00:09,  2.33s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 2/5 [00:04<00:06,  2.25s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 3/5 [00:06<00:04,  2.20s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 4/5 [00:08<00:02,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.10s/it]
[32m[2024-01-02 18:27:16,157] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-02 18:27:16,157] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-02 18:27:16,200] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-02 18:27:16,201] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-02 18:27:16,220] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 641, in <module>
    main()
  File "run_pretrain.py", line 529, in main
    model.resize_token_embeddings(len(new_tokenizer))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/transformers/model_utils.py", line 1351, in resize_token_embeddings
    self.model.lm_head.weight = new_lm_head_weight
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/nn/layer/layers.py", line 1474, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'LlamaForCausalLM' object has no attribute 'model'
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:03:44,876] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:03:44,877] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:03:44,877] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:03:44.878561  7618 tcp_utils.cc:130] Successfully connected to 10.174.138.221:54784
W0104 21:03:49.816864  7618 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:03:49.843730  7618 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:03:53,258] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:03:54,423] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:03:54,424] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:03:56,336] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:03:56,337] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:03:56,338] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,339] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:03:56,340] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - [0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:03:56,341] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - [0m
[33m[2024-01-04 21:03:56,342] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,342] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:03:56,354] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:03:56,354] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:03:56,423] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,425] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,452] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,453] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:03:56,495] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:03:56,496] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:03:56,535] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:03:56,535] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:03:56,537] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11491.24it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.05s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.02s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.01s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.01it/s]
[32m[2024-01-04 21:04:03,607] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:04:03,607] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:04:03,643] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:04:03,644] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:04:03,666] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:04:03,695] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:04:03,699] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:04:03,705] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:04:03,979] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:04:05,062] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:04:05,067] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:04:05,071] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:04:05,702] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:04:05,770] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:04:05,770] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:04:05,792] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:04:05,793] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:04:05,794] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-03-44_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:04:05,795] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,796] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:04:05,797] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:04:05,798] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - [0m
[32m[2024-01-04 21:04:05,799] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:04:06,053] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:04:06,092] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:04:06,180] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:04:06,180] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:04:06,180] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:04:06,180] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:04:06,180] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:04:06,181] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:04:06,185] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:04:06,187] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:04:24,577] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:06:05,759] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:06:05,760] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:06:05,760] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:06:05.761461  9354 tcp_utils.cc:130] Successfully connected to 10.174.138.221:59733
W0104 21:06:07.978724  9354 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:06:08.002013  9354 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:06:10,140] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:06:13,291] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:06:13,293] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:06:16,486] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:06:16,487] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:06:16,488] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:06:16,490] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:06:16,491] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - [0m
[32m[2024-01-04 21:06:16,492] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:06:16,493] [    INFO][0m - [0m
[33m[2024-01-04 21:06:16,493] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:06:16,494] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,495] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:06:16,507] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:06:16,507] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:06:16,547] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,549] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,578] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,579] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:06:16,607] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:06:16,608] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:06:16,644] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:06:16,644] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,647] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,648] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:06:16,648] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12199.84it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.10s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:01<00:02,  1.06it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:02<00:01,  1.10it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.17it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.13it/s]
[32m[2024-01-04 21:06:23,311] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:06:23,312] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:06:23,359] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:06:23,359] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:06:23,377] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:06:23,390] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:06:23,391] [    INFO][0m - new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:06:23,398] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:06:23,673] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:06:28,144] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:06:28,149] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:06:28,149] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:06:28,153] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:06:28,654] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:06:28,726] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:06:28,726] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:06:28,752] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:06:28,752] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:06:28,753] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:06:28,754] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:06:28,755] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-06-05_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:06:28,756] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:06:28,757] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:06:28,758] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:06:28,759] [    INFO][0m - [0m
[32m[2024-01-04 21:06:28,760] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:06:29,168] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:06:29,206] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:06:29,275] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:06:29,275] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:06:29,275] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:06:29,275] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:06:29,276] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:06:29,280] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:06:29,281] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:07:38,130] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:22:33,169] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:22:33,169] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:22:33,169] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:22:33.170857 19916 tcp_utils.cc:107] Retry to connect to 10.174.138.221:41794 while the server is not yet listening.
I0104 21:22:36.171025 19916 tcp_utils.cc:130] Successfully connected to 10.174.138.221:41794
W0104 21:22:38.323936 19916 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:22:38.349954 19916 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:22:39,531] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:22:40,324] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:22:40,325] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:22:42,140] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:22:42,141] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:22:42,141] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:22:42,142] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:22:42,143] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - [0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:42,144] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - [0m
[33m[2024-01-04 21:22:42,145] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:22:42,145] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,146] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:22:42,155] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:22:42,156] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:22:42,191] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,192] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,217] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,217] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:22:42,244] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:22:42,245] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:22:42,279] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:22:42,279] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:22:42,282] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:22:42,283] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 8778.37it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.05s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:01,  1.00it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.01it/s]
[32m[2024-01-04 21:22:49,200] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:22:49,200] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:22:49,239] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:22:49,240] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:22:49,258] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:22:49,270] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:22:49,278] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:22:49,551] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:22:49,991] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:22:49,996] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:22:50,000] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:22:50,577] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:22:50,650] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:22:50,650] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:22:50,732] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,733] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:22:50,734] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:22:50,735] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-22-33_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:22:50,736] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:22:50,737] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:22:50,738] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:22:50,739] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - [0m
[32m[2024-01-04 21:22:50,740] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:22:51,011] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:22:51,050] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:22:51,146] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:22:51,147] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:22:51,147] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:22:51,147] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:22:51,151] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:22:51,152] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:23:59,332] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:27:55,443] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:27:55,443] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:27:55,444] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:27:55.445196 23878 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55680 while the server is not yet listening.
I0104 21:27:58.445365 23878 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55680
W0104 21:28:00.856276 23878 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:28:00.881598 23878 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:28:02,045] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:28:03,067] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:28:03,068] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:28:04,340] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:28:04,341] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:28:04,341] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:04,342] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:28:04,343] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:28:04,344] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - [0m
[33m[2024-01-04 21:28:04,345] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,345] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:28:04,355] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:28:04,355] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:28:04,399] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,400] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,427] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,427] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:28:04,454] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:28:04,455] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:28:04,500] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:28:04,500] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:28:04,502] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12314.46it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.07s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.15s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.06s/it]
[32m[2024-01-04 21:28:11,921] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:28:11,921] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:28:11,959] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:28:11,960] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:28:11,978] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:28:12,001] [    INFO][0m - old_lm_head type is <class 'paddlenlp.transformers.llama.modeling.LlamaLMHead'>[0m
[32m[2024-01-04 21:28:12,004] [    INFO][0m - -------------------------------------new_lm_head_weight.split_axis is 1[0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:28:12,016] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:28:12,291] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:28:12,566] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:28:12,571] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:28:12,575] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:28:13,369] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:28:13,441] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:28:13,441] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:28:14,419] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:28:14,420] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:28:14,421] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:28:14,422] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-27-55_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:28:14,423] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:28:14,424] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:28:14,425] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:28:14,426] [    INFO][0m - [0m
[32m[2024-01-04 21:28:14,427] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:28:14,433] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:28:14,473] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:28:14,621] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:28:14,621] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:28:14,622] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:28:14,622] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:28:14,626] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:28:14,629] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:29:23,341] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:34:45,664] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:34:45,664] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:34:45,664] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:34:45.666298 28841 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63421 while the server is not yet listening.
I0104 21:34:48.666545 28841 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63421
W0104 21:34:50.896246 28841 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:34:50.922645 28841 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:34:52,154] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:34:53,142] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:34:53,144] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:34:55,344] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:34:55,346] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:34:55,347] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:34:55,349] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,350] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:34:55,351] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:34:55,352] [    INFO][0m - [0m
[33m[2024-01-04 21:34:55,352] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:34:55,353] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,354] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:34:55,367] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:34:55,367] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:34:55,457] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,460] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,486] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,486] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:34:55,517] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:34:55,517] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:34:55,557] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:34:55,558] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:34:55,562] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 10749.11it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.10s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:01<00:02,  1.05it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:02<00:01,  1.12it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.27it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.18it/s]
[32m[2024-01-04 21:35:01,943] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:35:01,943] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:35:01,989] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:35:01,990] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:35:02,007] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:35:02,034] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:35:02,315] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:35:03,499] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:35:03,504] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:35:03,505] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:35:03,509] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:35:04,031] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:35:04,102] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:35:04,102] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:35:04,129] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:35:04,130] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - eval_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:35:04,131] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-34-45_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:35:04,132] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:35:04,133] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_steps                    : 2000[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:35:04,134] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:35:04,135] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - [0m
[32m[2024-01-04 21:35:04,136] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:35:04,458] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:35:04,496] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:35:04,589] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:35:04,589] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:35:04,589] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:35:04,589] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:35:04,590] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-04 21:35:04,593] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:35:04,594] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:36:10,868] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:36:10,868] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:36:10,868] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0104 21:36:10.869936 30161 tcp_utils.cc:107] Retry to connect to 10.174.138.221:35737 while the server is not yet listening.
I0104 21:36:13.870103 30161 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35737
W0104 21:36:16.802392 30161 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:36:16.828914 30161 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:36:17,882] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-04 21:36:18,914] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-04 21:36:18,915] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:36:18,916] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:36:20,314] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-04 21:36:20,315] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:36:20,317] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:36:20,319] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:36:20,320] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - [0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:36:20,321] [    INFO][0m - [0m
[33m[2024-01-04 21:36:20,321] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:36:20,322] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,323] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:36:20,334] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:36:20,334] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:36:20,373] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,375] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,401] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,401] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:36:20,479] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:36:20,480] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:36:20,515] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:36:20,516] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:36:20,520] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 10934.06it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:04,  1.09s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:02<00:03,  1.09s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:03<00:02,  1.08s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:04<00:01,  1.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02s/it]
[32m[2024-01-04 21:36:27,592] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:36:27,592] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:36:27,632] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:36:27,632] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:36:27,654] [    INFO][0m - self.config.tensor_parallel_degree: 4[0m
[32m[2024-01-04 21:36:27,686] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:36:27,687] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:36:27,687] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:36:27,958] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:36:34,497] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:36:34,502] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:36:34,506] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-04 21:36:35,398] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:36:35,470] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:36:35,470] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:36:35,536] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:36:35,537] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:36:35,538] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-36-10_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:36:35,539] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:36:35,540] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:36:35,541] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:36:35,542] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - [0m
[32m[2024-01-04 21:36:35,543] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:36:35,549] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:36:35,588] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-04 21:36:35,753] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:36:35,753] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:36:35,753] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 128[0m
[32m[2024-01-04 21:36:35,753] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:36:35,754] [    INFO][0m -   Total num train samples = 256,000[0m
[32m[2024-01-04 21:36:35,757] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-04 21:36:35,758] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:36:46,685] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:36:46,689] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:36:46,690] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-04 21:36:48,557] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-04 21:36:50,140] [    INFO][0m - ret is None[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-04 21:36:50,141] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:37:48,073] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:37:48,073] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:37:48,073] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0104 21:37:48.074854 31399 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63978 while the server is not yet listening.
I0104 21:37:51.075101 31399 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63978
W0104 21:37:53.370215 31399 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:37:53.397647 31399 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:37:55,342] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-04 21:37:57,769] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:37:57,771] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:37:59,330] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [2], pp_group: [2], dp_group: [0, 2], check/clip group: [2, 3]
[32m[2024-01-04 21:37:59,332] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:37:59,333] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,335] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:37:59,336] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - [0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:37:59,337] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:37:59,338] [    INFO][0m - [0m
[33m[2024-01-04 21:37:59,338] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:37:59,339] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,339] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:37:59,352] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:37:59,352] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:37:59,390] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,391] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,423] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,423] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:37:59,450] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:37:59,451] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:37:59,484] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:37:59,484] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:37:59,487] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11841.63it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:01<00:07,  1.97s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:03<00:05,  1.97s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:05<00:03,  1.97s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:07<00:01,  1.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  1.86s/it]
[32m[2024-01-04 21:38:13,459] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:38:13,459] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:38:13,498] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:38:13,498] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:38:13,517] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:38:14,053] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:38:14,054] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:38:14,054] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:38:14,324] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:38:19,280] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:38:19,285] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:38:19,285] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:38:19,290] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-04 21:38:20,881] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:38:20,955] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:38:20,955] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:38:20,989] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:38:20,989] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:38:20,990] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:38:20,991] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:38:20,992] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-37-48_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - optimizer_name_suffix         : tp00[0m
[32m[2024-01-04 21:38:20,993] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:38:20,994] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:38:20,995] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:38:20,996] [    INFO][0m - [0m
[32m[2024-01-04 21:38:20,997] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:38:21,116] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:38:21,155] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:38:21,273] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:38:21,273] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:38:21,273] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:38:21,273] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:38:21,274] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:38:21,278] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:38:21,313] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:38:34,592] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:34,597] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-04 21:38:58,205] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:38:58,206] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-04 21:39:40,921] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-04 21:39:40,922] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-04 21:39:40,922] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0104 21:39:40.923679 32985 tcp_utils.cc:130] Successfully connected to 10.174.138.221:35415
W0104 21:39:45.811290 32985 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0104 21:39:45.835819 32985 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-04 21:39:48,386] [    INFO] topology.py:276 - Total 2 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-04 21:39:49,020] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-04 21:39:49,023] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-04 21:39:50,932] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 2, mp_group: [2, 3],  sharding_group: [2], pp_group: [2], dp_group: [0, 2], check/clip group: [2, 3]
[32m[2024-01-04 21:39:50,933] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-04 21:39:50,933] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:39:50,934] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - model_name_or_path            : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-04 21:39:50,935] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - tokenizer_name_or_path        : facebook/llama-7b[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-04 21:39:50,936] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - [0m
[33m[2024-01-04 21:39:50,937] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - We are using (<class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'>, False) to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:50,937] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/sentencepiece.bpe.model[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/facebook/llama-7b/tokenizer_config.json[0m
[32m[2024-01-04 21:39:50,948] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/facebook/llama-7b/special_tokens_map.json[0m
[32m[2024-01-04 21:39:50,987] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:50,989] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,017] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,017] [    INFO][0m - Loading configuration file /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 32000
}

[32m[2024-01-04 21:39:51,043] [    INFO][0m - Found /root/.paddlenlp/models/facebook/llama-7b/config.json[0m
[32m[2024-01-04 21:39:51,044] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'facebook/llama-7b'.[0m
[32m[2024-01-04 21:39:51,078] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
[32m[2024-01-04 21:39:51,078] [    INFO][0m - Loading weights file model.safetensors from cache at /root/.paddlenlp/models/facebook/llama-7b/model.safetensors.index.json[0m
Downloading shards:   0%|                                                                                                                                      | 0/5 [00:00<?, ?it/s][32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00001-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00002-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,080] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00003-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,081] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00004-of-00005.safetensors[0m
[32m[2024-01-04 21:39:51,081] [    INFO][0m - Already cached /root/.paddlenlp/models/facebook/llama-7b/model-00005-of-00005.safetensors[0m
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12115.26it/s]
Loading checkpoint shards:   0%|                                                                                                                               | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 1/5 [00:02<00:08,  2.20s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 2/5 [00:04<00:06,  2.22s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 3/5 [00:06<00:04,  2.16s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 4/5 [00:08<00:02,  2.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  1.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.04s/it]
[32m[2024-01-04 21:40:06,720] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-04 21:40:06,720] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at facebook/llama-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-04 21:40:06,759] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-04 21:40:06,760] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf'.[0m
[32m[2024-01-04 21:40:06,780] [    INFO][0m - self.config.tensor_parallel_degree: 2[0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - Init new lora model[0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-04 21:40:06,802] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-04 21:40:07,074] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-04 21:40:07,584] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-04 21:40:07,588] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-04 21:40:07,589] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-04 21:40:07,593] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-04 21:40:09,231] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-04 21:40:09,301] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-04 21:40:09,301] [    INFO][0m - Using half precision[0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m - ============================================================[0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-04 21:40:11,060] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - data_parallel_degree          : 2[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - data_parallel_rank            : 1[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-04 21:40:11,061] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-04 21:40:11,062] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - gradient_accumulation_steps   : 32[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan04_21-39-40_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-04 21:40:11,063] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - optimizer_name_suffix         : tp00[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-04 21:40:11,064] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-04 21:40:11,065] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-04 21:40:11,066] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - [0m
[32m[2024-01-04 21:40:11,067] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-04 21:40:11,077] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-04 21:40:11,116] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-04 21:40:11,401] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-04 21:40:11,401] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-04 21:40:11,402] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 256[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Gradient Accumulation steps = 32[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-04 21:40:11,402] [    INFO][0m -   Total num train samples = 512,000[0m
[32m[2024-01-04 21:40:11,407] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-04 21:40:11,440] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-04 21:40:26,823] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-04 21:40:26,829] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-04 21:40:26,830] [    INFO][0m -   Total Batch size = 16[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:02:57,788] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:02:57,788] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:02:57,788] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:02:57.789602 110466 tcp_utils.cc:107] Retry to connect to 10.174.138.221:46099 while the server is not yet listening.
I0105 13:03:00.789805 110466 tcp_utils.cc:130] Successfully connected to 10.174.138.221:46099
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 395, in main
    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/argparser.py", line 246, in parse_json_file
    obj = dtype(**inputs)
  File "<string>", line 95, in __init__
  File "run_pretrain.py", line 86, in __post_init__
    super().__post_init__()
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py", line 1122, in __post_init__
    fleet.init(is_collective=True, strategy=strategy)
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/fleet/fleet.py", line 305, in init
    paddle.distributed.init_parallel_env()
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/parallel.py", line 1100, in init_parallel_env
    paddle.distributed.barrier(group=group)
  File "/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py", line 328, in barrier
    task = group.process_group.barrier(device_id)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::distributed::ProcessGroupNCCL::Barrier(paddle::distributed::BarrierOptions const&)
1   phi::DenseTensor::DenseTensor(phi::Allocator*, phi::DenseTensorMeta const&)
2   paddle::experimental::DefaultAllocator::Allocate(unsigned long)
3   phi::memory_utils::Alloc(phi::Place const&, unsigned long)
4   phi::MemoryUtils::Alloc(phi::Place const&, unsigned long)
5   paddle::memory::Alloc(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
12  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
13  phi::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 6. Cannot allocate 256.000000B memory on GPU 6, 127.997530TB memory has been allocated and available memory is only 0.000000B.

Please check whether there is any other process using GPU 6.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/fluid/memory/allocation/cuda_allocator.cc:86)

/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:14:24,957] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:14:24,957] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:14:24,957] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:14:24.958945 49445 tcp_utils.cc:130] Successfully connected to 10.174.138.221:43579
W0105 13:14:29.718417 49445 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:14:29.742635 49445 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:14:30,743] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:14:31,379] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:14:31,380] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:14:32,890] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:14:32,892] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:14:32,893] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:14:32,894] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:14:32,895] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:14:32,896] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:14:32,897] [    INFO][0m - [0m
[33m[2024-01-05 13:14:32,897] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:14:32,898] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,920] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,921] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:14:32,922] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:14:32,923] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:14:47,394] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:14:55,254] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:21<00:21, 21.47s/it][32m[2024-01-05 13:15:00,684] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:15:03,176] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 13.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.36s/it]
[32m[2024-01-05 13:15:03,686] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[33m[2024-01-05 13:15:03,686] [ WARNING][0m - Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path and are newly initialized: ['lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - Generation config file not found, using a generation config created from the model config.[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:15:03,688] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:15:03,960] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:15:06,317] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:15:06,323] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:15:06,323] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:15:06,328] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:15:07,344] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:15:07,415] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:15:07,451] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:15:07,452] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:15:07,453] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-14-24_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:15:07,454] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:15:07,455] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:15:07,456] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:15:07,457] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - [0m
[32m[2024-01-05 13:15:07,458] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:15:07,503] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:15:07,542] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:15:07,655] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:15:07,655] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:15:07,655] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:15:07,655] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:15:07,656] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:15:07,660] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:15:07,661] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:16:25,626] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:16:25,630] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:16:25,631] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:16:27,850] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:17:36,993] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:17:36,993] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:17:36,994] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
=======================================================================
I0105 13:17:36.995771 52409 tcp_utils.cc:107] Retry to connect to 10.174.138.221:38381 while the server is not yet listening.
I0105 13:17:39.996007 52409 tcp_utils.cc:130] Successfully connected to 10.174.138.221:38381
W0105 13:17:44.980296 52409 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:17:45.007508 52409 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:17:46,186] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:17:46,928] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:17:48,083] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:17:48,083] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:17:48,084] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:17:48,087] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:17:48,088] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - [0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:17:48,089] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - [0m
[33m[2024-01-05 13:17:48,090] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:17:48,090] [    INFO][0m - Checkpoint detected, resuming training at ./checkpoints/checkpoint-1. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.[0m
[32m[2024-01-05 13:17:48,092] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,125] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,125] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:17:48,128] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:17:48,129] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:38:43,627] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:38:43,627] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:38:43,628] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:38:43.629176 71802 tcp_utils.cc:107] Retry to connect to 10.174.138.221:53539 while the server is not yet listening.
I0105 13:38:46.629343 71802 tcp_utils.cc:130] Successfully connected to 10.174.138.221:53539
W0105 13:38:48.650488 71802 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:38:48.677554 71802 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:38:50,050] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:38:51,165] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:38:51,166] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:38:52,587] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:38:52,587] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:38:52,588] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:38:52,589] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:38:52,590] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:38:52,591] [    INFO][0m - [0m
[33m[2024-01-05 13:38:52,591] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:38:52,592] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,614] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,615] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:38:52,616] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:38:52,617] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:39:06,106] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:13,762] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:19<00:19, 19.91s/it][32m[2024-01-05 13:39:19,518] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:39:22,592] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 13.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.10s/it]
[32m[2024-01-05 13:39:23,129] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:39:23,130] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:39:23,131] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:39:23,131] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:39:23,132] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:39:23,132] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:39:23,407] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:39:24,093] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:39:24,098] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:39:24,102] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:39:25,076] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:39:25,193] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:39:25,193] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:39:25,232] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:39:25,233] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:39:25,234] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:39:25,235] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:39:25,236] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,237] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-38-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:39:25,238] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:39:25,239] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,240] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:39:25,241] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:39:25,242] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:39:25,243] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:39:25,244] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - [0m
[32m[2024-01-05 13:39:25,245] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:39:25,258] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:39:25,296] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:39:25,375] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:39:25,375] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:39:25,375] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:39:25,375] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:39:25,376] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:39:25,379] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:39:25,381] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:40:33,582] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:40:33,586] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:40:35,464] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
Traceback (most recent call last):
  File "run_pretrain.py", line 640, in <module>
    main()
  File "run_pretrain.py", line 619, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1011, in train
    self._maybe_log_save_evaluate(tr_loss, model, epoch, ignore_keys_for_eval, inputs=inputs)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 1233, in _maybe_log_save_evaluate
    self._save_checkpoint(model, metrics=metrics)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2028, in _save_checkpoint
    self.save_model(output_dir, True)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2013, in save_model
    self._save(output_dir=output_dir, merge_tensor_parallel=merge_tensor_parallel)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/trainer.py", line 2212, in _save
    self.model.save_pretrained(
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 255, in save_pretrained
    trainable_state_dict = self._merge_trainable_tensor_parallel(trainable_state_dict)
  File "/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/peft/lora/lora_model.py", line 198, in _merge_trainable_tensor_parallel
    logger.info(f"ret is {ret[0].shape,ret[0]} ")
TypeError: 'NoneType' object is not subscriptable
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:41:37,833] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:41:37,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:41:37,833] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:41:37.835350 73845 tcp_utils.cc:107] Retry to connect to 10.174.138.221:63585 while the server is not yet listening.
I0105 13:41:40.835592 73845 tcp_utils.cc:130] Successfully connected to 10.174.138.221:63585
W0105 13:41:42.851826 73845 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:41:42.880256 73845 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:41:43,858] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:41:44,656] [    INFO] topology.py:276 - Total 1 model comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:41:44,658] [    INFO] topology.py:276 - Total 4 sharding comm group(s) create successfully!
[2024-01-05 13:41:45,959] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 4, sharding_degree: 1, pp_degree: 1, dp_degree: 1, mp_group: [0, 1, 2, 3],  sharding_group: [2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:41:45,960] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:41:45,960] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:41:45,961] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:41:45,962] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - new_tokenizer_name_or_path    : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer/merged_tokenizer_hf[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - [0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:41:45,963] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:41:45,964] [    INFO][0m - [0m
[33m[2024-01-05 13:41:45,964] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:41:45,965] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,986] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,987] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 4,
  "tensor_parallel_output": true,
  "tensor_parallel_rank": 2,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:41:45,989] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:41:45,989] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:41:59,902] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:07,758] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:20<00:20, 20.98s/it][32m[2024-01-05 13:42:13,555] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:42:16,364] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 13.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.44s/it]
[32m[2024-01-05 13:42:16,883] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:42:16,883] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:42:16,884] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:42:16,885] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:42:17,160] [    INFO][0m - Frozen parameters: 1.62e+09 || Trainable parameters:1.33e+08 || Total parameters:1.75e+09|| Trainable:7.59%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:42:21,524] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:42:21,529] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:42:21,530] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:42:21,534] [    INFO][0m - æ¯ä¸€æ—¶åˆ»åªå«æœ‰çº¦15çš®å…‹çš„æ°¡ã€‚
æº¶æ¶²æ‰€å«çš„é•­-226ç»Î±è¡°å˜ï¼ŒåŠè¡°æœŸä¸º1600å¹´ã€‚
æ°¡æ˜¯å…¶ä¸­ä¸€ç§è¡°å˜äº§ç‰©ï¼Œå…¶ç´¯ç§¯é€ŸçŽ‡çº¦ä¸º1Â mmæ¯å¤©æ¯1å…‹é•­ã€‚
è¡°å˜è¿…é€Ÿè¾¾è‡´å¹³è¡¡ï¼Œæº¶æ¶²ä¼šä¸æ–­äº§ç”Ÿæ–°çš„æ°¡ï¼Œå…¶æ”¾å°„æ€§å°†ä¸Žé•­ç›¸åŒï¼ˆ50 Bqï¼‰ã€‚
æ°”æ€Rnï¼ˆåŠè¡°æœŸçº¦ä¸º4å¤©ï¼‰ä¼šæ‰©æ•£å¹¶é€ƒé€¸å‡ºå®¹å™¨ã€‚
UNSCEAR 1993å¹´æŠ¥å‘Šä¸­é‡‡ç”¨çš„æ°¡-222çš„å‰‚é‡è½¬æ¢å› å­ä¸º9nSv/(Bq/m)ï¼›å¹³è¡¡å› å­ï¼ˆæ°¡ä¸Žå…¶çŸ­å¯¿å‘½å­ä½“å¹³è¡¡æ€çš„åº¦é‡ï¼‰å®¤å†…å–0.4ï¼Œå®¤å¤–å–0.6ï¼›å±…ç•™å› å­å®¤å†…å–0.8ï¼Œå®¤å¤–å–0.2ã€‚
å¯¼å‡ºæ°¡-222çš„å¹´æœ‰æ•ˆå‰‚é‡ï¼š
20ä¸–çºªåˆï¼Œåº¸åŒ»æ›¾åˆ©ç”¨æ°¡æ¥æ²»ç–—å„ç§ç–¾ç—…ã€‚
ç—…äººåœ¨å¯†å°çš„å°æˆ¿é—´å†…æŽ¥è§¦æ°¡ï¼Œä»¥èŽ·å–ã€Œæ²»ç–—åŠŸæ•ˆã€ã€‚
å¾ˆå¿«äººä»¬ä¾¿å‘çŽ°ï¼Œæ°¡çš„è‡´ç”µç¦»è¾å°„èƒ½å¤Ÿè‡´ç™Œã€‚
è™½ç„¶æ°¡çš„æ”¾å°„æ€§å¯ä»¥æ€æ­»ç™Œç»†èƒžï¼Œä½†å®ƒå¯¹å¥åº·ç»†èƒžåŒæ ·æœ‰æŸå®³ã€‚
è‡´ç”µç¦»è¾å°„ä¼šå¯¼è‡´è‡ªç”±åŸºçš„å½¢æˆï¼Œè¿›è€Œåœ¨ç»†èƒžåŠåŸºå› ä¸Šé€ æˆæ›´å¤§çš„ä¼¤å®³ï¼Œç”šè‡³ä¼šå¼•å‘ç™Œç—‡ã€‚
æ›¾æœ‰äººæå‡ºç”¨æ°¡çš„è¾å°„æ¿€æ•ˆæ¥æ²»ç–—å…³èŠ‚ç‚Žç­‰è‡ªä½“å…ç–«æ€§ç–¾ç—…ã€‚
20ä¸–çºªæœ«è‡³21ä¸–çºªåˆï¼Œç¾Žå›½è’™å¤§æ‹¿å·žæ°ä½›é€ŠåŽ¿çš„ä¸€äº›ã€Œå¥åº·çŸ¿äº•ã€å¸å¼•äº†ä¸å°‘æ¸´æœ›æ¶ˆé™¤å…³èŠ‚ç‚Žç­‰ç–¾ç—…çš„äººæ¥é¥®ç”¨æ”¾å°„æ€§äº•æ°´å’Œæš´éœ²åœ¨æ°¡æ°”ä¹‹ä¸­ã€‚
ç„¶è€Œå› ä¸ºé«˜å‰‚é‡è¾å°„ä¼šå¯¹èº«ä½“äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œæ‰€ä»¥è¿™ä¸€ç–—æ³•å¹¶ä¸å—åˆ°åŒ»ç”Ÿçš„é¼“åŠ±ã€‚
æ·å…‹äºšå¸ŒèŽ«å¤«è‡ª1906å¹´èµ·ä¾¿æœ‰ä½¿ç”¨æ”¾å°„æ€§æ°´æµ´ï¼Œè€Œå¥¥åœ°åˆ©å·´ç‰¹åŠ æ–¯æ³°å› åˆ™åœ¨æ°¡è¢«å‘çŽ°ä¹‹å‰å°±å·²æœ‰æ”¾å°„æ€§æ°´æµ´çš„ä½¿ç”¨ã€‚
æ—¥æœ¬é¸Ÿå–åŽ¿ä¸‰æœç”ºä¹Ÿæœ‰å¯Œå«é•­çš„æ¸©æ³‰ã€‚
å¾·å›½å·´å¾·å¸ƒå…°å·´èµ«åˆ™æœ‰é¥®ç”¨æ”¾å°„æ€§æ°´çš„ç–—æ³•ã€‚
å¥¥åœ°åˆ©åŠ æ–¯æ³°çº³-æµ·å°”æ–½å¤šå…°ï¼ˆGasteiner-Heilstollenï¼‰ã€æ³¢å…°å¸Œç»´æ‹‰æœå¤«-å…¹å¾·é²ä¼Šã€åˆ‡å°”æ¶…ç“¦-å…¹å¾·é²ä¼Šï¼ˆCzerniawa-ZdrÃ³jï¼‰ã€ç§‘ç“¦é›·ã€éš†ä»£å…‹-å…¹å¾·é²[0m
[32m[2024-01-05 13:42:22,548] [    INFO][0m - The global seed is set to 42, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:42:22,627] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:42:22,627] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:42:22,659] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:42:22,659] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:42:22,660] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataset_rank                  : 0[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - dataset_world_size            : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,661] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - gradient_accumulation_steps   : 256[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:42:22,662] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-41-37_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:42:22,663] [    INFO][0m - optimizer_name_suffix         : tp02[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:42:22,664] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding                      : [][0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_degree      : 1[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - sharding_parallel_rank        : 0[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save_model_state       : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:42:22,665] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - tensor_parallel_degree        : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - tensor_parallel_rank          : 2[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - weight_name_suffix            : tp02[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - [0m
[32m[2024-01-05 13:42:22,666] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:42:22,673] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:42:22,711] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:42:22,793] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:42:22,794] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:42:22,794] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Gradient Accumulation steps = 256[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:42:22,794] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:42:22,798] [    INFO][0m -   Number of trainable parameters = 132,956,160 (per device)[0m
[32m[2024-01-05 13:42:22,799] [    INFO][0m -   Number of trainable parameters = 531,824,640 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:43:32,924] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:43:32,942] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Num examples = 306725[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:43:32,943] [    INFO][0m -   Total Batch size = 8[0m
[32m[2024-01-05 13:43:34,820] [    INFO][0m - Saving model checkpoint to ./checkpoints/checkpoint-1[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving with merge_tensor_parallel, tensor_parallel_rank > 0 don't need save[0m
[32m[2024-01-05 13:43:36,263] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:43:57,123] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:43:57,123] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:43:57,124] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
=======================================================================
I0105 13:43:57.125466 75816 tcp_utils.cc:130] Successfully connected to 10.174.138.221:62346
W0105 13:43:59.260133 75816 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:43:59.290457 75816 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:44:00,453] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 model comm group(s) create successfully!
[2024-01-05 13:44:00,454] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:44:01,228] [    INFO] topology.py:276 - Total 1 sharding comm group(s) create successfully!
[2024-01-05 13:44:01,824] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 1, sharding_degree: 4, pp_degree: 1, dp_degree: 1, mp_group: [2],  sharding_group: [0, 1, 2, 3], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:44:01,824] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:44:01,825] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:44:01,827] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:44:01,828] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:44:01,829] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - [0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:44:01,830] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - [0m
[33m[2024-01-05 13:44:01,831] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:44:01,831] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,867] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,867] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 1,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:44:01,870] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:44:01,870] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:21<00:21, 21.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 13.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 14.56s/it]
[32m[2024-01-05 13:44:42,671] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:44:42,671] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:44:42,673] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:44:42,905] [    INFO][0m - Frozen parameters: 6.48e+09 || Trainable parameters:5.10e+08 || Total parameters:6.99e+09|| Trainable:7.30%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=True, share_folder False, check_rank_flag False
building indices on rank 0 ...
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:44:46,115] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:44:46,120] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:44:46,121] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:44:46,126] [    INFO][0m -  ï¼ˆå…‰å®³ï¼‰ä¹Ÿè¶Šæ¥è¶Šå¤šã€‚
è¿™äº›äººé€ çš„ç¯å…‰ç…§äº®å¤œæ™šçš„å¤©ç©ºï¼ŒåŠ ä¸Šäººç±»æ´»åŠ¨é€ æˆçš„å„ç§ç©ºæ°”æ±¡æŸ“å¼•èµ·çš„ç°å°˜æ•£å°„ç¯å…‰çš„åŒé‡æ•ˆåº”ï¼Œä½¿å¾—èƒŒæ™¯å¤©ç©ºå˜å¾—ç°æš—ï¼Œå¯¹å…‰åº¦å¾®å¼±çš„å¤©ä½“è§‚æµ‹è¶ŠåŠ å›°éš¾ï¼Œå¿…é¡»è¦ç‰¹åˆ«çš„æ»¤å…‰é•œæ¥éš”ç»èƒŒæ™¯å…‰çš„å¹²æ‰°ã€‚
åœ¨æœ‰äº›åœ°åŒºï¼Œå¦‚ç¾Žå›½äºšåˆ©æ¡‘é‚£å·žã€è‹±å›½å’Œæ—¥æœ¬ä¸Žé¦™æ¸¯ï¼Œéƒ½æœ‰æ°‘é—´å›¢ä½“åœ¨ç ”ç©¶ä¸Žå‘èµ·ä»¥å‡å°‘å…‰æ±¡æŸ“ã€‚
é¼“åŠ±ä¸ºè¡—ç¯è£…ä¸Šåå°„ç½©ï¼Œä¸ä»…èƒ½ä½¿ç…§å‘åœ°é¢çš„ç¯å…‰å¢žåŠ ï¼Œä¹Ÿä½¿ç›´æŽ¥å°„è¿›å¤©ç©ºçš„å…‰é‡é™ä½Žã€‚
ä½†å› ä¸ŽåŸŽå¸‚çš„ç»æµŽå‘å±•ç›¸è¿èƒŒï¼Œå¯¹äºŽæŽ¨å¹¿æ„è¯†çš„æ·¡è–„ä¸Žå¯¹ç§‘å­¦çš„ä¸é‡è§†ä¹‹ä¸‹ï¼Œå°¤å…¶åœ¨å‘å±•ä¸­å›½å®¶ï¼Œè¿™æ ·çš„çŠ¶å†µèš•é£Ÿä¸€äº›å¤è€çš„å¤©æ–‡å°è‡³ä¸å†èƒ½ä½œè§‚æµ‹ã€‚
å¦‚åŒ—äº¬å¤è§‚è±¡å°ä¸Žå—äº¬ç´«é‡‘å±±å¤©æ–‡å°ç­‰ã€‚</s> æœ‰é‚£äº›è¿‡èª‰æˆ–ä¸å®žç”¨çš„æ–‡å…·ï¼Ÿ
å°±ç®—å†å¤šäººæŽ¨èæˆ‘ä¹Ÿè¦è¯´.. ç™¾ä¹V5ä¸­æ€§ç¬”ï¼
ä¹‹å‰çœ‹åˆ°çš„æ‰€æœ‰è¯„ä»·éƒ½è¯´è¿™æ˜¯æœ€å¥½å†™çš„ä¸­æ€§ç¬”ä»€ä¹ˆçš„. ä½†æ˜¯å¯èƒ½æ˜¯æ°´æ€§ç¬”çš„å…³ç³»ï¼Œè¿™ç¬”ç”¨åœ¨ç¨è–„ä¸€ç‚¹çš„çº¸ä¸Šéƒ½ä¼šé€åˆ°èƒŒé¢ï¼Œä¸€èˆ¬70gä»¥ä¸‹çš„çº¸å†™åœ¨æ­£é¢ï¼ŒèƒŒé¢ä¼šé€å‡ºå®Œå…¨é•œåƒçš„å­—ï¼Œè‡ªå·±åšç¬”è®°ç”¨å¥½ä¸€ç‚¹çš„çº¸V5å†™èµ·æ¥å¾ˆä¸é”™ï¼Œä½†æ˜¯è¯¾æœ¬è¯•å·ä»€ä¹ˆçš„çœŸçš„é€å¾—å¾ˆä¸¥é‡. è¿˜æœ‰ä¸€ä¸ªå°±æ˜¯daycraftçš„æœ¬å­ï¼Œè®¾è®¡æ–¹é¢è§ä»è§æ™ºï¼Œä½†æ˜¯ç”¨é’¢ç¬”å®¹æ˜“æ´‡ï¼Œè¯•è¿‡4001çš„å¢¨å’Œç™¾ä¹çš„å¢¨ï¼Œå†™åœ¨ä¸Šé¢éƒ½ä¼šå‘å››å‘¨è¾å°„ä¸€æ ·æ•£å¼€æ¥ï¼ŒFå°–çš„éƒ½</s>ITå¤§ç‰›æ˜¯æ€Žæ ·ç‚¼æˆçš„ï¼Ÿ
å…¬å¸æ¥äº†ä¸ªITå¤§ç‰›ï¼Œ92å¹´å‡ºç”Ÿçš„äººï¼ŒçŽ°åœ¨æ˜¯æˆ‘ä»¬çš„team leaderã€‚
æ˜¯æˆ‘è§è¿‡çš„ç¨‹åºå‘˜é‡Œæœ€åŽ‰å®³çš„ä¸€ä¸ªï¼Œä¸çŸ¥é“ä»–çš„èƒ½åŠ›æ˜¯æ€Žä¹ˆç‚¼æˆçš„ã€‚
è¯´è¯´ä»–çš„äº‹è¿¹å§ã€‚
ä»–é«˜ä¸­çš„æ—¶å€™ï¼Œå°±æŽ¥è§¦ç¼–ç¨‹ï¼ˆVB6ï¼‰ï¼Œä»–ä¸€ç›´åœ¨æƒ³ï¼Œè®¾è®¡ä¸€ä¸ªç®—æ³•å’Œè‡ªå·±å¯¹å¼ˆï¼ˆè±¡æ£‹ï¼‰ï¼Œçœ‹çœ‹æ˜¯è‡ªå·±è®¾è®¡çš„ç®—æ³•åŽ‰å®³ï¼Œè¿˜æ˜¯è‡ªå·±åŽ‰å®³ã€‚
ä»–è‡ªå·±ç‹¬ç«‹åšï¼Œæ²¡æœ‰å‚è€ƒä»»ä½•èµ„æ–™ï¼ŒèŠ±äº†åŠä¸ªå­¦æœŸï¼Œç«Ÿç„¶å†™å‡ºæ¥äº†ã€‚
ä»–è‡ªå·±åŠ¨æ‰‹åšäº†ä¸€å°æ–¯ç‰¹æž—å‘åŠ¨æœºï¼Œè®¾è®¡äº†ä¸€å°é…’ç²¾ç¯å¸¦åŠ¨çš„ç¼çº«æœºåŽŸåž‹ã€‚
ä»–å®¶æ˜¯å†œæ‘çš„ï¼Œä»–çˆ¸çˆ¸å¼€è´§è½¦ï¼Œè½¦æŒ‚è¢«å·äº†ã€‚
ä»–å°±ç”¨ç»§ç”µå™¨åšäº†ä¸€ä¸ªæŠ¥è­¦å™¨ï¼Œå¦‚æžœå·è½¦è´¼å·èµ°è½¦æŒ‚ï¼Œå°±ä¼šå¼„æ–­é“œçº¿ï¼Œè§¦å‘ç»§ç”µå™¨æŠ¥è­¦ã€‚
å†œæ‘çš„å»ºç­‘å·¥äºº[0m
[32m[2024-01-05 13:44:49,569] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:44:49,637] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:44:49,637] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:44:49,700] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataset_rank                  : 2[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - dataset_world_size            : 4[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:44:49,701] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - gradient_accumulation_steps   : 64[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:44:49,702] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-43-57_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,703] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - optimizer_name_suffix         : shard02[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:44:49,704] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:44:49,705] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_degree      : 4[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - sharding_parallel_rank        : 2[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_degree        : 1[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:44:49,706] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - weight_name_suffix            : [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - [0m
[32m[2024-01-05 13:44:49,707] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:44:49,714] [    INFO] sharding_parallel.py:30 - start broadcast sharding parameters
[2024-01-05 13:44:49,901] [    INFO] sharding_parallel.py:37 - sharding's parameters is ready
[2024-01-05 13:44:49,902] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:44:49,903] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Gradient Accumulation steps = 64[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:44:49,903] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:44:49,908] [    INFO][0m -   Number of trainable parameters = 509,804,544 (per device)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:45:22,946] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Num examples = 920176[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:45:22,950] [    INFO][0m -   Total Batch size = 32[0m
[32m[2024-01-05 13:45:27,158] [    INFO][0m - Saving optimizer files.[0m
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[2024-01-05 13:45:43,661] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[33m[2024-01-05 13:45:43,661] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
[33m[2024-01-05 13:45:43,661] [ WARNING][0m - segment parallel is not supported!!!, Ignore it.[0m
/root/paddlejob/workspace/zhengxiong/pr/PaddleNLP/paddlenlp/trainer/training_args.py:1110: UserWarning: The enable_stage1_tensor_fusion or enable_stage1_overlap is not supported by current version of Paddle. Please try latest develop Paddle.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='6', default_value='')
FLAGS(name='FLAGS_cudnn_deterministic', current_value=True, default_value=False)
FLAGS(name='FLAGS_embedding_deterministic', current_value=1, default_value=0)
=======================================================================
I0105 13:45:43.663062 77264 tcp_utils.cc:107] Retry to connect to 10.174.138.221:55899 while the server is not yet listening.
I0105 13:45:46.663247 77264 tcp_utils.cc:130] Successfully connected to 10.174.138.221:55899
W0105 13:45:48.742448 77264 gpu_resources.cc:119] Please NOTE: device: 6, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.8
W0105 13:45:48.769078 77264 gpu_resources.cc:149] device: 6, cuDNN Version: 8.6.
[2024-01-05 13:45:50,278] [    INFO] topology.py:276 - Total 4 data comm group(s) create successfully!
NCCL version 2.15.5+cuda11.8
[2024-01-05 13:45:50,818] [    INFO] topology.py:276 - Total 2 model comm group(s) create successfully!
[2024-01-05 13:45:50,819] [    INFO] topology.py:276 - Total 4 pipe comm group(s) create successfully!
[2024-01-05 13:45:51,208] [    INFO] topology.py:276 - Total 2 sharding comm group(s) create successfully!
[2024-01-05 13:45:52,261] [    INFO] topology.py:222 - HybridParallelInfo: rank_id: 2, mp_degree: 2, sharding_degree: 2, pp_degree: 1, dp_degree: 1, mp_group: [2, 3],  sharding_group: [0, 2], pp_group: [2], dp_group: [2], check/clip group: [0, 1, 2, 3]
[32m[2024-01-05 13:45:52,262] [    INFO][0m -     +==============================================================================+
    |                                                                              |
    |                         DistributedStrategy Overview                         |
    |                                                                              |
    +==============================================================================+
    |                        a_sync=True <-> a_sync_configs                        |
    +------------------------------------------------------------------------------+
    |                               k_steps                    -1                  |
    |                     max_merge_var_num                    1                   |
    |                       send_queue_size                    16                  |
    |               independent_recv_thread                  False                 |
    |         min_send_grad_num_before_recv                    1                   |
    |                      thread_pool_size                    1                   |
    |                       send_wait_times                    1                   |
    |               runtime_split_send_recv                  False                 |
    |                        launch_barrier                   True                 |
    |             heter_worker_device_guard                   cpu                  |
    |                        lr_decay_steps                    10                  |
    |                            use_ps_gpu                    0                   |
    +==============================================================================+
    |                    Environment Flags, Communication Flags                    |
    +------------------------------------------------------------------------------+
    |                                  mode                    1                   |
    |                               elastic                  False                 |
    |                                  auto                  False                 |
    |                   sync_nccl_allreduce                   True                 |
    |                         nccl_comm_num                    1                   |
    |            use_hierarchical_allreduce                  False                 |
    |   hierarchical_allreduce_inter_nranks                    1                   |
    |                       sync_batch_norm                  False                 |
    |                   fuse_all_reduce_ops                   True                 |
    |                  fuse_grad_size_in_MB                    32                  |
    |              fuse_grad_size_in_TFLOPS                   50.0                 |
    |               cudnn_exhaustive_search                  False                 |
    |             conv_workspace_size_limit                   512                  |
    |    cudnn_batchnorm_spatial_persistent                  False                 |
    |                        fp16_allreduce                  False                 |
    |               last_comm_group_size_MB                   1.0                  |
    |                find_unused_parameters                  False                 |
    |            without_graph_optimization                   True                 |
    |                 fuse_grad_size_in_num                    8                   |
    |                 calc_comm_same_stream                  False                 |
    |                                   asp                  False                 |
    |                       fuse_grad_merge                  False                 |
    |                             semi_auto                  False                 |
    |                            adam_d2sum                  False                 |
    |                           auto_search                  False                 |
    |                        heter_ccl_mode                  False                 |
    |                         is_fl_ps_mode                  False                 |
    |                      with_coordinator                  False                 |
    |                            split_data                   True                 |
    |                  downpour_table_param                    []                  |
    |                       fs_client_param                                        |
    +==============================================================================+
    |                                Build Strategy                                |
    +------------------------------------------------------------------------------+
    |           enable_sequential_execution                  False                 |
    |              fuse_elewise_add_act_ops                  False                 |
    |                       fuse_bn_act_ops                  False                 |
    |              fuse_relu_depthwise_conv                  False                 |
    |                    fuse_broadcast_ops                  False                 |
    |                fuse_all_optimizer_ops                  False                 |
    |                        enable_inplace                  False                 |
    |     enable_backward_optimizer_op_deps                   True                 |
    |                 cache_runtime_context                  False                 |
    |                   fuse_bn_add_act_ops                   True                 |
    |                    enable_auto_fusion                  False                 |
    |                          enable_addto                  False                 |
    |                      fix_op_run_order                  False                 |
    |              allow_cuda_graph_capture                  False                 |
    |                       reduce_strategy                    0                   |
    |                    fuse_gemm_epilogue                  False                 |
    |                   debug_graphviz_path                                        |
    |                       fused_attention                  False                 |
    |                     fused_feedforward                  False                 |
    +==============================================================================+
    |                              Execution Strategy                              |
    +------------------------------------------------------------------------------+
    |                           num_threads                    1                   |
    |          num_iteration_per_drop_scope                    10                  |
    |                 num_iteration_per_run                    1                   |
    |                    use_thread_barrier                  False                 |
    +==============================================================================+
[0m
[32m[2024-01-05 13:45:52,263] [    INFO][0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m -      Model Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,265] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - attention_probs_dropout_prob  : 0.1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - continue_training             : 1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_attention_ffn            : False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_attention_qkv            : False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - fuse_sequence_parallel_allreduce: False[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - hidden_dropout_prob           : 0.1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_alpha                    : 32[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_dropout                  : 0.05[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_rank                     : 8[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - lora_trainable                : q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - model_name_or_path            : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - modules_to_save               : .*embed_tokens.*,.*lm_head.*[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - new_tokenizer_name_or_path    : None[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - no_recompute_layers           : None[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - pp_recompute_interval         : 1[0m
[32m[2024-01-05 13:45:52,266] [    INFO][0m - recompute_granularity         : full[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - recompute_use_reentrant       : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - sequence_parallel             : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - tokenizer_name_or_path        : /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_flash_attention           : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_fused_rms_norm            : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_fused_rope                : False[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - use_vocab_extend              : True[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - virtual_pp_degree             : 1[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - [0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m -       Data Configuration Arguments      [0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - data_cache                    : None[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - data_impl                     : mmap[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - input_dir                     : /root/paddlejob/workspace/zhengxiong/tmp_save/script/unigram_merged_tokenizer[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - max_seq_length                : 512[0m
[32m[2024-01-05 13:45:52,267] [    INFO][0m - share_folder                  : False[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - skip_warmup                   : True[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - split                         : 949,50,1[0m
[32m[2024-01-05 13:45:52,268] [    INFO][0m - [0m
[33m[2024-01-05 13:45:52,268] [ WARNING][0m - Process rank: 2, device: gpu, world_size: 4, distributed training: True, 16-bits training: True[0m
[32m[2024-01-05 13:45:52,269] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,290] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.configuration.LlamaConfig'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,291] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/config.json[0m
Final pre-training config: LlamaConfig {
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "fuse_attention_ffn": false,
  "fuse_attention_qkv": false,
  "fuse_sequence_parallel_allreduce": false,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "no_recompute_layers": null,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pp_recompute_interval": 1,
  "recompute_granularity": "full",
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "sep_parallel_degree": 1,
  "seq_length": 512,
  "sequence_parallel": false,
  "tensor_parallel_degree": 2,
  "tensor_parallel_output": true,
  "tie_word_embeddings": false,
  "use_fused_rms_norm": false,
  "use_fused_rope": false,
  "use_recompute": 0,
  "virtual_pp_degree": 1,
  "vocab_size": 59792
}

[32m[2024-01-05 13:45:52,293] [    INFO][0m - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load '/root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path'.[0m
[32m[2024-01-05 13:45:52,294] [    INFO][0m - Loading weights file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/model_state.pdparams.index.json[0m
Loading checkpoint shards:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s][32m[2024-01-05 13:46:08,314] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:15,472] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 1/2 [00:21<00:21, 21.07s/it][32m[2024-01-05 13:46:23,151] [    INFO][0m - Converting state_dict to Tensor Parallel Format[0m
[32m[2024-01-05 13:46:26,266] [    INFO][0m - Converted state_dict to Tensor Parallel Format[0m
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 13.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 14.98s/it]
[32m[2024-01-05 13:46:27,583] [    INFO][0m - All model checkpoint weights were used when initializing LlamaForCausalLM.
[0m
[32m[2024-01-05 13:46:27,583] [    INFO][0m - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - Loading configuration file /root/paddlejob/workspace/zhengxiong/pr_test/vocab_extend_model_name_path/generation_config.json[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - Init new lora model[0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - target_modules: ['.*q_proj.*', '.*v_proj.*', '.*k_proj.*', '.*o_proj.*', '.*gate_proj.*', '.*down_proj.*', '.*up_proj.*'][0m
[32m[2024-01-05 13:46:27,585] [    INFO][0m - lora_rank: 8[0m
[32m[2024-01-05 13:46:27,859] [    INFO][0m - Frozen parameters: 3.24e+09 || Trainable parameters:2.59e+08 || Total parameters:3.50e+09|| Trainable:7.39%[0m
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
searching for causual dataset, build_indices=False, share_folder False, check_rank_flag False
build success
[32m[2024-01-05 13:46:30,440] [    INFO][0m - Sample data for train mode.[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - é¢œè‰²æ­é…æŠ€å·§ä¹‹å§å®¤å§å®¤æ˜¯ä¼‘æ¯ã€æ”¾æ¾çš„ç©ºé—´ï¼Œæ€Žä¹ˆæ ·æ¥æé«˜ç¡çœ æ˜¯å§å®¤è‰²å½©æ­é…çš„å…³é”®ã€‚
å§å®¤é¡¶éƒ¨å¤šç”¨ç™½è‰²ï¼Œç™½è‰²å’Œå…‰æ»‘çš„å¢™é¢ä½¿å…‰çš„åå°„çŽ‡è¾ƒé«˜ï¼Œå¢žåŠ æ˜Žäº®ç¨‹åº¦ã€‚
å¢™å£å¯ç”¨æ˜Žäº®å¹¶ä¸”å®é™çš„è‰²å½©ï¼Œé»„ã€é»„ç°è‰²ç­‰æµ…è‰²èƒ½å¤Ÿå¢žåŠ æˆ¿é—´çš„å¼€é˜”æ„Ÿã€‚
å®¤å†…åœ°é¢ä¸€èˆ¬é‡‡ç”¨æ·±è‰²ï¼Œåœ°é¢çš„è‰²å½©ä¸è¦å’Œå®¶å…·çš„è‰²å½©å¤ªæŽ¥è¿‘ï¼Œå¦åˆ™å½±å“å®¶åº­åŽŸæœ¬è‰¯å¥½çš„é£Žæ°´å’Œè®¾è®¡ç«‹ä½“æ„Ÿã€‚
å¦‚æžœé€‰æ‹©å£çº¸æ¥è£…é¥°çš„è¯ï¼Œç‚Žçƒ­çš„å¤å¤©å®œç”¨å†·è‰²è°ƒã€æµ…è¤ã€æµ…ç´«ç½—å…°ã€æµ…è‹¹æžœç»¿ã€æ¹–è“è‰²ã€è“è‰²ï¼Œæœ‰å®é™ã€å‡‰çˆ½ã€æ¬é€‚ä¹‹æ„Ÿï¼›è€Œå¯’å†·çš„å†¬å­£å®œç”¨çº¢æ —ã€ å¥¶é»„ã€ç±³é»„è‰²ã€å’–å•¡è‰²ç­‰æš–è‰²ç³»ï¼Œç»™äººä»¥æ¸©æš–ã€å¼€æœ—ã€æ´»æ³¼ã€å……å®žçš„æ„Ÿè§‰ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹å®¢åŽ…å®¢åŽ…çš„é¢œè‰²æ­é…ä¸»è¦å–å†³äºŽå®¢åŽ…çª—æˆ·çš„æœå‘ï¼Œå¦‚å—å‘çš„å®¢åŽ…åº”ä»¥ç™½è‰²ä½œä¸ºä¸»è‰²è°ƒï¼Œè€Œè¥¿å‘å®¢åŽ…åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒã€‚
è¿™æ˜¯å› ä¸ºï¼Œå—æ–¹äº”è¡Œå±žç«ï¼ŒæŒ‰äº”è¡Œç”Ÿå…‹ç†è®ºï¼Œç«å…‹é‡‘ä¸ºè´¢ï¼Œè¦ä¿è¯å—å‘å®¢åŽ…çš„è´¢æ°”ï¼Œé€‰ç”¨çš„æ²¹æ¼†ã€å¢™çº¸åŠæ²™å‘å‡å®œä»¥ç™½è‰²ä¸ºé¦–é€‰ï¼Œå› ä¸ºç™½è‰²æ˜¯â€œé‡‘â€çš„ä»£è¡¨è‰²ï¼Œå±žè¿Žè´¢ä¹‹è‰²ã€‚
è¥¿æ–¹äº”è¡Œå±žé‡‘ï¼Œé‡‘å…‹æœ¨ä¸ºè´¢ï¼Œå³æ˜¯è¯´æœ¨ä¹ƒé‡‘ä¹‹è´¢ï¼Œè€Œç»¿è‰²æ˜¯â€œæœ¨â€çš„ä»£è¡¨è‰²ï¼Œå¹¶ä¸”å‘è¥¿çš„å®¢åŽ…ä¸‹åˆè¥¿ç…§çš„é˜³å…‰ç”šä¸ºå¼ºçƒˆï¼Œä¸ä½†é…·çƒ­è€Œä¸”åˆºçœ¼ï¼Œæ‰€ä»¥ç”¨è¾ƒæ¸…æ·¡è€Œåˆå¯æŠ¤ç›®å…»çœ¼çš„ç»¿è‰²ï¼Œæ–¹ä¸ºé€‚å®œã€‚
å®¢åŽ…çš„å®¶å…·é…ç½®ä¸»è¦åŒ…æ‹¬ï¼šæ²™å‘ã€èŒ¶å‡ ã€ç»„åˆæŸœå’Œå®¢åŽ…é¥°ç‰©ç­‰ã€‚
å®¢åŽ…æ–¹ä½æœå‘çš„ä¸åŒï¼Œå¯¹å®¢åŽ…å®¶å…·çš„è‰²å½©æ­é…è¦æ±‚ä¹Ÿä¸ä¸€æ ·ï¼Œå¦‚è¥¿å‘å®¢åŽ…ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆåœ¨è¿›è¡Œå®¢åŽ…æ­é…æ—¶åº”ä»¥ç»¿è‰²ä½œä¸ºä¸»è‰²è°ƒæ¥é€‰é…æ²™å‘ã€èŒ¶å‡ åŠç»„åˆæŸœçš„é¢œè‰²ï¼Œå†å†³å®šå®¶å…·çš„æ¬¾å¼ã€è§„æ ¼çš„é…ç½®ã€‚
å®¤å†…é£Žæ°´è®¾è®¡é¢œè‰²æ­é…æŠ€å·§ä¹‹ä¹¦æˆ¿ä¹¦æˆ¿çš„è‰²è°ƒåŠ›æ±‚é›…è‡´ã€‚
ä¹¦æˆ¿æ˜¯äººä»¬ç”¨äºŽé˜…è¯»ã€ä¹¦å†™å’Œå­¦ä¹ çš„ä¸€ç§é™æ€å·¥ä½œç©ºé—´ï¼Œäººä»¬è¦æ±‚å¤´è„‘å†·é™ã€æ³¨æ„åŠ›é›†ä¸­ã€å®‰å®ã€‚
å®¤å†…è‰²è°ƒè¦æ±‚ä»¥é›…è‡´ã€åº„é‡ã€å’Œè°ä¸ºä¸»è‰²è°ƒï¼Œå•ä»Žè‰²ç“·ä¸Šæ¥è€ƒè™‘ï¼Œå¹¿å·žçŽ„æ˜“è€å¸ˆå»ºè®®è®¾è®¡å¸ˆä»¬å®œé€‰ç”¨ç°è‰²ã€è¤ç°è‰²ã€è¤ç»¿è‰²ã€æµ…è“è‰²ã€æµ…ç»¿è‰²ç­‰ï¼Œåœ°é¢[0m
[32m[2024-01-05 13:46:30,445] [    INFO][0m - Sample data for valid mode.[0m
[32m[2024-01-05 13:46:30,450] [    INFO][0m - å² ä»–ä¹åä½™å¹´çš„äººç”Ÿé‡Œç¬”è€•ä¸è¾ï¼Œç•™ä¸‹ä¸‰åƒä½™é¦–è„ç‚™äººå£çš„æ­Œè¯ï¼›ä»–ç”ŸäºŽåŒ—äº¬ï¼Œç››åäºŽå°æ¹¾ï¼Œå°†æ™šå¹´ç”Ÿæ´»ç•™ç»™äº†ä»–çš„ç¬¬äºŒæ•…ä¹¡â€”â€”é‡åº†ï¼›ä»–å½“è¿‡è®°è€…ã€æ¼”è¿‡è¯å‰§ã€æœ€åŽå´åœ¨éŸ³ä¹åˆ›ä½œä¸Šæ‰åŽå°½å±•â€¦â€¦å¿˜ä¸äº†ï¼Œç”œèœœèœœï¼Œåž„ä¸Šè¡Œã€‚
ä¸ä¸å°é›¨é‡Œï¼Œå‡ æ—¶å†è§ï¼Œå°åŸŽæ•…äº‹ã€‚
ä»Šå¤©å‡Œæ™¨6ç‚¹11åˆ†ï¼Œæˆ‘å›½åŽè¯­æ­Œæ›²â€œè¯å›æ³°æ–—â€åº„å¥´åœ¨é‡åº†ä¸Žä¸–é•¿è¾žï¼Œäº«å¹´95å²ã€‚
ä¸€ç”Ÿå†™ä¸‹3000å¤šé¦–æ­Œè¯ã€Šç”œèœœèœœã€‹å·²æˆç»å”± åº„å¥´è€å…ˆç”Ÿè¿™ä¸€ç”Ÿå½“è¿‡è®°è€…ã€ç¼–è¾‘ï¼Œæ¼”è¿‡è¯å‰§ï¼Œå¯å°±è¿žä»–è‡ªå·±ä¹Ÿæ²¡æ–™åˆ°ï¼Œä¸€æ¬¡é—²æš‡æ—¶å…‰é‡Œæ‰€å†™ä¸‹çš„ã€Šç»¿å²›å°å¤œæ›²ã€‹ï¼Œå´è®©ä»–ä»¥è¯äººçš„èº«ä»½ä¸€å¤œæˆåï¼Œä»Žæ­¤å‰¯ä¸šå˜æ­£è¡Œï¼Œå¹¶ä¸ºæ­¤ç¬”è€•ä¸è¾äº†äº”åä½™è½½ã€‚
è¯´åˆ°åº„å¥´æ‰€ä½œçš„æ­Œè¯ï¼Œä¸å¾—ä¸æåˆ°èœšå£°ä¸–ç•Œçš„ç”œæ­Œçš‡åŽé‚“ä¸½å›ã€‚
ä¸Šä¸–çºªå…­ä¸ƒåå¹´ä»£ï¼Œå°æ¹¾è‘—åæ­Œæ˜Ÿé‚“ä¸½å›å”±ç€ã€Šç”œèœœèœœã€‹ã€ã€Šå°åŸŽæ•…äº‹ã€‹ã€ã€Šåˆè§ç‚ŠçƒŸã€‹èµ°éäº†ä¸–ç•Œï¼Œè¿™äº›è‡³ä»Šä»åœ¨äººä»¬å£ä¸­ä¼ å”±çš„æ­Œè¯ï¼Œå…¨éƒ¨å‡ºè‡ªåº„å¥´ä¹‹æ‰‹ã€‚
éšç€é‚“ä¸½å›çš„æ­Œå£°ä¼ éä¸–ç•Œï¼Œå†™ä¸‹åŠ¨äººæ­Œè¯çš„åº„å¥´ä¹Ÿè¢«æ›´å¤šçš„äººæ‰€ç†ŸçŸ¥ï¼Œä»–æ‰€ä½œçš„æ¯å¥æ­Œè¯éƒ½å……æ»¡æ¸©æƒ…ï¼Œæ¸©æš–ç€æ¯ä¸€ä¸ªäººçš„å¿ƒçµã€‚
æ­Œè¯è™½æš–ï¼Œå¯åœ¨åº„å¥´è‡ªå·±çœ‹æ¥ï¼Œå†™è¯çš„ç”Ÿæ´»ï¼Œå´æ˜¯å¸¦ç€å‡ åˆ†æ¸…è‹¦ä¹‹æ„ï¼Œä»–æ›¾è‡ªå·±å†™è¿‡ä¸€é¦–æ‰“æ²¹è¯—ï¼Œæ‰“è¶£å†™è¯ç”Ÿæ¶¯â€”â€”â€œåŠæ¯è‹¦èŒ¶åŠæ¯é…’ï¼ŒåŠå¥æ­Œè¯å†™åŠå¤©ï¼›åŠå¤œä¸‰æ›´ä¸¤ç‚¹åŠï¼ŒåŠç¡åŠé†’åŠæ— çœ â€¦â€¦â€ä»Žå¶ä¸€ç»“ç¼˜åˆ°å¦‚ä»Šï¼Œåº„è€å·²åˆ›ä½œäº”åè½½ï¼Œä½œå“è¶…ä¸‰åƒç¯‡ï¼Œäººä»¬éƒ½èµžå…¶ä¸ºâ€œä¸Žæ—¶é—´èµ›è·‘â€ï¼Œè€Œä»–å°†è‡ªå·±ä¸€ç”Ÿçš„åˆ›ä½œæ€»ç»“ä¸ºâ€œè¡Œäº‘æµæ°´äº”åå¹´ï¼ŒåŸé£Žå¼„æœˆæ­Œä¸‰åƒâ€â€œæ­Œè¯ä¸èƒ½å¤ªé•¿ã€å¤ªéš¾ï¼Œæˆ‘ä»¬æ˜¯ä¸ºåƒåƒä¸‡ä¸‡æ™®é€šäººå†™æ­Œï¼Œè¦ç®€å•æ˜“æ‡‚ï¼Œåˆè¦ä¼ æƒ…è¾¾æ„ï¼Œè¦å†™å‡ºä»–ä»¬çš„å¿ƒå£°â€ï¼Œåº„å¥´è¯´ï¼Œå¾ˆå¤šäººè¯´ä»–çš„æ­Œå†™å‡ºäº†æƒ³è¯´åˆè¯´ä¸å‡ºæ¥çš„è¯ï¼Œå°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚
ç”œèœœèœœç”œèœœèœœ ä½ ç¬‘å¾—ç”œèœœèœœå¥½è±¡èŠ±å„¿å¼€åœ¨æ˜¥é£Žé‡Œå¼€åœ¨æ˜¥é£Žé‡Œåœ¨å“ªé‡Œ åœ¨å“ªé‡Œè§è¿‡ä½ ä½ çš„ç¬‘å®¹è¿™æ ·ç†Ÿæ‚‰æˆ‘ä¸€æ—¶æƒ³ä¸èµ·å•Š åœ¨[0m
[32m[2024-01-05 13:46:32,008] [    INFO][0m - The global seed is set to 44, local seed is set to 48 and random seed is set to 42.[0m
[32m[2024-01-05 13:46:32,076] [    INFO][0m - max_steps is given, it will override any value given in num_train_epochs[0m
[32m[2024-01-05 13:46:32,076] [    INFO][0m - Using half precision[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - ============================================================[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m -     Training Configuration Arguments    [0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - paddlenlp commit id           : be130d089c15d41748580b46876528870b45c35c.dirty[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - _no_sync_in_gradient_accumulation: True[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_beta1                    : 0.9[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_beta2                    : 0.999[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - adam_epsilon                  : 1e-08[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_custom_black_list         : None[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_custom_white_list         : None[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - amp_master_grad               : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - autotuner_benchmark           : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - bf16                          : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - bf16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,099] [    INFO][0m - current_device                : gpu:6[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - data_parallel_degree          : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - data_parallel_rank            : 0[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataloader_drop_last          : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataloader_num_workers        : 0[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataset_rank                  : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - dataset_world_size            : 2[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - decay_steps                   : 2000[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - device                        : gpu[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - disable_tqdm                  : True[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - distributed_dataloader        : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_eval                       : 1[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_export                     : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_predict                    : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - do_train                      : True[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - enable_linear_fused_grad_add  : False[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_accumulation_steps       : 16[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_batch_size               : 8[0m
[32m[2024-01-05 13:46:32,100] [    INFO][0m - eval_iters                    : 10[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - eval_steps                    : 1[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - evaluation_strategy           : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - flatten_param_grads           : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - force_reshard_pp              : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16                          : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16_full_eval                : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - fp16_opt_level                : O2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - gradient_accumulation_steps   : 128[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - greater_is_better             : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - hybrid_parallel_topo_order    : pp_first[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - ignore_data_skip              : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - ignore_load_lr_and_optim      : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - label_names                   : None[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - lazy_data_processing          : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - learning_rate                 : 0.001[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - load_best_model_at_end        : True[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - load_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - local_process_index           : 2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - local_rank                    : 2[0m
[32m[2024-01-05 13:46:32,101] [    INFO][0m - log_level                     : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - log_level_replica             : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - log_on_each_node              : True[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_dir                   : ./checkpoints/runs/Jan05_13-45-43_bddwd-inf-hic-k8s-a100-a12ni6-0015.bddwd.baidu.com[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_first_step            : False[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_steps                 : 1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logging_strategy              : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - logical_process_index         : 2[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - lr_end                        : 1e-07[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - lr_scheduler_type             : SchedulerType.COSINE[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_evaluate_steps            : -1[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_grad_norm                 : 1.0[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - max_steps                     : 2000[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - metric_for_best_model         : loss[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - min_learning_rate             : 1e-05[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - minimum_eval_times            : None[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - no_cuda                       : False[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - num_cycles                    : 0.5[0m
[32m[2024-01-05 13:46:32,102] [    INFO][0m - num_train_epochs              : 3.0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - optim                         : OptimizerNames.ADAMW[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - optimizer_name_suffix         : tp00_shard01[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - output_dir                    : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - overwrite_output_dir          : False[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - past_index                    : -1[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - per_device_eval_batch_size    : 8[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - per_device_train_batch_size   : 4[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_config      : [0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_degree      : 1[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - pipeline_parallel_rank        : 0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - power                         : 1.0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - prediction_loss_only          : False[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - process_index                 : 2[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - recompute                     : 0[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - remove_unused_columns         : True[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - report_to                     : ['visualdl'][0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - resume_from_checkpoint        : None[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - run_name                      : ./checkpoints/[0m
[32m[2024-01-05 13:46:32,103] [    INFO][0m - save_on_each_node             : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_sharded_model            : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_steps                    : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_strategy                 : IntervalStrategy.STEPS[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - save_total_limit              : 3[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - scale_loss                    : 32768[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - seed                          : 42[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sep_parallel_degree           : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding                      : [<ShardingOption.SHARD_OP: 'stage1'>][0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_degree               : -1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_config      : [0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_degree      : 2[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - sharding_parallel_rank        : 1[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_load_dataset           : True[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_load_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_log                    : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save                   : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save_model_state       : False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - should_save_sharding_stage1_model: False[0m
[32m[2024-01-05 13:46:32,104] [    INFO][0m - skip_memory_metrics           : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - skip_profile_timer            : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_config        : [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_degree        : 2[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - tensor_parallel_rank          : 0[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - test_iters                    : 100[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - to_static                     : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - train_batch_size              : 4[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - unified_checkpoint            : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - unified_checkpoint_config     : [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - use_auto_parallel             : False[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - use_hybrid_parallel           : True[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - warmup_ratio                  : 0.05[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - warmup_steps                  : 0[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - weight_decay                  : 0.01[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - weight_name_suffix            : tp00[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - world_size                    : 4[0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - [0m
[32m[2024-01-05 13:46:32,105] [    INFO][0m - Starting training from resume_from_checkpoint : None[0m
[2024-01-05 13:46:32,112] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,150] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,233] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
/root/paddlejob/workspace/zhengxiong/pd_zx/lib/python3.8/site-packages/paddle/distributed/communication/group.py:114: UserWarning: Current global rank 2 is not in group _default_pg12
  warnings.warn(
[2024-01-05 13:46:32,291] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,291] [    INFO] tensor_parallel.py:32 - start broadcast mp parameters
[2024-01-05 13:46:32,330] [    INFO] tensor_parallel.py:36 - start broadcast sharding parameters
[2024-01-05 13:46:32,383] [    INFO] tensor_parallel.py:39 - start broadcast dp parameters
[2024-01-05 13:46:32,435] [    INFO] tensor_parallel.py:42 - mp's parameters is ready
[2024-01-05 13:46:32,436] [ WARNING] hybrid_parallel_optimizer.py:293 - While using ClipGradByGlobalNorm in TensorParallel, PipelineParallel or Sharding, the grad clip of original optimizer will be changed.
[32m[2024-01-05 13:46:32,436] [    INFO][0m - ***** Running training *****[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Num examples = 5,803,418[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Num Epochs = 1[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Instantaneous batch size per device = 4[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total train batch size (w. parallel, distributed & accumulation) = 1024[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Gradient Accumulation steps = 128[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total optimization steps = 2,000[0m
[32m[2024-01-05 13:46:32,436] [    INFO][0m -   Total num train samples = 2,048,000[0m
[32m[2024-01-05 13:46:32,440] [    INFO][0m -   Number of trainable parameters = 258,572,288 (per device)[0m
[32m[2024-01-05 13:46:32,450] [    INFO][0m -   Number of trainable parameters = 517,144,576 (all devices, roughly)[0m
Found inf or nan, current scale is: 32768.0, decrease to: 32768.0*0.5
[33m[2024-01-05 13:47:20,138] [ WARNING][0m - optimizer not run, scale_before: 32768.0, scale_after: 16384.0[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m - ***** Running Evaluation *****[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Num examples = 613450[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total prediction steps = 10[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Pre device batch size = 8[0m
[32m[2024-01-05 13:47:20,143] [    INFO][0m -   Total Batch size = 16[0m
[32m[2024-01-05 13:47:22,865] [    INFO][0m - Saving optimizer files.[0m
