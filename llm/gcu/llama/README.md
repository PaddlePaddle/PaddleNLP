## ğŸš£â€â™‚ï¸ ä½¿ç”¨PaddleNLPåœ¨ç‡§åŸS60ä¸‹è¿è¡Œllama2-13bæ¨¡å‹ ğŸš£

ç‡§åŸS60ï¼ˆ[äº†è§£ç‡§åŸ](https://www.enflame-tech.com/)ï¼‰æ˜¯é¢å‘æ•°æ®ä¸­å¿ƒå¤§è§„æ¨¡éƒ¨ç½²çš„æ–°ä¸€ä»£äººå·¥æ™ºèƒ½æ¨ç†åŠ é€Ÿå¡ï¼Œæ»¡è¶³å¤§è¯­è¨€æ¨¡å‹ã€æœå¹¿æ¨åŠä¼ ç»Ÿæ¨¡å‹çš„éœ€æ±‚ï¼Œå…·æœ‰æ¨¡å‹è¦†ç›–é¢å¹¿ã€æ˜“ç”¨æ€§å¼ºã€æ˜“è¿ç§»æ˜“éƒ¨ç½²ç­‰ç‰¹ç‚¹ï¼Œå¯å¹¿æ³›åº”ç”¨äºå›¾åƒåŠæ–‡æœ¬ç”Ÿæˆç­‰åº”ç”¨ã€æœç´¢ä¸æ¨èã€æ–‡æœ¬ã€å›¾åƒåŠè¯­éŸ³è¯†åˆ«ç­‰ä¸»æµæ¨ç†åœºæ™¯ã€‚

PaddleNLPåœ¨ç‡§åŸS60ä¸Šå¯¹llama2-13Bæ¨¡å‹è¿›è¡Œäº†æ·±åº¦é€‚é…å’Œä¼˜åŒ–ï¼Œå®ç°äº†GCUæ¨ç†å…¥å£å’ŒGPUçš„åŸºæœ¬ç»Ÿä¸€ï¼Œä»…éœ€ä¿®æ”¹deviceå³å¯å®Œæˆæ¨ç†ä»»åŠ¡çš„è¿ç§»ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€

### 0. æœºå™¨å‡†å¤‡ã€‚å¿«é€Ÿå¼€å§‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦å‡†å¤‡ä¸€å°æ’æœ‰ç‡§åŸS60åŠ é€Ÿå¡çš„æœºå™¨ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š

| èŠ¯ç‰‡ç±»å‹ | é©±åŠ¨ç‰ˆæœ¬ | TopsPlatformç‰ˆæœ¬ |
| :---: | :---: | :---: |
| ç‡§åŸS60 | 1.0.5.1 | TopsPlatform_1.0.5.1-2c3111 |

**æ³¨ï¼šå¦‚æœéœ€è¦éªŒè¯æ‚¨çš„æœºå™¨æ˜¯å¦æ’æœ‰ç‡§åŸS60åŠ é€Ÿå¡ï¼Œåªéœ€ç³»ç»Ÿç¯å¢ƒä¸‹è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼Œçœ‹æ˜¯å¦æœ‰è¾“å‡ºï¼š**
```bash
lspci | grep S60

# ä¾‹å¦‚ï¼šlspci | grep S60 , è¾“å‡ºå¦‚ä¸‹
01:00.0 Processing accelerators: Shanghai Enflame Technology Co. Ltd S60 [Enflame] (rev 01)
09:00.0 Processing accelerators: Shanghai Enflame Technology Co. Ltd S60 [Enflame] (rev 01)
```
### 1. ç¯å¢ƒå‡†å¤‡ï¼š(è¿™å°†èŠ±è´¹æ‚¨10ï½20minæ—¶é—´)

1. åˆå§‹åŒ–ç¯å¢ƒï¼Œå®‰è£…é©±åŠ¨<br/>
  **æ³¨ï¼šæ‚¨å¯ä»¥è”ç³»ç‡§åŸ(Email: developer-enflame@enflame-tech.com)ä»¥è·å–è½¯ä»¶é©±åŠ¨åŒ…å’Œå…¶ä»–å¸®åŠ©**
```bash
# å‡è®¾å®‰è£…åŒ…ä½äºï¼š/home/paddle_user/deps/ï¼Œ åç§°ä¸ºï¼šTopsPlatform.tar.gz
cd /home/paddle_user/deps/ && tar -zxf TopsPlatform.tar.gz
cd TopsPlatform
./TopsPlatform_1.0.5.1-2c3111_deb_amd64.run --no-auto-load --driver -y
```
2. æ‹‰å–é•œåƒ
```bash
# æ³¨æ„æ­¤é•œåƒä»…ä¸ºpaddleå¼€å‘ç¯å¢ƒï¼Œé•œåƒä¸­ä¸åŒ…å«é¢„ç¼–è¯‘çš„é£æ¡¨å®‰è£…åŒ…ã€TopsPlatformå®‰è£…åŒ…ç­‰
docker pull registry.baidubce.com/paddlepaddle/paddle:latest-dev
```
3. å‚è€ƒå¦‚ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨
```bash
docker run --name paddle-gcu-test -v /home:/home --network=host --ipc=host -it --privileged registry.baidubce.com/paddlepaddle/paddle:latest-dev /bin/bash
```
4. å®‰è£…ç¼–è¯‘å¥—ä»¶
```bash
# å®‰è£…cmakeç”¨äºæºç ç¼–è¯‘
cd /root
wget https://github.com/Kitware/CMake/releases/download/v3.23.4/cmake-3.23.4-linux-x86_64.tar.gz
tar -zxf ./cmake-3.23.4-linux-x86_64.tar.gz
ln -sf /root/cmake-3.23.4-linux-x86_64/bin/cmake /usr/bin/cmake && ln -sf /root/cmake-3.23.4-linux-x86_64/bin/ctest /usr/bin/ctest
```
5. å®‰è£…ç‡§åŸè½¯ä»¶æ ˆ
```bash
# åœ¨paddle dockeré‡Œå®‰è£…ç‡§åŸè½¯ä»¶æ ˆï¼Œç¼–è¯‘æ‰§è¡Œä¼šä¾èµ–sdkã€runtimeã€ecclã€atenã€topstx(for profiler)
cd /home/paddle_user/deps/TopsPlatform
./TopsPlatform_1.0.5.1-2c3111_deb_amd64.run --no-auto-load -y
dpkg -i topsfactor_*.deb tops-sdk_*.deb eccl_*.deb topsaten_*.deb
```
6. å®‰è£…PaddlePaddle
```bash
# PaddlePaddleã€é£æ¡¨ã€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›è¿ç®—åŸºç¡€èƒ½åŠ›
python -m pip install paddlepaddle==3.0.0b0 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/
```
7. ç¼–è¯‘å®‰è£…PaddleCustomDevice<br/>
  PaddleCustomDeviceæ˜¯PaddlePaddleã€é£æ¡¨ã€æ·±åº¦å­¦ä¹ æ¡†æ¶çš„è‡ªå®šä¹‰ç¡¬ä»¶æ¥å…¥å®ç°ï¼Œæä¾›GCUçš„è®¾å¤‡ç®¡ç†åŠç®—å­å®ç°ã€‚<br/>
  **æ³¨ï¼šå½“å‰ä»éœ€æºç ç¼–è¯‘PaddleCustomDeviceï¼Œpaddle-custom-gcué¢„ç¼–è¯‘ç‰ˆæœ¬å¾…å‘å¸ƒ**
```bash
# ä¸‹è½½æºç 
mkdir -p /home/paddle_user/workspace && cd /home/paddle_user/workspace
git clone https://github.com/PaddlePaddle/PaddleCustomDevice.git
cd PaddleCustomDevice
# åˆ‡æ¢åˆ°v3.0.0-beta1ç‰ˆæœ¬
git checkout -b v3.0-beta v3.0.0-beta1
# ä¾èµ–çš„ç®—å­åº“
cp /home/paddle_user/deps/TopsPlatform/libtopsop.a ./backends/gcu/kernels/topsflame/
# å¼€å§‹ç¼–è¯‘ï¼Œä¾èµ–çš„ç¬¬ä¸‰æ–¹åº“ä¼šåœ¨é¦–æ¬¡ç¼–è¯‘æ—¶æŒ‰éœ€ä¸‹è½½ã€‚ä»githubä¸‹è½½å¯èƒ½ä¼šæ¯”è¾ƒæ…¢
cd backends/gcu/ && mkdir -p build && cd build
export PADDLE_CUSTOM_PATH=`python -c "import re, paddle; print(re.compile('/__init__.py.*').sub('',paddle.__file__))"`
cmake .. -DWITH_TESTING=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DPY_VERSION=3.9
make -j64
# ç¼–è¯‘äº§ç‰©åœ¨build/distï¼Œä½¿ç”¨pipå®‰è£…
python -m pip install --force-reinstall -U dist/paddle_custom_gcu*.whl
```
8. ä¸‹è½½PaddleNLPä»“åº“ä»£ç ï¼Œå¹¶å®‰è£…ä¾èµ–
```bash
# PaddleNLPæ˜¯åŸºäºPaddlePaddleã€é£æ¡¨ã€çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§è¯­è¨€æ¨¡å‹(LLM)å¼€å‘åº“ï¼Œå­˜æ”¾äº†åŸºäºã€é£æ¡¨ã€æ¡†æ¶å®ç°çš„å„ç§å¤§æ¨¡å‹ï¼Œllama2-13Bæ¨¡å‹ä¹ŸåŒ…å«å…¶ä¸­ã€‚ä¸ºäº†ä¾¿äºæ‚¨æ›´å¥½åœ°ä½¿ç”¨PaddleNLPï¼Œæ‚¨éœ€è¦cloneæ•´ä¸ªä»“åº“ã€‚
cd /home/paddle_user/workspace
git clone https://github.com/PaddlePaddle/PaddleNLP.git
cd PaddleNLP
# åˆ‡æ¢åˆ°v3.0.0-beta0ç‰ˆæœ¬
git checkout -b v3.0-beta v3.0.0-beta0
# å®‰è£…ä¾èµ–åº“
python -m pip install -r requirements.txt
# æºç ç¼–è¯‘å®‰è£… paddlenlp v3.0.0-beta0
python setup.py bdist_wheel && python -m pip uninstall paddlenlp -y && python -m pip install dist/paddlenlp*
```
### 2. æ•°æ®å‡†å¤‡ï¼š(è¿™å°†èŠ±è´¹æ‚¨2ï½5minæ—¶é—´)
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨wikitext-103ä¸Šè¯„ä¼°
```bash
cd llm/gcu/llama
wget https://paddlenlp.bj.bcebos.com/data/benchmark/wikitext-103.tar.gz
tar -zxf wikitext-103.tar.gz
```
### 3. æ¨ç†ï¼š(è¿™å°†èŠ±è´¹æ‚¨15~30minæ—¶é—´)
æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼š
```bash
bash predict_llama_gcu.sh
```
é¦–æ¬¡æ¨ç†å°†è‡ªåŠ¨ä¸‹è½½æƒé‡å’Œé…ç½®ï¼Œä½äº```/root/.paddlenlp/models/__internal_testing__/sci-benchmark-llama-13b-5k/```ç›®å½•ä¸‹ã€‚<br/>
**æ¨èåœ¨é¦–æ¬¡ä¸‹è½½æƒé‡æ–‡ä»¶åæ›´æ”¹æ¨ç†é…ç½®æ–‡ä»¶ï¼Œä»¥è·å–æ›´å¤§çš„æ€§èƒ½æå‡ã€‚**<br/>
å°†```/root/.paddlenlp/models/__internal_testing__/sci-benchmark-llama-13b-5k/config.json```æ›´æ”¹ä¸ºä¸‹é¢çš„å†…å®¹ï¼š
```json
{
  "alibi": false,
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 1,
  "dtype": "float16",
  "eos_token_id": 2,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 5120,
  "initializer_range": 0.002,
  "intermediate_size": 13824,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "rms_norm_eps": 1e-06,
  "rope_scaling_factor": 1.0,
  "rope_scaling_type": null,
  "tie_word_embeddings": false,
  "use_recompute": false,
  "virtual_pp_degree": 1,
  "vocab_size": 32000,
  "use_fused_rope": true,
  "use_fused_rms_norm": true,
  "use_flash_attention": true,
  "fuse_attention_qkv": true,
  "fuse_attention_ffn": true
}
```
æˆåŠŸè¿è¡Œåï¼Œå¯ä»¥æŸ¥çœ‹åˆ°æ¨ç†ç»“æœçš„å›°æƒ‘åº¦æŒ‡æ ‡(ppl)ï¼Œæœ€ç»ˆè¯„ä¼°ç»“æœppl: 12.785ã€‚
```bash
[2024-08-16 01:55:24,753] [    INFO] - step 2000, batch: 2000, loss: 2.323283, speed: 1.40 step/s
[2024-08-16 01:55:31,813] [    INFO] - step 2010, batch: 2010, loss: 2.341318, speed: 1.42 step/s
[2024-08-16 01:55:38,859] [    INFO] - step 2020, batch: 2020, loss: 2.357684, speed: 1.42 step/s
[2024-08-16 01:55:45,897] [    INFO] - step 2030, batch: 2030, loss: 2.371745, speed: 1.42 step/s
[2024-08-16 01:55:52,942] [    INFO] - step 2040, batch: 2040, loss: 2.386801, speed: 1.42 step/s
[2024-08-16 01:55:59,991] [    INFO] - step 2050, batch: 2050, loss: 2.399686, speed: 1.42 step/s
[2024-08-16 01:56:07,037] [    INFO] - step 2060, batch: 2060, loss: 2.410638, speed: 1.42 step/s
[2024-08-16 01:56:14,080] [    INFO] - step 2070, batch: 2070, loss: 2.421459, speed: 1.42 step/s
[2024-08-16 01:56:21,141] [    INFO] - step 2080, batch: 2080, loss: 2.431433, speed: 1.42 step/s
[2024-08-16 01:56:28,170] [    INFO] - step 2090, batch: 2090, loss: 2.443705, speed: 1.42 step/s
[2024-08-16 01:56:35,238] [    INFO] - step 2100, batch: 2100, loss: 2.454847, speed: 1.41 step/s
[2024-08-16 01:56:42,275] [    INFO] - step 2110, batch: 2110, loss: 2.464446, speed: 1.42 step/s
[2024-08-16 01:56:49,323] [    INFO] - step 2120, batch: 2120, loss: 2.475107, speed: 1.42 step/s
[2024-08-16 01:56:56,348] [    INFO] - step 2130, batch: 2130, loss: 2.487760, speed: 1.42 step/s
[2024-08-16 01:57:03,372] [    INFO] - step 2140, batch: 2140, loss: 2.501706, speed: 1.42 step/s
[2024-08-16 01:57:10,395] [    INFO] - step 2150, batch: 2150, loss: 2.513665, speed: 1.42 step/s
[2024-08-16 01:57:17,411] [    INFO] - step 2160, batch: 2160, loss: 2.524555, speed: 1.43 step/s
[2024-08-16 01:57:24,437] [    INFO] - step 2170, batch: 2170, loss: 2.536793, speed: 1.42 step/s
[2024-08-16 01:57:31,461] [    INFO] - step 2180, batch: 2180, loss: 2.547897, speed: 1.42 step/s
[2024-08-16 01:57:34,378] [    INFO] -  validation results on ./wikitext-103/wiki.valid.tokens | avg loss: 2.5483E+00 | ppl: 1.2785E+01 | adjusted ppl: 2.6434E+01 | token ratio: 1.285056584007609 |
'Original Tokens: 279682, Detokenized tokens: 217642'
'Original Tokens: 279682, Detokenized tokens: 217642'
I0816 01:57:34.386860 10925 runtime.cc:130] Backend GCU finalize device:0
I0816 01:57:34.386868 10925 runtime.cc:98] Backend GCU Finalize
```
