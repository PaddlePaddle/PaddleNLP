# é«˜æ€§èƒ½æ¨ç†

PaddleNLP ä¸­å·²ç»æ·»åŠ é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ç›¸å…³å®ç°ï¼Œæ”¯æŒï¼š

| Model                       | Inference Model | PTuning | Wint8 | PTQ |
|-----------------------------|-----------------|---------|-------|-----|
| [LLaMA1/2](./llama)         | âœ…               | âœ…       | âœ…     | ğŸš§   |
| [ChatGLM](./chatglm)        | âœ…               | âœ…       | âœ…     | âŒ   |
| [ChatGLM2](./chatglm2)      | âœ…               | âŒ       | âŒ     | âŒ   |
| [Bloom](./bloom)            | âœ…               | âœ…       | âœ…     | âŒ   |
| [GPT-3](./gpt-3)            | âœ…               | âŒ       | âŒ     | âŒ   |
| [Qwen](./qwen)              | âŒ               | âŒ       | âŒ     | âŒ   |
| [BaiChuan1](./baichuan)     | âœ…               | âœ…       | âœ…     | ğŸš§   |
| [BaiChuan2-7B](./baichuan)  | âœ…               | âœ…       | âœ…     | ğŸš§   |
| [BaiChuan2-13B](./baichuan) | âŒ               | âŒ       | âŒ     | âŒ   |

[TOC]

## å®‰è£…è‡ªå®šä¹‰ç®—å­åº“

PaddleNLP é’ˆå¯¹äºTransformer ç³»åˆ—ç¼–å†™äº†é«˜æ€§èƒ½è‡ªå®šä¹‰ç®—å­ï¼Œæå‡æ¨¡å‹åœ¨æ¨ç†å’Œè§£ç è¿‡ç¨‹ä¸­çš„æ€§èƒ½ã€‚

```shell
git clone https://github.com/PaddlePaddle/PaddleNLP
cd ./paddlenlp/csrc && python setup_cuda.py install
```

## é¢„è®­ç»ƒ & SFT æ¨¡å‹ & Lora æ¨ç†

> Lora æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰æ˜¯éœ€è¦åˆå¹¶å‚æ•°ï¼Œè¯¦ç»†å¯è§ï¼š[åˆå¹¶ Lora å‚æ•°](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/llm#37-lora-%E5%8F%82%E6%95%B0%E5%90%88%E5%B9%B6)ã€‚

é¢„è®­ç»ƒæ¨¡å‹å’Œ SFT æ¨¡å‹åœ¨ç»“æ„ä¸Šä¸€æ ·ï¼Œæ¨ç†åŠŸèƒ½åŒ…å«ï¼š

* åŠ¨æ€å›¾æ¨ç†
* é™æ€å›¾æ¨ç†

### åŠ¨æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --dtype float16
```

### é™æ€å›¾æ¨ç†

åœ¨é™æ€å›¾æ¨ç†ä¹‹å‰éœ€è¦æ‰§è¡ŒåŠ¨è½¬é™ï¼Œå°†æ¨¡å‹è½¬åŒ–ä¸ºé™æ€å›¾ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

* åŠ¨è½¬é™

```python
python export_model.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --output_path ./inference \
    --dtype float16
```

* é™æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path ./inference \
    --inference_model \
    --dtype "float16" \
    --mode "static"
```

## PTuning æ¨¡å‹æ¨ç†

PTuning æ¨¡å‹å’Œé PTuning æ¨¡å‹æ¨ç†éå¸¸ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºå‰è€…ä¼šæ·»åŠ  pre_caches.npy çš„è¾“å…¥ï¼ŒåŠ¨é™æ¨ç†å‘½ä»¤å¯è§ï¼š

### åŠ¨æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --export_precache true \
    --prefix_path /path/to/pre_caches \
    --dtype float16
```

### é™æ€å›¾æ¨ç†

åœ¨é™æ€å›¾æ¨ç†ä¹‹å‰éœ€è¦æ‰§è¡ŒåŠ¨è½¬é™ï¼Œå°†æ¨¡å‹è½¬åŒ–ä¸ºé™æ€å›¾ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

* åŠ¨è½¬é™

```python
python export_model.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --export_precache true \
    --output_path ./inference_ptuning \
    --dtype float16
```

* é™æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path ./inference_ptuning \
    --inference_model \
    --dtype "float16" \
    --export_precache true \
    --prefix_path /path/to/pre_caches \
    --mode "static"
```

## Weight Only Int8/4 æ¨ç†

Weight Only Int8/4 çš„æ¨ç†è„šæœ¬ç›¸æ¯”SFT æ¨¡å‹æ¨ç†ä»…å¢åŠ äº†ï¼š`quant_type`å‚æ•°ï¼Œå€¼ä¸ºï¼š`weight_only_int8`å’Œ `weight_only_int4`ã€‚

> å½“å‰ weight_only_int8/4 ä»…æ”¯æŒA100ï¼ŒV100 ä¸Šçš„ weight only int8/4 å­˜åœ¨ç²¾åº¦é—®é¢˜ã€‚

### åŠ¨æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --quant_type weight_only_int8 \
    --dtype float16
```

### é™æ€å›¾æ¨ç†

åœ¨é™æ€å›¾æ¨ç†ä¹‹å‰éœ€è¦æ‰§è¡ŒåŠ¨è½¬é™ï¼Œå°†æ¨¡å‹è½¬åŒ–ä¸ºé™æ€å›¾ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

* åŠ¨è½¬é™

```python
python export_model.py \
    --model_name_or_path meta-llama/Llama-2-7b-chat \
    --inference_model \
    --quant_type weight_only_int8 \
    --output_path ./inference \
    --dtype float16
```

* é™æ€å›¾æ¨ç†

```python
python predictor.py \
    --model_name_or_path ./inference \
    --inference_model \
    --quant_type weight_only_int8 \
    --dtype "float16" \
    --mode "static"
```

## PTQ Int8 æ¨ç†

è¿™ä¸€æ­¥ä¾èµ–PTQæ ¡å‡†äº§å‡ºçš„é‡åŒ–æ¨¡å‹ï¼Œæ— é¡»é¢å¤–è®¾ç½®ç›¸å…³å‚æ•°ã€‚
### åŠ¨æ€å›¾æ¨ç†
```shell
python predictor.py \
    --model_name_or_path checkpoints/llama_ptq_ckpts \
    --dtype float16 \
    --max_length 1024 \
    --mode "dynamic" \
    --inference_model
```


### é™æ€å›¾æ¨ç†
åœ¨é™æ€å›¾æ¨ç†ä¹‹å‰éœ€è¦æ‰§è¡ŒåŠ¨è½¬é™ï¼Œå°†æ¨¡å‹è½¬åŒ–ä¸ºé™æ€å›¾ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

* åŠ¨è½¬é™

```shell
python export_model.py \
    --model_name_or_path checkpoints/llama_ptq_ckpts \
    --output_path ./inference_ptq \
    --dtype float16 \
    --inference_model
```

* é™æ€å›¾æ¨ç†

```shell
# ä»¥ä¸‹ç¯å¢ƒå˜é‡ç”¨äºå¼€å¯int8çŸ©é˜µä¹˜çš„ç®—æ³•é€‰æ‹©ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œæ‰“å¼€ä¹‹åç¬¬ä¸€æ¬¡æ‰§è¡Œä¼šæ‰§è¡Œç®—æ³•é€‰æ‹©ä»è€Œå¯¼è‡´é€Ÿåº¦è¾ƒæ…¢ã€‚
export FLAGS_use_autotune=1
export FLAGS_cublaslt_exhaustive_search_times=10
export FLAGS_cache_inference_while_scope=1

python predictor.py \
    --model_name_or_path ./inference_ptq \
    --dtype float16 \
    --max_length 1024 \
    --mode "static" \
    --inference_model
```

## å¤šå¡æ¨ç†

TODO: æœªæ¥å°†æ”¯æŒæ›´å¤šå¤šå¡æ¨ç†æ–‡æ¡£è¯´æ˜

## FastLLMDeploy éƒ¨ç½²

TODO: æœªæ¥å°†è”åˆ [FastLLMDeploy](https://github.com/PaddlePaddle/FastDeploy) ç»™å‡ºæ›´å¤šç”Ÿäº§ç¯å¢ƒä¸‹çš„é«˜æ€§èƒ½æ¨ç†æ¨¡å‹éƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚

## å‚æ•°ä»‹ç»

- `model_name_or_path`: å¿…é¡»ï¼Œé¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è€…æœ¬åœ°çš„æ¨¡å‹è·¯å¾„ï¼Œç”¨äºçƒ­å¯æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `batch_size`: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º8ã€‚è¯¥å‚æ•°è¶Šå¤§ï¼Œå ç”¨æ˜¾å­˜è¶Šé«˜ï¼›è¯¥å‚æ•°è¶Šå°ï¼Œå ç”¨æ˜¾å­˜è¶Šä½ã€‚
- `src_length`: æ¨¡å‹è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§tokené•¿åº¦ï¼Œé»˜è®¤ä¸º1024ã€‚
- `max_length`:æ¨¡å‹è¾“å…¥ï¼ˆä¸Šä¸‹æ–‡+ç”Ÿæˆå†…å®¹ï¼‰çš„æœ€å¤§tokené•¿åº¦, é»˜è®¤ä¸º2048ã€‚
- `lora_path`: LoRAå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹LoRAå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `prefix_path`: Prefix Tuningå‚æ•°å’Œé…ç½®è·¯å¾„ï¼Œå¯¹Prefix Tuningå‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `top_k`: â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¸º top-k è¿‡æ»¤ä¿ç•™çš„æœ€é«˜æ¦‚ç‡æ ‡è®°çš„æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œç­‰ä»·äºè´ªå¿ƒç­–ç•¥ã€‚
- `top_p`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ top-p è¿‡æ»¤çš„ç´¯ç§¯æ¦‚ç‡ã€‚é»˜è®¤ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚
- `temperature`:â€œé‡‡æ ·â€ç­–ç•¥ä¸­ä¼šå¯¹è¾“å‡ºlogité™¤ä»¥temperatureã€‚é»˜è®¤ä¸º1.0ï¼Œè¡¨ç¤ºä¸èµ·ä½œç”¨ã€‚
- `data_file`:å¿…é¡»ï¼Œå¾…æ¨ç†jsonæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºNoneã€‚
- `output_file`:ä¿å­˜æ¨ç†ç»“æœæ–‡ä»¶åï¼Œé»˜è®¤ä¸ºoutput.jsonã€‚
- `device`: è¿è¡Œç¯å¢ƒï¼Œé»˜è®¤ä¸ºgpuã€‚
- `dtype`: æ¨¡å‹å‚æ•°dtypeï¼Œé»˜è®¤ä¸ºNoneã€‚å¦‚æœæ²¡æœ‰ä¼ å…¥`lora_path`ã€`prefix_path`åˆ™å¿…é¡»ä¼ å…¥
- `model_type`: åˆå§‹åŒ–ä¸åŒç±»å‹æ¨¡å‹ï¼Œgpt-3: GPTForCausalLM; ernie-3.5-se: Ernie35ForCausalLM; é»˜è®¤ä¸º Noneã€‚
- `mode`: ä½¿ç”¨åŠ¨æ€å›¾æˆ–è€…é™æ€å›¾æ¨ç†ï¼Œå€¼ä¸ºï¼š[dynamic, static]ï¼Œé»˜è®¤ä¸º dynamicã€‚
- `inference_model`: æ˜¯å¦ä½¿ç”¨Inference Model æ¨ç†ï¼Œé»˜è®¤å€¼ä¸º Falseã€‚
