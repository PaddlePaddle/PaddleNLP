# 手把手搭建一个语义检索系统 

## 1. 场景概述

检索系统存在于我们日常使用的很多产品中，比如商品搜索系统、学术文献检索系等等，本方案提供了检索系统完整实现。限定场景是用户通过输入检索词query，快速得在海量数据中查找相似文档。

所谓语义检索（也称基于向量的检索），是指检索系统不再拘泥于用户query字面本身，而是能精准捕捉到用户query后面的真正意图并以此来搜索，从而更准确地向用户返回最符合的结果。通过使用最先进的语言模型找到文本的向量表示，在高维向量空间中对它们进行索引，并度量查询向量与索引文档的相似程度，从而解决了关键词索引带来的缺陷。

## 2. 产品功能介绍

通常检索业务的数据都比较庞大，都会分为召回（索引）、排序两个环节。召回阶段主要是从至少千万级别的候选集合里面，筛选出相关的文档，这样候选集合的数目就会大大降低，在之后的排序阶段就可以使用一些复杂的模型做精细化或者个性化的排序。排序是对召回方法的得到的候选结果进行统一的打分，这个召回方法可能是多种方法的召回结果的聚合，经过统一的打分以后就能够选出最优的TopK的结果。

### 2.1 系统特色
    
+ 易用性( or 低门槛)
    + 手把手搭建起检索系统
    + 无需标注数据也能构建检索系统
    + 一站式提供 训练、预测、ANN 引擎能力

+ 效果好
    + 针对各数据场景专业的基础解决方案
        + 仅有无监督数据: SimCSE
        + 仅有有监督数据: InBatchNegative
        + 无监督数据 + 有监督数据都有
    + 进一步优化方案: 面向领域的预训练 Post-Training
+ 性能快
    + 基于 PaddleInference 快速抽取向量性能数据
    + Milvus 引擎相关性能
    + ANN 建库性能
    + ANN 查询相应性能

###  2.2 架构&功能

索引环节有两类方法：基于字面的关键词索引；语义索引。语义索引能够较好地表征语义信息，解决字面不相似但语义相似的情形。本系统针对的是语义索引的方案。下面就详细介绍整个方案的架构和功能。

#### 2.2.1 结构图整体介绍

下面是整个索引系统的召回系统和排序系统的架构：

![](./img/whole_module.png)


#### 2.2.2 召回模块

召回模块需要从千亿、万亿等海量数据中快速召回候选数据。首先需要抽取数据集文本的Embedding，然后借助向量搜索引擎实现高效ANN，实现候选集召回。

![](./img/recall_mindmap.png)

我们针对大规模无监督数据、有监督数据两种情况推出三种语义索引方案，如下图所示，您可以参照此方案，快速建立语义索引。数据量要求参考下表：

|  无监督数据 |  有监督数据 | 召回方案 |
| ------------ | ------------ | ------------ | 
|  多 |  无 | SimCSE | 
|  无 |  多 | InBatchNegative|
|  有 | 有  | SimCSE+ InBatchNegative |

最基本的情况是只有无监督数据，我们推荐您使用SimCSE进行无监督训练；另一种方案是只有有监督数据，我们推荐您使用InBatchNegative的方法进行有监督训练。

如果想进一步提升模型效果：还可以使用大规模业务数据，对预训练模型进行Post-Training，训练完以后得到预训练模型，再进行无监督的SimCSE。

此外，如果您同时拥有监督数据和无监督数据，我们推荐将两种方案结合使用，这样能训练出更加强大的语义索引模型。对SimCSE之后的模型，通过in-batch negative方法进行微调，就可以得到语义索引模型。得到语义匹配的模型后，就可以把建库的文本放入模型中抽取特征向量，然后把抽取后的向量放到语义索引引擎milvus中，利用milvus就可以很高效的实现召回了。

![](./img/recall_pipeline.png)

#### 2.2.3 排序模块

基于预训练模型 ERNIE-Gram 训练Pair-wise语义匹配模型。



## 3. 万方文献检索实践

### 3.1 数据说明

数据集来源于万方系统的点击数据，总共600万条，经过数据去重和过滤掉脏数据后，剩下的数据量是4017738条。数据的格式为：

```
每行4列:  query \t title \t keyword \t abstract
```


Post-Training的时候使用的是全量的数据，使用了文本的query,title,abstract三个字段的内容。


|  阶段 |模型 |   测试集 | 召回集 |训练集 |
| ------------ | ------------ |------------ | ------------ | ------------ |
|  召回 |  Post-Training |  - | - |20097095 |
|  召回 |  无监督预训练 |  20000 | 300000 |7984598 |
|  召回 |  有监督训练 | 20000  | 300000 |3998 |
|  排序 |  - | 37862  | - |1874103 |

数据集的下载地址为：

```
wget https://bj.bcebos.com/v1/paddlenlp/data/wanfang_opensource_data.zip
```


### 3.2 技术方案和评估指标

#### 3.2.1 技术方案

**语义索引**：结合SimCSE和In-batch Negative方案，并采取Post-Training优化模型效果

**排序**：使用ERNIE-Gram的单塔结构对召回后的数据精排序。

首先是利用Ernie模型进行Post-Training，训练完以后得到预训练模型，然后再进行无监督的SimCSE进行无监督训练，最后利用in-batch negative方法进行微调，就可以得到语义索引模型。得到语义匹配的模型后，就可以把建库的文本放入模型中抽取特征向量，然后把抽取后的向量放到语义索引引擎milvus中，然后利用milvus就可以很方便的实现召回了。

#### 3.2.2 评估指标

在语义索引召回阶段使用的指标是Recall@K,表示的是在查询的所有实际相关结果中在所有的实际相关结果的比例,在排序阶段使用的指标为AUC，AUC反应的是分类器对样本的排序能力，如果完全随机的对样本分类，那么AUC应该接近0.5。分类器越可能把真正的正样本排在前面，auc越大，分类性能越好。


### 3.3 运行环境和安装说明

（1）运行环境

a. 软件环境：

```
python 3.x
paddlenlp                          2.1.1        
paddlepaddle-gpu                   2.1.3.post101
CUDA Version: 10.2
NVIDIA Driver Version: 440.64.00 
Ubuntu 16.04.6 LTS (Docker)
```

b. 硬件环境：

```
NVIDIA Tesla V100 16GB x4卡
Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
```

c. 依赖安装:

```
pip install -r requirements.txt
```

## 4. 动手实践——搭建自己的检索系统

这里展示了能够从头至尾跑通的完整代码，您使用自己的业务数据，照着跑，能搭建出一个给定query，返回topN相关文档的小型检索系统。请参照我们给出的效果、性能数据，check自己的运行过程是否正确。

### 4.1 召回阶段

**召回模型训练**

召回提供了四种实现方案：

|  方案名 | 说明 | 
| ------------ | ------------ |
|召回方案一|SimCSE|
|召回方案二|In-batch Negative|
|召回方案三|SimCSE+In-batch Negative|
|召回方案四|Post-Training+SimCSE+In-batch Negative|


第一步：Post-Training

训练教程请参考：

[ernie-1.0](./ernie-1.0/)


第二步：无监督预训练


无监督预训练的教程请参考SimCSE的文档：

[simcse](./simcse/)

无监督预训练：16hour54min30s


第三步：有监督训练

有监督训练的教程请参考InbatchNegative的文档：

[in_batch_negative](./in_batch_negative/)


召回阶段的效果评估：


|  模型 |  Recall@1 | Recall@5 |Recall@10 |Recall@20 |Recall@50 |策略简要说明|
| ------------ | ------------ | ------------ |--------- |--------- |--------- |--------- |
|  Baseline | 30.077| 43.513| 48.633 | 53.448 |59.632| 标准 pair-wise 训练范式，通过随机采样产生负样本|
|  In-batch negatives |  51.301 | 65.309| 69.878| 73.996|78.881| Inbatch-negative有监督训练|
|  SimCSE |  42.374 | 57.505| 62.641| 67.09|72.331| SimCSE无监督训练|
|  SimCSE+Inbatch-negative |  55.976 | 71.849| 76.363| 80.49|84.809| SimCSE无监督训练，Inbatch-negative有监督训练|
|  Post Training+SimCSE |  51.031 | 66.648| 71.338 | 75.676 |80.144| Ernie预训练，SimCSE无监督训练|
|  Post Training+SimCSE+Inbatch-negative|  **58.248** | **75.099**| **79.813**| **83.801**|**87.733**| Ernie预训练，simcse无监督训训练，Inbatch-negative有监督训练|


**召回系统搭建**

我们首先展示一下系统的效果，首先输入的文本如下：

```
{0:'中西方语言与文化的差异'}

```
下面是召回的部分结果，第一个是召回的title，第二个数字是计算的相似度距离

```
跨文化中的文化习俗对翻译的影响翻译,跨文化,文化习俗	0.615584135055542
试论翻译过程中的文化差异与语言空缺翻译过程,文化差异,语言空缺,文化对比	0.6155391931533813
中英文化差异及习语翻译习语,文化差异,翻译	0.6153547763824463
英语中的中国文化元素英语,中国文化,语言交流	0.6151996850967407
跨文化交际中的文化误读研究文化误读,影响,中华文化,西方文明	0.6137217283248901
在语言学习中了解中法文化差异文化差异,对话交际,语言	0.6134252548217773
从翻译视角看文化差异影响下的中式英语的应对策略文化差异;中式英语现;汉英翻译;动态对等理论	0.6127341389656067
归化与异化在跨文化传播中的动态平衡归化,异化,翻译策略,跨文化传播,文化外译	0.6127211451530457
浅谈中西言语交际行为中的文化差异交际用语,文化差异,中国,西方	0.6125463843345642
翻译中的文化因素--异化与归化文化翻译,文化因素,异化与归化	0.6111845970153809
历史与文化差异对翻译影响的分析研究历史与文化差异,法汉翻译,翻译方法	0.6107486486434937
从中、韩、美看跨文化交际中的东西方文化差异跨文化交际,东西方,文化差异	0.6091923713684082
试论文化差异对翻译工作的影响文化差异,翻译工作,影响	0.6084284782409668
从归化与异化看翻译中的文化冲突现象翻译,文化冲突,归化与异化,跨文化交际	0.6063553690910339
中西方问候语的文化差异问候语,文化差异,文化背景	0.6054259538650513
中英思维方式的差异对翻译的影响中英文化的差异,中英思维方式的差异,翻译	0.6026732921600342
略论中西方语言文字的特性与差异语言,会意,确意,特性,差异	0.6009351015090942
......

```

排序阶段的结果展示如下，第一个是title，第二个数字是计算的概率：

```
中西方文化差异以及语言体现中西方文化,差异,语言体现	0.999848484992981
论中西方语言与文化差异的历史渊源中西方语言,中西方文化,差异,历史渊源	0.9998375177383423
从日常生活比较中西方语言与文化的差异中西方,语言,文化,比较	0.9985846281051636
试论中西方语言文化教育的差异比较与融合中西方,语言文化教育,差异	0.9972485899925232
中西方文化差异对英语学习的影响中西方文化,差异,英语,学习	0.9831035137176514
跨文化视域下的中西文化差异研究跨文化,中西,文化差异	0.9781349897384644
中西方文化差异对跨文化交际的影响分析文化差异,跨文化交际,影响	0.9735479354858398
探析跨文化交际中的中西方语言差异跨文化交际,中西方,语言差异	0.9668175578117371
中西方文化差异解读中英文差异表达中西文化,差异表达,跨文化交际	0.9629314541816711
中西方文化差异对英语翻译的影响中西方文化差异,英语翻译,翻译策略,影响	0.9538986086845398
论跨文化交际中的中西方文化冲突跨文化交际,冲突,文化差异,交际策略,全球化	0.9493677616119385
中西方文化差异对英汉翻译的影响中西方文化,文化差异,英汉翻译,影响	0.9430705904960632
中西方文化差异与翻译中西方,文化差异,翻译影响,策略方法,译者素质	0.9401137828826904
外语教学中的中西文化差异外语教学,文化,差异	0.9397934675216675
浅析西语国家和中国的文化差异-以西班牙为例跨文化交际,西语国家,文化差异	0.9373322129249573
中英文化差异在语言应用中的体现中英文化,汉语言,语言应用,语言差异	0.9359155297279358
....
```

召回系统使用的索引引擎Milvus，Milvus的详细文档请参考：

[milvus_system](./milvus/)




### 4.2 排序阶段

排序阶段使用的模型是ERNIE-Gram，ERNIE-Gram的详细运行步骤请参考下面的链接：

[ernie_matching](./ernie_matching/)

|  训练集 | 测试集 | 
| ------------ | ------------ | 
 |  1874103| 56793 |


排序阶段的效果评估：

|  模型 |  AUC |
| ------------ | ------------ |
|  In-batchNegative |  0.582 | 
|  ERNIE-Gram |  0.801 | 



## Reference

[1] Tianyu Gao, Xingcheng Yao, Danqi Chen: SimCSE: Simple Contrastive Learning of Sentence Embeddings. EMNLP (1) 2021: 6894-6910

[2] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih, Dense Passage Retrieval for Open-Domain Question Answering, Preprint 2020.

[3] Dongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang:
ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding. NAACL-HLT 2021: 1702-1715

[4] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu:
ERNIE: Enhanced Representation through Knowledge Integration. CoRR abs/1904.09223 (2019)

[5] Xiao, Dongling, Yu-Kun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. “ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding.” ArXiv:2010.12148 [Cs].